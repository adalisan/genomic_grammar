{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the size of convolution kernels affect classifier performance? (Assuming parameter count is the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import Bio.motifs\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import scipy\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('/home/jtao/analysis/genomic_grammar_analysis/'):\n",
    "    os.mkdir('/home/jtao/analysis/genomic_grammar_analysis')\n",
    "os.chdir('/home/jtao/analysis/genomic_grammar_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sequences_to_array(sequences):\n",
    "    '''\n",
    "    inputs: sequence of nucleotides represented as a string composed of A, C, G, T\n",
    "    outputs: a list of numpy array representations of a sequence with:\n",
    "             A = [1, 0, 0, 0]\n",
    "             C = [0, 1, 0, 0]\n",
    "             G = [0, 0, 1, 0]\n",
    "             T = [0, 0, 0, 1]\n",
    "             \n",
    "    '''\n",
    "\n",
    "    nucleotide_array_dict = {'A': [1, 0, 0, 0],\n",
    "                             'C': [0, 1, 0, 0],\n",
    "                             'G': [0, 0, 1, 0],\n",
    "                             'T': [0, 0, 0, 1],\n",
    "                             'N': [0.25,0.25,0.25,0.25]}\n",
    "\n",
    "    sequence_array_list = []\n",
    "    for seq in sequences:\n",
    "        seq_array = []\n",
    "        for nuc in seq:\n",
    "            seq_array.append(nucleotide_array_dict[nuc])\n",
    "        seq_array = np.array(seq_array, dtype=np.float16)\n",
    "        sequence_array_list.append(seq_array)\n",
    "    sequence_array_list = np.array(sequence_array_list,dtype=np.float16)\n",
    "    return sequence_array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_normalize_df(df_input):\n",
    "    df = df_input.copy()\n",
    "    #compute rank\n",
    "    dic = {}\n",
    "    for col in df:\n",
    "        dic.update({col : sorted(df[col])})\n",
    "    sorted_df = pd.DataFrame(dic)\n",
    "    rank = sorted_df.mean(axis = 1).tolist()\n",
    "    #sort\n",
    "    for col in df:\n",
    "        t = np.searchsorted(np.sort(df[col]), df[col])\n",
    "        df[col] = [rank[i] for i in t]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read KLA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_seqRecords = list(SeqIO.parse('./fasta_files/c57bl6_il4-24h_peaks.fa', 'fasta'))\n",
    "# negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_il4-24h_peaks_background.fasta', 'fasta'))\n",
    "\n",
    "positive_seqRecords = list(SeqIO.parse('./peak_sequences/c57bl6_kla-1h_peaks.fasta', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_kla-1h_background.fasta', 'fasta'))[:len(positive_seqRecords)]\n",
    "\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords] + [str(x[:200].seq) for x in negative_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords] + \\\n",
    "    [str(x[:200].reverse_complement().seq) for x in negative_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "\n",
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)\n",
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Convolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_seq_length = len(fasta_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convolution_model(\n",
    "    total_seq_length=200,\n",
    "    seq_size = 150,\n",
    "    num_classes = 2,\n",
    "    num_motifs = 25,\n",
    "    motif_size = 20,\n",
    "    num_dense_neurons = 25, \n",
    "    dropout_rate = 0.25\n",
    "    ):\n",
    "    input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "    input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "    # find motifs\n",
    "    convolution_layer = Conv1D(filters=num_motifs, \n",
    "        kernel_size=motif_size,\n",
    "        activation='relu',\n",
    "        input_shape=(total_seq_length,4),\n",
    "        name='convolution_layer',\n",
    "        padding = 'same'\n",
    "        )\n",
    "    forward_motif_scores = convolution_layer(input_fwd)\n",
    "    reverse_motif_scores = convolution_layer(input_rev)\n",
    "\n",
    "    # crop motif scores to avoid parts of sequence where motif score is computed in only one direction\n",
    "    to_crop = int((total_seq_length - seq_size)/2)\n",
    "    crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "        name='crop_layer')\n",
    "    cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "    cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "\n",
    "    # calculate max scores for each orientation\n",
    "    seq_pool_layer = MaxPool1D(pool_size=seq_size)\n",
    "    max_fwd_scores = seq_pool_layer(cropped_fwd_scores)\n",
    "    max_rev_scores = seq_pool_layer(cropped_rev_scores)\n",
    "\n",
    "    # calculate max score for strand\n",
    "    orientation_max_layer = Maximum()\n",
    "    max_seq_scores = orientation_max_layer([max_fwd_scores, max_rev_scores])\n",
    "\n",
    "    # fully connected layer\n",
    "    dense_out = Dense(num_dense_neurons, activation='relu', \n",
    "                     )(max_seq_scores)\n",
    "\n",
    "    # drop out\n",
    "    drop_out = Dropout(0.25)(dense_out)\n",
    "\n",
    "    # make prediction\n",
    "    flattened = Flatten()(drop_out)\n",
    "    predictions = Dense(num_classes,\n",
    "                        activation = 'softmax', \n",
    "                       )(flattened)\n",
    "    \n",
    "    # define and compile model\n",
    "    convolution_model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "    return convolution_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_model = get_convolution_model(total_seq_length=200,\n",
    "    seq_size = 150,\n",
    "    num_classes = 2,\n",
    "    num_motifs = 16,\n",
    "    motif_size = 24,\n",
    "    num_dense_neurons = 32, \n",
    "    dropout_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_fwd (InputLayer)           (None, 200, 4)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_rev (InputLayer)           (None, 200, 4)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution_layer (Conv1D)       (None, 200, 16)       1552                                         \n",
      "____________________________________________________________________________________________________\n",
      "crop_layer (Cropping1D)          (None, 150, 16)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D)  (None, 1, 16)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "maximum_11 (Maximum)             (None, 1, 16)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1, 32)         544                                          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 1, 32)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 32)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 2)             66                                           \n",
      "====================================================================================================\n",
      "Total params: 2,162\n",
      "Trainable params: 2,162\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convolution_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolution_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 9s - loss: 0.3787 - acc: 0.8318 - val_loss: 0.3953 - val_acc: 0.8248\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3707 - acc: 0.8363 - val_loss: 0.3793 - val_acc: 0.8340\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3664 - acc: 0.8391 - val_loss: 0.3810 - val_acc: 0.8318\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3635 - acc: 0.8399 - val_loss: 0.3806 - val_acc: 0.8319\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3578 - acc: 0.8435 - val_loss: 0.3821 - val_acc: 0.8323\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3567 - acc: 0.8427 - val_loss: 0.3815 - val_acc: 0.8316\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3561 - acc: 0.8433 - val_loss: 0.3890 - val_acc: 0.8252\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3522 - acc: 0.8460 - val_loss: 0.3817 - val_acc: 0.8310\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3518 - acc: 0.8458 - val_loss: 0.3866 - val_acc: 0.8286\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3487 - acc: 0.8479 - val_loss: 0.3791 - val_acc: 0.8324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a12a1d7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolution_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9116724301528729"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = convolution_model.predict([x_test, x_rc_test])\n",
    "\n",
    "sklearn.metrics.roc_auc_score([y[1] for y in y_test], probs[:,1], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different k-mer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 3s - loss: 0.6904 - acc: 0.5256 - val_loss: 0.6876 - val_acc: 0.5371\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6879 - acc: 0.5375 - val_loss: 0.6864 - val_acc: 0.5444\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6875 - acc: 0.5418 - val_loss: 0.6860 - val_acc: 0.5444\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6873 - acc: 0.5443 - val_loss: 0.6860 - val_acc: 0.5444\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6867 - acc: 0.5459 - val_loss: 0.6862 - val_acc: 0.5444\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6871 - acc: 0.5448 - val_loss: 0.6855 - val_acc: 0.5444\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6865 - acc: 0.5438 - val_loss: 0.6852 - val_acc: 0.5444\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6864 - acc: 0.5455 - val_loss: 0.6856 - val_acc: 0.5444\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6864 - acc: 0.5457 - val_loss: 0.6853 - val_acc: 0.5444\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 2s - loss: 0.6862 - acc: 0.5463 - val_loss: 0.6847 - val_acc: 0.5444\n",
      "2 16 16 1548\n",
      "0.5495634823847595 0.5232491389207807 0.5443691883825049\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6915 - acc: 0.5220 - val_loss: 0.6866 - val_acc: 0.5607\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6841 - acc: 0.5586 - val_loss: 0.6816 - val_acc: 0.5731\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6802 - acc: 0.5690 - val_loss: 0.6815 - val_acc: 0.5583\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6779 - acc: 0.5723 - val_loss: 0.6815 - val_acc: 0.5663\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6768 - acc: 0.5746 - val_loss: 0.6742 - val_acc: 0.5800\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6756 - acc: 0.5768 - val_loss: 0.6733 - val_acc: 0.5811\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6751 - acc: 0.5791 - val_loss: 0.6728 - val_acc: 0.5781\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6741 - acc: 0.5797 - val_loss: 0.6726 - val_acc: 0.5813\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6736 - acc: 0.5785 - val_loss: 0.6720 - val_acc: 0.5788\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.6721 - acc: 0.5798 - val_loss: 0.6705 - val_acc: 0.5812\n",
      "3 64 64 1547\n",
      "0.6141700349785627 0.5789473684210527 0.5812191482034209\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6845 - acc: 0.5558 - val_loss: 0.6673 - val_acc: 0.5870\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6405 - acc: 0.6355 - val_loss: 0.6253 - val_acc: 0.6442\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6134 - acc: 0.6634 - val_loss: 0.6016 - val_acc: 0.6679\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6047 - acc: 0.6721 - val_loss: 0.5925 - val_acc: 0.6852\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5990 - acc: 0.6787 - val_loss: 0.5898 - val_acc: 0.6852\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5981 - acc: 0.6812 - val_loss: 0.5967 - val_acc: 0.6763\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5924 - acc: 0.6884 - val_loss: 0.5954 - val_acc: 0.6848\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5917 - acc: 0.6879 - val_loss: 0.5847 - val_acc: 0.6943\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5904 - acc: 0.6906 - val_loss: 0.5848 - val_acc: 0.6879\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5886 - acc: 0.6909 - val_loss: 0.5818 - val_acc: 0.6925\n",
      "4 256 91 1547\n",
      "0.761180254168487 0.671410365335599 0.6924578119618873\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6688 - acc: 0.5984 - val_loss: 0.6375 - val_acc: 0.6353\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5754 - acc: 0.7076 - val_loss: 0.5354 - val_acc: 0.7369\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5337 - acc: 0.7371 - val_loss: 0.5150 - val_acc: 0.7456\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5171 - acc: 0.7460 - val_loss: 0.5065 - val_acc: 0.7558\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5099 - acc: 0.7531 - val_loss: 0.5008 - val_acc: 0.7602\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5051 - acc: 0.7550 - val_loss: 0.4968 - val_acc: 0.7618\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4984 - acc: 0.7614 - val_loss: 0.5026 - val_acc: 0.7543\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5017 - acc: 0.7564 - val_loss: 0.4952 - val_acc: 0.7639\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4916 - acc: 0.7631 - val_loss: 0.4915 - val_acc: 0.7644\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4931 - acc: 0.7625 - val_loss: 0.4921 - val_acc: 0.7595\n",
      "5 1024 73 1533\n",
      "0.8453445274013505 0.7213940370668815 0.7594994834117782\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6560 - acc: 0.6168 - val_loss: 0.5715 - val_acc: 0.7292\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5424 - acc: 0.7334 - val_loss: 0.5341 - val_acc: 0.7357\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.5070 - acc: 0.7565 - val_loss: 0.4897 - val_acc: 0.7699\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4890 - acc: 0.7666 - val_loss: 0.4790 - val_acc: 0.7738\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4793 - acc: 0.7710 - val_loss: 0.4709 - val_acc: 0.7753\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4683 - acc: 0.7786 - val_loss: 0.4642 - val_acc: 0.7786\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4624 - acc: 0.7837 - val_loss: 0.4663 - val_acc: 0.7809\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4585 - acc: 0.7842 - val_loss: 0.4594 - val_acc: 0.7817\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4540 - acc: 0.7870 - val_loss: 0.4736 - val_acc: 0.7701\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.4507 - acc: 0.7879 - val_loss: 0.4669 - val_acc: 0.7774\n",
      "6 4096 62 1550\n",
      "0.8665130376895569 0.7341822522881019 0.7774078751004477\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6512 - acc: 0.6264 - val_loss: 0.5595 - val_acc: 0.7375\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5259 - acc: 0.7455 - val_loss: 0.4865 - val_acc: 0.7697\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4848 - acc: 0.7722 - val_loss: 0.4704 - val_acc: 0.7744\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4629 - acc: 0.7864 - val_loss: 0.4564 - val_acc: 0.7866\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4560 - acc: 0.7871 - val_loss: 0.4547 - val_acc: 0.7849\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4481 - acc: 0.7913 - val_loss: 0.4701 - val_acc: 0.7774\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4428 - acc: 0.7964 - val_loss: 0.4417 - val_acc: 0.7910\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4402 - acc: 0.7991 - val_loss: 0.4514 - val_acc: 0.7804\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4303 - acc: 0.8028 - val_loss: 0.4504 - val_acc: 0.7790\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34841/34841 [==============================] - 5s - loss: 0.4310 - acc: 0.8041 - val_loss: 0.4359 - val_acc: 0.7949\n",
      "7 16384 53 1537\n",
      "0.8803780291476423 0.7687687687687688 0.7948570772586385\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6438 - acc: 0.6355 - val_loss: 0.5507 - val_acc: 0.7360\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5080 - acc: 0.7582 - val_loss: 0.4612 - val_acc: 0.7827\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4645 - acc: 0.7855 - val_loss: 0.4411 - val_acc: 0.7966\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4403 - acc: 0.7985 - val_loss: 0.4388 - val_acc: 0.7937\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4282 - acc: 0.8045 - val_loss: 0.4246 - val_acc: 0.8039\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4185 - acc: 0.8111 - val_loss: 0.4274 - val_acc: 0.8032\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4118 - acc: 0.8152 - val_loss: 0.4236 - val_acc: 0.7981\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4087 - acc: 0.8153 - val_loss: 0.4408 - val_acc: 0.7983\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4013 - acc: 0.8225 - val_loss: 0.4176 - val_acc: 0.8105\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3992 - acc: 0.8204 - val_loss: 0.4156 - val_acc: 0.8075\n",
      "8 65536 47 1551\n",
      "0.8916735129576536 0.8044692737430168 0.8074847893468029\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6517 - acc: 0.6209 - val_loss: 0.5687 - val_acc: 0.7115\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5043 - acc: 0.7617 - val_loss: 0.4572 - val_acc: 0.7915\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4544 - acc: 0.7921 - val_loss: 0.4368 - val_acc: 0.8016\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4336 - acc: 0.8035 - val_loss: 0.4204 - val_acc: 0.8116\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4217 - acc: 0.8106 - val_loss: 0.4211 - val_acc: 0.8058\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4131 - acc: 0.8156 - val_loss: 0.4122 - val_acc: 0.8149\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4101 - acc: 0.8153 - val_loss: 0.4091 - val_acc: 0.8174\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4049 - acc: 0.8187 - val_loss: 0.4098 - val_acc: 0.8195\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3992 - acc: 0.8212 - val_loss: 0.4041 - val_acc: 0.8192\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3933 - acc: 0.8232 - val_loss: 0.4013 - val_acc: 0.8168\n",
      "9 262144 41 1517\n",
      "0.8992321925519735 0.8093599449415003 0.8167833773389966\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6257 - acc: 0.6589 - val_loss: 0.5131 - val_acc: 0.7624\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4801 - acc: 0.7756 - val_loss: 0.4411 - val_acc: 0.7947\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4362 - acc: 0.8021 - val_loss: 0.4280 - val_acc: 0.8028\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4155 - acc: 0.8132 - val_loss: 0.4017 - val_acc: 0.8147\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3998 - acc: 0.8221 - val_loss: 0.3942 - val_acc: 0.8224\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3874 - acc: 0.8289 - val_loss: 0.3881 - val_acc: 0.8232\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3812 - acc: 0.8326 - val_loss: 0.3862 - val_acc: 0.8250\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3768 - acc: 0.8339 - val_loss: 0.3965 - val_acc: 0.8190\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3697 - acc: 0.8377 - val_loss: 0.3867 - val_acc: 0.8256\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3681 - acc: 0.8392 - val_loss: 0.3882 - val_acc: 0.8225\n",
      "10 1048576 37 1517\n",
      "0.9098087499025913 0.7827877290508545 0.8225232464699804\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6452 - acc: 0.6319 - val_loss: 0.5483 - val_acc: 0.7323\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4851 - acc: 0.7768 - val_loss: 0.4490 - val_acc: 0.7932\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4343 - acc: 0.8027 - val_loss: 0.4160 - val_acc: 0.8149\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4084 - acc: 0.8188 - val_loss: 0.4177 - val_acc: 0.8089\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3929 - acc: 0.8272 - val_loss: 0.4254 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3829 - acc: 0.8328 - val_loss: 0.4023 - val_acc: 0.8145\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3766 - acc: 0.8355 - val_loss: 0.3886 - val_acc: 0.8248\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3683 - acc: 0.8398 - val_loss: 0.3843 - val_acc: 0.8291\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3639 - acc: 0.8416 - val_loss: 0.3912 - val_acc: 0.8239\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3628 - acc: 0.8418 - val_loss: 0.3809 - val_acc: 0.8286\n",
      "11 4194304 34 1530\n",
      "0.9111794843891385 0.7971137521222411 0.8286075077488233\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6264 - acc: 0.6613 - val_loss: 0.5052 - val_acc: 0.7810\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4711 - acc: 0.7823 - val_loss: 0.4254 - val_acc: 0.8096\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4216 - acc: 0.8093 - val_loss: 0.4403 - val_acc: 0.7952\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4041 - acc: 0.8199 - val_loss: 0.3987 - val_acc: 0.8207\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3914 - acc: 0.8278 - val_loss: 0.3986 - val_acc: 0.8194\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3856 - acc: 0.8314 - val_loss: 0.3895 - val_acc: 0.8272\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3779 - acc: 0.8366 - val_loss: 0.3886 - val_acc: 0.8268\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3741 - acc: 0.8376 - val_loss: 0.3906 - val_acc: 0.8249\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3717 - acc: 0.8372 - val_loss: 0.3940 - val_acc: 0.8225\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3651 - acc: 0.8427 - val_loss: 0.3801 - val_acc: 0.8281\n",
      "12 16777216 31 1519\n",
      "0.9102240616785178 0.8082010582010583 0.8281483182183447\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6676 - acc: 0.6003 - val_loss: 0.5728 - val_acc: 0.7567\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4989 - acc: 0.7685 - val_loss: 0.4647 - val_acc: 0.7842\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4361 - acc: 0.8061 - val_loss: 0.4159 - val_acc: 0.8185\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4080 - acc: 0.8192 - val_loss: 0.4128 - val_acc: 0.8159\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3933 - acc: 0.8299 - val_loss: 0.3960 - val_acc: 0.8228\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3825 - acc: 0.8333 - val_loss: 0.3966 - val_acc: 0.8240\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3708 - acc: 0.8395 - val_loss: 0.3857 - val_acc: 0.8284\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34841/34841 [==============================] - 5s - loss: 0.3660 - acc: 0.8427 - val_loss: 0.3973 - val_acc: 0.8242\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3629 - acc: 0.8448 - val_loss: 0.3959 - val_acc: 0.8213\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3555 - acc: 0.8490 - val_loss: 0.3772 - val_acc: 0.8372\n",
      "13 67108864 29 1537\n",
      "0.9122656114151739 0.8370457209847597 0.8372173114452991\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6414 - acc: 0.6309 - val_loss: 0.5468 - val_acc: 0.7485\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4934 - acc: 0.7670 - val_loss: 0.4476 - val_acc: 0.7905\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4351 - acc: 0.8036 - val_loss: 0.4213 - val_acc: 0.8082\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4127 - acc: 0.8167 - val_loss: 0.4116 - val_acc: 0.8132\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4012 - acc: 0.8200 - val_loss: 0.4065 - val_acc: 0.8168\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3931 - acc: 0.8262 - val_loss: 0.4203 - val_acc: 0.8113\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3826 - acc: 0.8321 - val_loss: 0.3992 - val_acc: 0.8221\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3775 - acc: 0.8351 - val_loss: 0.4047 - val_acc: 0.8175\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3750 - acc: 0.8358 - val_loss: 0.3922 - val_acc: 0.8263\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3693 - acc: 0.8387 - val_loss: 0.3880 - val_acc: 0.8302\n",
      "14 268435456 27 1539\n",
      "0.9064976442115703 0.819047619047619 0.8302146711054988\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6540 - acc: 0.6218 - val_loss: 0.5660 - val_acc: 0.7305\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5134 - acc: 0.7522 - val_loss: 0.4576 - val_acc: 0.7888\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4455 - acc: 0.7956 - val_loss: 0.4233 - val_acc: 0.8070\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4138 - acc: 0.8142 - val_loss: 0.4049 - val_acc: 0.8187\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3988 - acc: 0.8240 - val_loss: 0.4017 - val_acc: 0.8209\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3825 - acc: 0.8321 - val_loss: 0.4090 - val_acc: 0.8191\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3755 - acc: 0.8344 - val_loss: 0.3881 - val_acc: 0.8228\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3672 - acc: 0.8392 - val_loss: 0.3780 - val_acc: 0.8306\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3611 - acc: 0.8448 - val_loss: 0.3825 - val_acc: 0.8259\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3582 - acc: 0.8448 - val_loss: 0.3737 - val_acc: 0.8356\n",
      "15 1073741824 25 1525\n",
      "0.913969407281501 0.828021096078881 0.8356101480886235\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6745 - acc: 0.5919 - val_loss: 0.6125 - val_acc: 0.6925\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5177 - acc: 0.7548 - val_loss: 0.4542 - val_acc: 0.7891\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4364 - acc: 0.8047 - val_loss: 0.4186 - val_acc: 0.8054\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4076 - acc: 0.8210 - val_loss: 0.4033 - val_acc: 0.8168\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3881 - acc: 0.8329 - val_loss: 0.3973 - val_acc: 0.8199\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3761 - acc: 0.8395 - val_loss: 0.4031 - val_acc: 0.8217\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3660 - acc: 0.8449 - val_loss: 0.3847 - val_acc: 0.8236\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3615 - acc: 0.8471 - val_loss: 0.3714 - val_acc: 0.8358\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3545 - acc: 0.8497 - val_loss: 0.3787 - val_acc: 0.8337\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3497 - acc: 0.8520 - val_loss: 0.3689 - val_acc: 0.8401\n",
      "16 4294967296 23 1495\n",
      "0.9170737797652848 0.8470560076591671 0.840087246010791\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6650 - acc: 0.5953 - val_loss: 0.5965 - val_acc: 0.7214\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5268 - acc: 0.7459 - val_loss: 0.4672 - val_acc: 0.7788\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4485 - acc: 0.7942 - val_loss: 0.4261 - val_acc: 0.8066\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4213 - acc: 0.8084 - val_loss: 0.4200 - val_acc: 0.8113\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4071 - acc: 0.8184 - val_loss: 0.4054 - val_acc: 0.8168\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3941 - acc: 0.8245 - val_loss: 0.4114 - val_acc: 0.8086\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3848 - acc: 0.8308 - val_loss: 0.3993 - val_acc: 0.8186\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3769 - acc: 0.8362 - val_loss: 0.3889 - val_acc: 0.8267\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3710 - acc: 0.8369 - val_loss: 0.3889 - val_acc: 0.8254\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3672 - acc: 0.8400 - val_loss: 0.3856 - val_acc: 0.8267\n",
      "17 17179869184 22 1518\n",
      "0.9076608546216866 0.8058914047043306 0.8266559522442888\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6667 - acc: 0.5992 - val_loss: 0.5914 - val_acc: 0.7488\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5179 - acc: 0.7553 - val_loss: 0.4621 - val_acc: 0.7864\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4528 - acc: 0.7944 - val_loss: 0.4408 - val_acc: 0.7977\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4250 - acc: 0.8082 - val_loss: 0.4231 - val_acc: 0.8054\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4082 - acc: 0.8187 - val_loss: 0.4281 - val_acc: 0.8032\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3967 - acc: 0.8258 - val_loss: 0.4040 - val_acc: 0.8175\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3849 - acc: 0.8357 - val_loss: 0.3972 - val_acc: 0.8216\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3759 - acc: 0.8381 - val_loss: 0.4012 - val_acc: 0.8188\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3685 - acc: 0.8408 - val_loss: 0.3918 - val_acc: 0.8249\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3671 - acc: 0.8410 - val_loss: 0.3923 - val_acc: 0.8261\n",
      "18 68719476736 21 1533\n",
      "0.9080842859880016 0.848644578313253 0.8260819653311905\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6606 - acc: 0.6066 - val_loss: 0.5804 - val_acc: 0.7221\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5301 - acc: 0.7423 - val_loss: 0.4865 - val_acc: 0.7699\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4579 - acc: 0.7880 - val_loss: 0.4405 - val_acc: 0.7950\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4278 - acc: 0.8075 - val_loss: 0.4285 - val_acc: 0.8012\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4105 - acc: 0.8169 - val_loss: 0.4140 - val_acc: 0.8075\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34841/34841 [==============================] - 5s - loss: 0.4026 - acc: 0.8201 - val_loss: 0.4067 - val_acc: 0.8171\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3932 - acc: 0.8255 - val_loss: 0.4094 - val_acc: 0.8135\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3896 - acc: 0.8263 - val_loss: 0.4018 - val_acc: 0.8154\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3830 - acc: 0.8312 - val_loss: 0.4000 - val_acc: 0.8188\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3787 - acc: 0.8332 - val_loss: 0.3980 - val_acc: 0.8187\n",
      "19 274877906944 20 1540\n",
      "0.9032442983339971 0.785804549283909 0.8187349328435312\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 8s - loss: 0.6759 - acc: 0.5734 - val_loss: 0.6186 - val_acc: 0.7291\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5360 - acc: 0.7445 - val_loss: 0.4622 - val_acc: 0.7887\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4534 - acc: 0.7925 - val_loss: 0.4417 - val_acc: 0.7935\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4280 - acc: 0.8057 - val_loss: 0.4337 - val_acc: 0.7993\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4138 - acc: 0.8157 - val_loss: 0.4159 - val_acc: 0.8124\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4045 - acc: 0.8193 - val_loss: 0.4104 - val_acc: 0.8100\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3954 - acc: 0.8244 - val_loss: 0.4067 - val_acc: 0.8151\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3958 - acc: 0.8234 - val_loss: 0.4109 - val_acc: 0.8182\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3861 - acc: 0.8301 - val_loss: 0.4075 - val_acc: 0.8106\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3827 - acc: 0.8317 - val_loss: 0.4041 - val_acc: 0.8166\n",
      "20 1099511627776 19 1539\n",
      "0.8989278397239255 0.793381232310037 0.8165537825737573\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 6s - loss: 0.6649 - acc: 0.5976 - val_loss: 0.5853 - val_acc: 0.7249\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5181 - acc: 0.7516 - val_loss: 0.4569 - val_acc: 0.7845\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4444 - acc: 0.7976 - val_loss: 0.4226 - val_acc: 0.8035\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4126 - acc: 0.8187 - val_loss: 0.4067 - val_acc: 0.8146\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3916 - acc: 0.8273 - val_loss: 0.3961 - val_acc: 0.8225\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3835 - acc: 0.8327 - val_loss: 0.3864 - val_acc: 0.8287\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3733 - acc: 0.8390 - val_loss: 0.3877 - val_acc: 0.8287\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3694 - acc: 0.8411 - val_loss: 0.3846 - val_acc: 0.8262\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3627 - acc: 0.8454 - val_loss: 0.3792 - val_acc: 0.8340\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3600 - acc: 0.8479 - val_loss: 0.3808 - val_acc: 0.8285\n",
      "21 4398046511104 18 1530\n",
      "0.9143701407019565 0.7869591962271888 0.8284927103662036\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 10s - loss: 0.6828 - acc: 0.5652 - val_loss: 0.6439 - val_acc: 0.6444\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5651 - acc: 0.7162 - val_loss: 0.4999 - val_acc: 0.7603\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4760 - acc: 0.7768 - val_loss: 0.4532 - val_acc: 0.7898\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4372 - acc: 0.8036 - val_loss: 0.4404 - val_acc: 0.7974\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4155 - acc: 0.8148 - val_loss: 0.4332 - val_acc: 0.7993\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4053 - acc: 0.8221 - val_loss: 0.4116 - val_acc: 0.8104\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3933 - acc: 0.8301 - val_loss: 0.4065 - val_acc: 0.8131\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3875 - acc: 0.8306 - val_loss: 0.4027 - val_acc: 0.8163\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3817 - acc: 0.8356 - val_loss: 0.3967 - val_acc: 0.8209\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3761 - acc: 0.8379 - val_loss: 0.3971 - val_acc: 0.8197\n",
      "22 17592186044416 17 1513\n",
      "0.9023224875641092 0.8223590715300805 0.8196533119044885\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 7s - loss: 0.6705 - acc: 0.5892 - val_loss: 0.6059 - val_acc: 0.7187\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5348 - acc: 0.7399 - val_loss: 0.4863 - val_acc: 0.7644\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4619 - acc: 0.7842 - val_loss: 0.4477 - val_acc: 0.7888\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4362 - acc: 0.7993 - val_loss: 0.4333 - val_acc: 0.7996\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4185 - acc: 0.8109 - val_loss: 0.4251 - val_acc: 0.8042\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4088 - acc: 0.8164 - val_loss: 0.4274 - val_acc: 0.7999\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3998 - acc: 0.8187 - val_loss: 0.4196 - val_acc: 0.8035\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3946 - acc: 0.8252 - val_loss: 0.4281 - val_acc: 0.8053\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3870 - acc: 0.8287 - val_loss: 0.4122 - val_acc: 0.8094\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3853 - acc: 0.8311 - val_loss: 0.4056 - val_acc: 0.8160\n",
      "23 70368744177664 16 1488\n",
      "0.8978526530550698 0.7949561403508771 0.8159797956606589\n",
      "Train on 34841 samples, validate on 8711 samples\n",
      "Epoch 1/10\n",
      "34841/34841 [==============================] - 11s - loss: 0.6689 - acc: 0.5877 - val_loss: 0.6102 - val_acc: 0.6824\n",
      "Epoch 2/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.5325 - acc: 0.7408 - val_loss: 0.4751 - val_acc: 0.7758\n",
      "Epoch 3/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4630 - acc: 0.7847 - val_loss: 0.4551 - val_acc: 0.7825\n",
      "Epoch 4/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4325 - acc: 0.8019 - val_loss: 0.4305 - val_acc: 0.8001\n",
      "Epoch 5/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4156 - acc: 0.8131 - val_loss: 0.4436 - val_acc: 0.7924\n",
      "Epoch 6/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.4070 - acc: 0.8163 - val_loss: 0.4349 - val_acc: 0.7985\n",
      "Epoch 7/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3945 - acc: 0.8235 - val_loss: 0.4115 - val_acc: 0.8114\n",
      "Epoch 8/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3867 - acc: 0.8313 - val_loss: 0.4045 - val_acc: 0.8171\n",
      "Epoch 9/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3821 - acc: 0.8326 - val_loss: 0.4019 - val_acc: 0.8180\n",
      "Epoch 10/10\n",
      "34841/34841 [==============================] - 5s - loss: 0.3739 - acc: 0.8364 - val_loss: 0.3979 - val_acc: 0.8205\n",
      "24 281474976710656 16 1552\n",
      "0.9041084811019697 0.7874447019170002 0.8204568935828264\n"
     ]
    }
   ],
   "source": [
    "all_rocs = []\n",
    "all_accuracies = []\n",
    "all_precisions = []\n",
    "k_list = []\n",
    "for k in range(2,25):\n",
    "    max_kmers = 4**k\n",
    "    param_count=0\n",
    "    kmers_to_use = 0\n",
    "    while param_count <= 1552 - (4*k+1):\n",
    "        kmers_to_use +=1\n",
    "        param_count += (4*k+1)\n",
    "    kmers_to_use = min(kmers_to_use, max_kmers)\n",
    "\n",
    "    \n",
    "    \n",
    "    current_model = get_convolution_model(total_seq_length=200,\n",
    "    seq_size = 150,\n",
    "    num_classes = 2,\n",
    "    num_motifs = kmers_to_use,\n",
    "    motif_size = k,\n",
    "    num_dense_neurons = 32, \n",
    "    dropout_rate = 0.5)\n",
    "    \n",
    "    current_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    current_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "    \n",
    "    probs = current_model.predict([x_test, x_rc_test])\n",
    "\n",
    "    roc = sklearn.metrics.roc_auc_score([y[1] for y in y_test], probs[:,1], )\n",
    "    precision = sklearn.metrics.precision_score([y[1] for y in y_test], [1 if x > 0.5 else 0 for x in probs[:,1]])\n",
    "    acc = sklearn.metrics.accuracy_score([y[1] for y in y_test], [1 if x > 0.5 else 0 for x in probs[:,1]])\n",
    "    print(k, max_kmers, kmers_to_use, param_count)\n",
    "    print(roc, precision, acc)\n",
    "    \n",
    "    all_rocs.append(roc)\n",
    "    all_accuracies.append(acc)\n",
    "    all_precisions.append(precision)\n",
    "    k_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f86de5db4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with sns.axes_style('whitegrid'):\n",
    "    plt.plot(k_list, all_rocs, label = 'aucROC')\n",
    "    plt.plot(k_list, all_precisions, label = 'Precision')\n",
    "    plt.plot(k_list, all_accuracies, label = 'Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Motif Size')\n",
    "    plt.ylabel('Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
