{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic Grammar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenhan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import Bio.motifs\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('/home/jtao/analysis/genomic_grammar_analysis/'):\n",
    "    os.mkdir('/home/jtao/analysis/genomic_grammar_analysis')\n",
    "os.chdir('/home/jtao/analysis/genomic_grammar_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sequences_to_array(sequences):\n",
    "    '''\n",
    "    inputs: sequence of nucleotides represented as a string composed of A, C, G, T\n",
    "    outputs: a list of numpy array representations of a sequence with:\n",
    "             A = [1, 0, 0, 0]\n",
    "             C = [0, 1, 0, 0]\n",
    "             G = [0, 0, 1, 0]\n",
    "             T = [0, 0, 0, 1]\n",
    "             \n",
    "    '''\n",
    "\n",
    "    nucleotide_array_dict = {'A': [1, 0, 0, 0],\n",
    "                             'C': [0, 1, 0, 0],\n",
    "                             'G': [0, 0, 1, 0],\n",
    "                             'T': [0, 0, 0, 1],\n",
    "                             'N': [0.25,0.25,0.25,0.25]}\n",
    "\n",
    "    sequence_array_list = []\n",
    "    for seq in sequences:\n",
    "        seq_array = []\n",
    "        for nuc in seq:\n",
    "            seq_array.append(nucleotide_array_dict[nuc])\n",
    "        seq_array = np.array(seq_array)\n",
    "        sequence_array_list.append(seq_array)\n",
    "    sequence_array_list = np.array(sequence_array_list)\n",
    "    return sequence_array_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./atac_idr_peaks/c57bl6_il4-24h_peaks.tsv\n",
      "./bed_files/c57bl6_il4-24h_peaks.bed\n",
      "./atac_idr_peaks/c57bl6_kla-1h_peaks.tsv\n",
      "./bed_files/c57bl6_kla-1h_peaks.bed\n",
      "./atac_idr_peaks/c57bl6_veh_peaks.tsv\n",
      "./bed_files/c57bl6_veh_peaks.bed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 7: pos2bed.pl: command not found\n",
      "bash: line 7: pos2bed.pl: command not found\n",
      "bash: line 7: pos2bed.pl: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d ./bed_files ]; then mkdir ./bed_files; else rm ./bed_files/*; fi\n",
    "for peak in ./atac_idr_peaks/*tsv;\n",
    "do echo $peak;\n",
    "new_path=${peak/atac_idr_peaks/bed_files};\n",
    "new_path=${new_path/.tsv/.bed};\n",
    "echo $new_path;\n",
    "pos2bed.pl $peak >$new_path\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bed_files/c57bl6_il4-24h_peaks.bed\n",
      "./fasta_files/c57bl6_il4-24h_peaks.fa\n",
      "reading genome mm10\n",
      "./bed_files/c57bl6_kla-1h_peaks.bed\n",
      "./fasta_files/c57bl6_kla-1h_peaks.fa\n",
      "reading genome mm10\n",
      "./bed_files/c57bl6_veh_peaks.bed\n",
      "./fasta_files/c57bl6_veh_peaks.fa\n",
      "reading genome mm10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./fasta_files/*’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d ./fasta_files ]; then mkdir ./fasta_files; else rm ./fasta_files/*; fi\n",
    "for bed_path in ./bed_files/*bed;\n",
    "do echo $bed_path;\n",
    "new_path=${bed_path/bed_files/fasta_files};\n",
    "new_path=${new_path/.bed/.fa};\n",
    "echo $new_path;\n",
    "/home/jtao/code/tba/model_training/extract_sequences.py $bed_path mm10 $new_path\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Background Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bed_files/c57bl6_il4-24h_peaks.bed\n",
      "/home/jtao/code/tba/model_training/generate_background_coordinates.py ./bed_files/c57bl6_il4-24h_peaks.bed ./background_files -genome mm10\n",
      "./bed_files/c57bl6_kla-1h_peaks.bed\n",
      "/home/jtao/code/tba/model_training/generate_background_coordinates.py ./bed_files/c57bl6_kla-1h_peaks.bed ./background_files -genome mm10\n",
      "./bed_files/c57bl6_veh_peaks.bed\n",
      "/home/jtao/code/tba/model_training/generate_background_coordinates.py ./bed_files/c57bl6_veh_peaks.bed ./background_files -genome mm10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./background_files/*’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d ./background_files ]; then mkdir ./background_files; else rm ./background_files/*; fi\n",
    "for bed_path in ./bed_files/*bed;\n",
    "do echo $bed_path;\n",
    "echo /home/jtao/code/tba/model_training/generate_background_coordinates.py $bed_path ./background_files -genome mm10\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ./make_background.sh\n",
    "script_path=\"./make_background.sh\"\n",
    "if [ ! -d ./background_files/ ]; then mkdir ./background_files/ ; fi\n",
    "for i in ./bed_files/*bed;\n",
    "do \n",
    "    factor=${i##*/};\n",
    "    factor=${factor%.bed};\n",
    "    fasta_path=\"./background_files/${factor}_background.fasta\"\n",
    "    bed_path=\"./background_files/${factor}_background.bed\"\n",
    "\n",
    "    echo \"/home/jtao/code/tba/model_training/generate_background_coordinates.py $i ./background_files/ -genome mm10\" >> $script_path;\n",
    "    echo \"mv ./background_files/background.bed $bed_path\" >> $script_path;\n",
    "    echo \"mv ./background_files/background.fasta $fasta_path\" >> $script_path;\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ./background/*\n",
    "chmod a+x ./*sh\n",
    "bash ./make_background.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# positive_seqRecords = list(SeqIO.parse('./fasta_files/c57bl6_il4-24h_peaks.fa', 'fasta'))\n",
    "# negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_il4-24h_peaks_background.fasta', 'fasta'))\n",
    "\n",
    "positive_seqRecords = list(SeqIO.parse('/home/jtao/analysis/ap1_fdr_analysis/fasta_files/c57bl6_atf3_veh_idr.fasta', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_il4-24h_peaks_background.fasta', 'fasta'))[:len(positive_seqRecords)]\n",
    "\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords] + [str(x[:200].seq) for x in negative_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords] + \\\n",
    "    [str(x[:200].reverse_complement().seq) for x in negative_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "sequence_rc_arrays = np.array(sequence_rc_arrays)\n",
    "\n",
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)\n",
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_num_params(seq_size,\n",
    "                     num_motifs, \n",
    "                     motif_size,\n",
    "                     adjacent_bp_pool_size,\n",
    "                     attention_dim,\n",
    "                     attention_hops,\n",
    "                     num_dense_neurons\n",
    "                    ):\n",
    "    total_params = 0\n",
    "    \n",
    "    # convolution layer\n",
    "    convolution_params = num_motifs * motif_size * 4\n",
    "    total_params += convolution_params\n",
    "    print('Convolution Params:', convolution_params)\n",
    "    \n",
    "    # lstm layer\n",
    "    # account for pooling\n",
    "    lstm_input_size = num_motifs\n",
    "    # account for reverse complement sequence\n",
    "    lstm_input_size = lstm_input_size * 2\n",
    "    \n",
    "    num_lstm_neurons = seq_size/ adjacent_bp_pool_size\n",
    "    \n",
    "    lstm_params = lstm_input_size * 4 * num_lstm_neurons\n",
    "    # account for bidrectional lstm\n",
    "    lstm_params = lstm_params * 2\n",
    "    \n",
    "    total_params += lstm_params\n",
    "    print('LSTM Params:', lstm_params)\n",
    "    \n",
    "    # attention layer\n",
    "    # first layer of perceptron\n",
    "    attention_params = attention_dim * (seq_size/adjacent_bp_pool_size) \n",
    "    # account for hops of attention (second layer of perceptron)\n",
    "    attention_params += attention_dim * attention_hops\n",
    "    \n",
    "    total_params += attention_params\n",
    "    print('Attention Params:', attention_params)\n",
    "    \n",
    "    # dense layer\n",
    "    dense_params = (seq_size/adjacent_bp_pool_size) * num_dense_neurons\n",
    "    total_params += dense_params\n",
    "    print('Dense Params:', dense_params)\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Params: 4000\n",
      "LSTM Params: 12000.0\n",
      "Attention Params: 800.0\n",
      "Dense Params: 7500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24300.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 50\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 10\n",
    "attention_dim = 50 # 350 from A Structured Self-attentive Sentence Embedding\n",
    "attention_hops = 1 # from A Structured Self-attentive Sentence Embedding\n",
    "num_dense_neurons = 500 # 2-layer, 2000 units, from A Structured Self-attentive Sentence Embedding\n",
    "\n",
    "count_num_params(seq_size,\n",
    "    num_motifs, \n",
    "    motif_size,\n",
    "    adjacent_bp_pool_size,\n",
    "    attention_dim,\n",
    "    attention_hops,\n",
    "    num_dense_neurons\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 100\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 1\n",
    "attention_dim = 50 \n",
    "attention_hops = 1 \n",
    "num_dense_neurons = 500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Motif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 100)\n",
      "cropped_fwd_scores (?, 150, 100)\n",
      "max_fwd_scores (?, 1, 100)\n",
      "max_seq_scores (?, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "total_seq_length = len(fasta_seq[0])\n",
    "\n",
    "input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "# find motifs\n",
    "convolution_layer = Conv1D(filters=num_motifs, \n",
    "    kernel_size=motif_size,\n",
    "    activation='relu',\n",
    "    input_shape=(total_seq_length,4),\n",
    "    name='convolution_layer',\n",
    "    padding = 'same'\n",
    "    )\n",
    "forward_motif_scores = convolution_layer(input_fwd)\n",
    "reverse_motif_scores = convolution_layer(input_rev)\n",
    "print('forward_motif_scores', forward_motif_scores.shape)\n",
    "\n",
    "# crop motif scores to avoid parts of sequence where motif score is computed in only one direction\n",
    "to_crop = int((total_seq_length - seq_size)/2)\n",
    "crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "    name='crop_layer')\n",
    "cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "print('cropped_fwd_scores', cropped_fwd_scores.shape)\n",
    "\n",
    "# # flip motif scores\n",
    "# flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "#     output_shape=(seq_size, num_motifs),\n",
    "#     name='flip_layer')\n",
    "# flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "# print('flipped_rev_scores', flipped_rev_scores.shape)\n",
    "\n",
    "# calculate max scores for each orientation\n",
    "seq_pool_layer = MaxPool1D(pool_size=seq_size)\n",
    "max_fwd_scores = seq_pool_layer(cropped_fwd_scores)\n",
    "max_rev_scores = seq_pool_layer(cropped_rev_scores)\n",
    "print('max_fwd_scores', max_fwd_scores.shape)\n",
    "\n",
    "# calculate max score for strand\n",
    "orientation_max_layer = Maximum()\n",
    "max_seq_scores = orientation_max_layer([max_fwd_scores, max_rev_scores])\n",
    "print('max_seq_scores', max_seq_scores.shape)\n",
    "\n",
    "# fully connected layer\n",
    "dense_out = Dense(num_dense_neurons, activation='relu', \n",
    "                 )(max_seq_scores)\n",
    "\n",
    "# drop out\n",
    "drop_out = Dropout(0.25)(dense_out)\n",
    "\n",
    "# make prediction\n",
    "flattened = Flatten()(drop_out)\n",
    "predictions = Dense(num_classes,\n",
    "                    activation = 'softmax', \n",
    "                   )(flattened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and compile model\n",
    "convolution_model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "convolution_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"719pt\" viewBox=\"0.00 0.00 606.00 719.00\" width=\"606pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 715)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-715 602,-715 602,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139649720136872 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139649720136872</title>\n",
       "<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 292,-710.5 292,-664.5 0,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"70\" y=\"-683.8\">input_fwd: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"140,-664.5 140,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"140,-687.5 195,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"195,-664.5 195,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243.5\" y=\"-695.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"195,-687.5 292,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243.5\" y=\"-672.3\">(None, 200, 4)</text>\n",
       "</g>\n",
       "<!-- 139649720136928 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139649720136928</title>\n",
       "<polygon fill=\"none\" points=\"133.5,-581.5 133.5,-627.5 466.5,-627.5 466.5,-581.5 133.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217.5\" y=\"-600.8\">convolution_layer: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"301.5,-581.5 301.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"301.5,-604.5 356.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"356.5,-581.5 356.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"411.5\" y=\"-612.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"356.5,-604.5 466.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"411.5\" y=\"-589.3\">(None, 200, 100)</text>\n",
       "</g>\n",
       "<!-- 139649720136872&#45;&gt;139649720136928 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139649720136872-&gt;139649720136928</title>\n",
       "<path d=\"M188.081,-664.366C206.925,-654.455 229.308,-642.682 249.087,-632.279\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"250.749,-635.36 257.97,-627.607 247.49,-629.164 250.749,-635.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649720136984 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139649720136984</title>\n",
       "<polygon fill=\"none\" points=\"310,-664.5 310,-710.5 598,-710.5 598,-664.5 310,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"378\" y=\"-683.8\">input_rev: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"446,-664.5 446,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"473.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"446,-687.5 501,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"473.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"501,-664.5 501,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549.5\" y=\"-695.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"501,-687.5 598,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549.5\" y=\"-672.3\">(None, 200, 4)</text>\n",
       "</g>\n",
       "<!-- 139649720136984&#45;&gt;139649720136928 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139649720136984-&gt;139649720136928</title>\n",
       "<path d=\"M411.919,-664.366C393.075,-654.455 370.692,-642.682 350.913,-632.279\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"352.51,-629.164 342.03,-627.607 349.251,-635.36 352.51,-629.164\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649719890776 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139649719890776</title>\n",
       "<polygon fill=\"none\" points=\"143,-498.5 143,-544.5 457,-544.5 457,-498.5 143,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217.5\" y=\"-517.8\">crop_layer: Cropping1D</text>\n",
       "<polyline fill=\"none\" points=\"292,-498.5 292,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"292,-521.5 347,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"347,-498.5 347,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-529.3\">(None, 200, 100)</text>\n",
       "<polyline fill=\"none\" points=\"347,-521.5 457,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-506.3\">(None, 150, 100)</text>\n",
       "</g>\n",
       "<!-- 139649720136928&#45;&gt;139649719890776 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139649720136928-&gt;139649719890776</title>\n",
       "<path d=\"M300,-581.366C300,-573.152 300,-563.658 300,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-554.607 300,-544.607 296.5,-554.607 303.5,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649720138272 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139649720138272</title>\n",
       "<polygon fill=\"none\" points=\"114,-415.5 114,-461.5 486,-461.5 486,-415.5 114,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"217.5\" y=\"-434.8\">max_pooling1d_8: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"321,-415.5 321,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"321,-438.5 376,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"376,-415.5 376,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"431\" y=\"-446.3\">(None, 150, 100)</text>\n",
       "<polyline fill=\"none\" points=\"376,-438.5 486,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"431\" y=\"-423.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 139649719890776&#45;&gt;139649720138272 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139649719890776-&gt;139649720138272</title>\n",
       "<path d=\"M300,-498.366C300,-490.152 300,-480.658 300,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-471.607 300,-461.607 296.5,-471.607 303.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649720137544 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139649720137544</title>\n",
       "<polygon fill=\"none\" points=\"99,-332.5 99,-378.5 501,-378.5 501,-332.5 99,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-351.8\">maximum_8: Maximum</text>\n",
       "<polyline fill=\"none\" points=\"252,-332.5 252,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"252,-355.5 307,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"279.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"307,-332.5 307,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"404\" y=\"-363.3\">[(None, 1, 100), (None, 1, 100)]</text>\n",
       "<polyline fill=\"none\" points=\"307,-355.5 501,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"404\" y=\"-340.3\">(None, 1, 100)</text>\n",
       "</g>\n",
       "<!-- 139649720138272&#45;&gt;139649720137544 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139649720138272-&gt;139649720137544</title>\n",
       "<path d=\"M300,-415.366C300,-407.152 300,-397.658 300,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-388.607 300,-378.607 296.5,-388.607 303.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649708523080 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139649708523080</title>\n",
       "<polygon fill=\"none\" points=\"169.5,-249.5 169.5,-295.5 430.5,-295.5 430.5,-249.5 169.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224\" y=\"-268.8\">dense_15: Dense</text>\n",
       "<polyline fill=\"none\" points=\"278.5,-249.5 278.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"278.5,-272.5 333.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"333.5,-249.5 333.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-280.3\">(None, 1, 100)</text>\n",
       "<polyline fill=\"none\" points=\"333.5,-272.5 430.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-257.3\">(None, 1, 500)</text>\n",
       "</g>\n",
       "<!-- 139649720137544&#45;&gt;139649708523080 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139649720137544-&gt;139649708523080</title>\n",
       "<path d=\"M300,-332.366C300,-324.152 300,-314.658 300,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-305.607 300,-295.607 296.5,-305.607 303.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649719891336 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139649719891336</title>\n",
       "<polygon fill=\"none\" points=\"161.5,-166.5 161.5,-212.5 438.5,-212.5 438.5,-166.5 161.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224\" y=\"-185.8\">dropout_8: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"286.5,-166.5 286.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"286.5,-189.5 341.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"341.5,-166.5 341.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-197.3\">(None, 1, 500)</text>\n",
       "<polyline fill=\"none\" points=\"341.5,-189.5 438.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-174.3\">(None, 1, 500)</text>\n",
       "</g>\n",
       "<!-- 139649708523080&#45;&gt;139649719891336 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139649708523080-&gt;139649719891336</title>\n",
       "<path d=\"M300,-249.366C300,-241.152 300,-231.658 300,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-222.607 300,-212.607 296.5,-222.607 303.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649708084192 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139649708084192</title>\n",
       "<polygon fill=\"none\" points=\"169,-83.5 169,-129.5 431,-129.5 431,-83.5 169,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224\" y=\"-102.8\">flatten_8: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"279,-83.5 279,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"279,-106.5 334,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"334,-83.5 334,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-114.3\">(None, 1, 500)</text>\n",
       "<polyline fill=\"none\" points=\"334,-106.5 431,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-91.3\">(None, 500)</text>\n",
       "</g>\n",
       "<!-- 139649719891336&#45;&gt;139649708084192 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>139649719891336-&gt;139649708084192</title>\n",
       "<path d=\"M300,-166.366C300,-158.152 300,-148.658 300,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-139.607 300,-129.607 296.5,-139.607 303.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649708081784 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139649708081784</title>\n",
       "<polygon fill=\"none\" points=\"176.5,-0.5 176.5,-46.5 423.5,-46.5 423.5,-0.5 176.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-19.8\">dense_16: Dense</text>\n",
       "<polyline fill=\"none\" points=\"285.5,-0.5 285.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"285.5,-23.5 340.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"340.5,-0.5 340.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-31.3\">(None, 500)</text>\n",
       "<polyline fill=\"none\" points=\"340.5,-23.5 423.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139649708084192&#45;&gt;139649708081784 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>139649708084192-&gt;139649708081784</title>\n",
       "<path d=\"M300,-83.3664C300,-75.1516 300,-65.6579 300,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"303.5,-56.6068 300,-46.6068 296.5,-56.6069 303.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(convolution_model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59352 samples, validate on 14838 samples\n",
      "Epoch 1/10\n",
      "59352/59352 [==============================] - 5s 80us/step - loss: 0.5249 - acc: 0.7198 - val_loss: 0.4040 - val_acc: 0.8152\n",
      "Epoch 2/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.3783 - acc: 0.8312 - val_loss: 0.3662 - val_acc: 0.8353\n",
      "Epoch 3/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.3459 - acc: 0.8490 - val_loss: 0.3574 - val_acc: 0.8425\n",
      "Epoch 4/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.3189 - acc: 0.8631 - val_loss: 0.3472 - val_acc: 0.8457\n",
      "Epoch 5/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.3075 - acc: 0.8688 - val_loss: 0.3667 - val_acc: 0.8375\n",
      "Epoch 6/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.2954 - acc: 0.8748 - val_loss: 0.3948 - val_acc: 0.8215\n",
      "Epoch 7/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.2853 - acc: 0.8799 - val_loss: 0.3662 - val_acc: 0.8397\n",
      "Epoch 8/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.2769 - acc: 0.8847 - val_loss: 0.3425 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.2734 - acc: 0.8859 - val_loss: 0.3539 - val_acc: 0.8464\n",
      "Epoch 10/10\n",
      "59352/59352 [==============================] - 4s 60us/step - loss: 0.2674 - acc: 0.8890 - val_loss: 0.3518 - val_acc: 0.8508\n",
      "Test loss: 0.35175506580404586\n",
      "Test accuracy: 0.8507885159885712\n"
     ]
    }
   ],
   "source": [
    "convolution_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = convolution_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9290040404301393"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = convolution_model.predict([x_test, x_rc_test])\n",
    "\n",
    "sklearn.metrics.roc_auc_score([y[1] for y in y_test], probs[:,1], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Params: 4000\n",
      "LSTM Params: 12000.0\n",
      "Attention Params: 5600.0\n",
      "Dense Params: 15000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36600.0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 50\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 10\n",
    "attention_dim = 350 \n",
    "attention_hops = 1 \n",
    "num_dense_neurons = 1000 \n",
    "count_num_params(seq_size,\n",
    "    num_motifs, \n",
    "    motif_size,\n",
    "    adjacent_bp_pool_size,\n",
    "    attention_dim,\n",
    "    attention_hops,\n",
    "    num_dense_neurons\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 100)\n",
      "cropped_fwd_scores (?, 150, 100)\n",
      "flipped_rev_scores (?, 150, 100)\n",
      "concatenated_motif_scores (?, 150, 200)\n",
      "pooled_scores (?, 15, 200)\n",
      "forward_hidden_states (?, ?, 15)\n",
      "reverse_hidden_states (?, ?, 15)\n",
      "bilstm_hidden_states (?, ?, 30)\n"
     ]
    }
   ],
   "source": [
    "total_seq_length = len(fasta_seq[0])\n",
    "\n",
    "input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "# find motifs\n",
    "convolution_layer = Conv1D(filters=num_motifs, \n",
    "    kernel_size=motif_size,\n",
    "    activation='relu',\n",
    "    input_shape=(total_seq_length,4),\n",
    "    name='convolution_layer',\n",
    "    padding = 'same'\n",
    "    )\n",
    "forward_motif_scores = convolution_layer(input_fwd)\n",
    "reverse_motif_scores = convolution_layer(input_rev)\n",
    "print('forward_motif_scores', forward_motif_scores.shape)\n",
    "\n",
    "# crop motif scores to avoid parts of sequence where motif score is computed in only one direction\n",
    "to_crop = int((total_seq_length - seq_size)/2)\n",
    "crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "    name='crop_layer')\n",
    "cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "print('cropped_fwd_scores', cropped_fwd_scores.shape)\n",
    "\n",
    "# flip motif scores\n",
    "flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "    output_shape=(seq_size, num_motifs),\n",
    "    name='flip_layer')\n",
    "flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "print('flipped_rev_scores', flipped_rev_scores.shape)\n",
    "\n",
    "# concatenate motif scores\n",
    "concatenate_layer = keras.layers.Concatenate(axis=2, name='concatenate_layer')\n",
    "concatenated_motif_scores = concatenate_layer([cropped_fwd_scores, flipped_rev_scores])\n",
    "print('concatenated_motif_scores', concatenated_motif_scores.shape)\n",
    "\n",
    "# pool across length of sequence\n",
    "sequence_pooling_layer = MaxPool1D(pool_size=adjacent_bp_pool_size, \n",
    "    strides=adjacent_bp_pool_size,\n",
    "    name='sequence_pooling_layer')\n",
    "pooled_scores = sequence_pooling_layer(concatenated_motif_scores)\n",
    "print('pooled_scores', pooled_scores.shape)\n",
    "\n",
    "# bidirectional LSTM\n",
    "forward_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "    return_sequences=True,\n",
    "    name = 'forward_lstm_layer'\n",
    "    )\n",
    "forward_hidden_states = forward_lstm_layer(pooled_scores)\n",
    "print('forward_hidden_states', forward_hidden_states.shape)\n",
    "\n",
    "reverse_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "    return_sequences=True,\n",
    "    name = 'reverse_lstm_layer',\n",
    "    go_backwards=True\n",
    "    )\n",
    "reverse_hidden_states = reverse_lstm_layer(pooled_scores)\n",
    "print('reverse_hidden_states', reverse_hidden_states.shape)\n",
    "\n",
    "# concatenate lstm hidden states\n",
    "lstm_concatenate_layer = Concatenate(axis=2)\n",
    "bilstm_hidden_states = lstm_concatenate_layer([forward_hidden_states, reverse_hidden_states])\n",
    "print('bilstm_hidden_states', bilstm_hidden_states.shape)\n",
    "\n",
    "# fully connected layer\n",
    "dense_layer = Dense(num_dense_neurons, \n",
    "    activation='relu', \n",
    "    name = 'dense_layer'\n",
    "    )\n",
    "\n",
    "dense_output = dense_layer(bilstm_hidden_states)\n",
    "\n",
    "# drop out\n",
    "drop_out = Dropout(0.25,name='dense_dropout')(dense_output)\n",
    "\n",
    "# make prediction\n",
    "flattened = Flatten(name='flatten')(drop_out)\n",
    "predictions = Dense(num_classes,\n",
    "                    name='predictions',\n",
    "                    activation = 'softmax', \n",
    "                   )(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and compile model\n",
    "bilstm_model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "bilstm_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"968pt\" viewBox=\"0.00 0.00 676.00 968.00\" width=\"676pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 964)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-964 672,-964 672,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139649673675496 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139649673675496</title>\n",
       "<polygon fill=\"none\" points=\"34.5,-913.5 34.5,-959.5 326.5,-959.5 326.5,-913.5 34.5,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-932.8\">input_fwd: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-913.5 174.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-936.5 229.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-913.5 229.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-944.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-936.5 326.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-921.3\">(None, 200, 4)</text>\n",
       "</g>\n",
       "<!-- 139649673675552 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139649673675552</title>\n",
       "<polygon fill=\"none\" points=\"168,-830.5 168,-876.5 501,-876.5 501,-830.5 168,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-849.8\">convolution_layer: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"336,-830.5 336,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363.5\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"336,-853.5 391,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363.5\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"391,-830.5 391,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"446\" y=\"-861.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"391,-853.5 501,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"446\" y=\"-838.3\">(None, 200, 100)</text>\n",
       "</g>\n",
       "<!-- 139649673675496&#45;&gt;139649673675552 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139649673675496-&gt;139649673675552</title>\n",
       "<path d=\"M222.581,-913.366C241.425,-903.455 263.808,-891.682 283.587,-881.279\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"285.249,-884.36 292.47,-876.607 281.99,-878.164 285.249,-884.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649673675608 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139649673675608</title>\n",
       "<polygon fill=\"none\" points=\"344.5,-913.5 344.5,-959.5 632.5,-959.5 632.5,-913.5 344.5,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"412.5\" y=\"-932.8\">input_rev: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"480.5,-913.5 480.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"508\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"480.5,-936.5 535.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"508\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"535.5,-913.5 535.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"584\" y=\"-944.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"535.5,-936.5 632.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"584\" y=\"-921.3\">(None, 200, 4)</text>\n",
       "</g>\n",
       "<!-- 139649673675608&#45;&gt;139649673675552 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139649673675608-&gt;139649673675552</title>\n",
       "<path d=\"M446.419,-913.366C427.575,-903.455 405.192,-891.682 385.413,-881.279\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"387.01,-878.164 376.53,-876.607 383.751,-884.36 387.01,-878.164\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649673720160 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139649673720160</title>\n",
       "<polygon fill=\"none\" points=\"177.5,-747.5 177.5,-793.5 491.5,-793.5 491.5,-747.5 177.5,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-766.8\">crop_layer: Cropping1D</text>\n",
       "<polyline fill=\"none\" points=\"326.5,-747.5 326.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"326.5,-770.5 381.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-747.5 381.5,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-778.3\">(None, 200, 100)</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-770.5 491.5,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-755.3\">(None, 150, 100)</text>\n",
       "</g>\n",
       "<!-- 139649673675552&#45;&gt;139649673720160 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139649673675552-&gt;139649673720160</title>\n",
       "<path d=\"M334.5,-830.366C334.5,-822.152 334.5,-812.658 334.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-803.607 334.5,-793.607 331,-803.607 338,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649673805720 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139649673805720</title>\n",
       "<polygon fill=\"none\" points=\"105,-664.5 105,-710.5 392,-710.5 392,-664.5 105,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-683.8\">flip_layer: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"227,-664.5 227,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"227,-687.5 282,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"282,-664.5 282,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337\" y=\"-695.3\">(None, 150, 100)</text>\n",
       "<polyline fill=\"none\" points=\"282,-687.5 392,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337\" y=\"-672.3\">(None, 150, 100)</text>\n",
       "</g>\n",
       "<!-- 139649673720160&#45;&gt;139649673805720 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139649673720160-&gt;139649673805720</title>\n",
       "<path d=\"M311,-747.366C301.235,-738.169 289.769,-727.369 279.341,-717.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"281.651,-714.915 271.971,-710.607 276.851,-720.011 281.651,-714.915\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649673719648 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139649673719648</title>\n",
       "<polygon fill=\"none\" points=\"102.5,-581.5 102.5,-627.5 566.5,-627.5 566.5,-581.5 102.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196.5\" y=\"-600.8\">concatenate_layer: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"290.5,-581.5 290.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"290.5,-604.5 345.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"318\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"345.5,-581.5 345.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456\" y=\"-612.3\">[(None, 150, 100), (None, 150, 100)]</text>\n",
       "<polyline fill=\"none\" points=\"345.5,-604.5 566.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456\" y=\"-589.3\">(None, 150, 200)</text>\n",
       "</g>\n",
       "<!-- 139649673720160&#45;&gt;139649673719648 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139649673720160-&gt;139649673719648</title>\n",
       "<path d=\"M370.639,-747.306C382.789,-737.778 394.92,-725.48 401.5,-711 410.142,-691.982 410.142,-683.018 401.5,-664 396.359,-652.687 387.831,-642.706 378.552,-634.338\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"380.548,-631.444 370.639,-627.694 376.046,-636.805 380.548,-631.444\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649673805720&#45;&gt;139649673719648 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139649673805720-&gt;139649673719648</title>\n",
       "<path d=\"M272,-664.366C281.765,-655.169 293.231,-644.369 303.659,-634.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"306.149,-637.011 311.029,-627.607 301.349,-631.915 306.149,-637.011\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649637232144 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139649637232144</title>\n",
       "<polygon fill=\"none\" points=\"132.5,-498.5 132.5,-544.5 536.5,-544.5 536.5,-498.5 132.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-517.8\">sequence_pooling_layer: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"371.5,-498.5 371.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"371.5,-521.5 426.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"399\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"426.5,-498.5 426.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"481.5\" y=\"-529.3\">(None, 150, 200)</text>\n",
       "<polyline fill=\"none\" points=\"426.5,-521.5 536.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"481.5\" y=\"-506.3\">(None, 15, 200)</text>\n",
       "</g>\n",
       "<!-- 139649673719648&#45;&gt;139649637232144 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139649673719648-&gt;139649637232144</title>\n",
       "<path d=\"M334.5,-581.366C334.5,-573.152 334.5,-563.658 334.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-554.607 334.5,-544.607 331,-554.607 338,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649673805776 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139649673805776</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 327,-461.5 327,-415.5 0,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84\" y=\"-434.8\">forward_lstm_layer: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"168,-415.5 168,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"195.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"168,-438.5 223,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"195.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"223,-415.5 223,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275\" y=\"-446.3\">(None, 15, 200)</text>\n",
       "<polyline fill=\"none\" points=\"223,-438.5 327,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275\" y=\"-423.3\">(None, 15, 15)</text>\n",
       "</g>\n",
       "<!-- 139649637232144&#45;&gt;139649673805776 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139649637232144-&gt;139649673805776</title>\n",
       "<path d=\"M287.773,-498.366C266.661,-488.366 241.547,-476.47 219.44,-465.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"220.706,-462.725 210.17,-461.607 217.709,-469.051 220.706,-462.725\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649637324728 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139649637324728</title>\n",
       "<polygon fill=\"none\" points=\"345,-415.5 345,-461.5 668,-461.5 668,-415.5 345,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"427\" y=\"-434.8\">reverse_lstm_layer: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"509,-415.5 509,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"536.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"509,-438.5 564,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"536.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"564,-415.5 564,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616\" y=\"-446.3\">(None, 15, 200)</text>\n",
       "<polyline fill=\"none\" points=\"564,-438.5 668,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616\" y=\"-423.3\">(None, 15, 15)</text>\n",
       "</g>\n",
       "<!-- 139649637232144&#45;&gt;139649637324728 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139649637232144-&gt;139649637324728</title>\n",
       "<path d=\"M381.5,-498.366C402.736,-488.366 427.996,-476.47 450.233,-465.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"452.001,-469.034 459.557,-461.607 449.019,-462.701 452.001,-469.034\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649633378376 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139649633378376</title>\n",
       "<polygon fill=\"none\" points=\"122.5,-332.5 122.5,-378.5 546.5,-378.5 546.5,-332.5 122.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210\" y=\"-351.8\">concatenate_12: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"297.5,-332.5 297.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"297.5,-355.5 352.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"352.5,-332.5 352.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"449.5\" y=\"-363.3\">[(None, 15, 15), (None, 15, 15)]</text>\n",
       "<polyline fill=\"none\" points=\"352.5,-355.5 546.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"449.5\" y=\"-340.3\">(None, 15, 30)</text>\n",
       "</g>\n",
       "<!-- 139649673805776&#45;&gt;139649633378376 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>139649673805776-&gt;139649633378376</title>\n",
       "<path d=\"M210.227,-415.366C231.339,-405.366 256.453,-393.47 278.56,-382.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"280.291,-386.051 287.83,-378.607 277.294,-379.725 280.291,-386.051\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649637324728&#45;&gt;139649633378376 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>139649637324728-&gt;139649633378376</title>\n",
       "<path d=\"M459.5,-415.366C438.264,-405.366 413.004,-393.47 390.767,-382.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"391.981,-379.701 381.443,-378.607 388.999,-386.034 391.981,-379.701\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649631792208 -->\n",
       "<g class=\"node\" id=\"node11\"><title>139649631792208</title>\n",
       "<polygon fill=\"none\" points=\"191,-249.5 191,-295.5 478,-295.5 478,-249.5 191,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-268.8\">dense_layer: Dense</text>\n",
       "<polyline fill=\"none\" points=\"313,-249.5 313,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"313,-272.5 368,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"368,-249.5 368,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"423\" y=\"-280.3\">(None, 15, 30)</text>\n",
       "<polyline fill=\"none\" points=\"368,-272.5 478,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"423\" y=\"-257.3\">(None, 15, 1000)</text>\n",
       "</g>\n",
       "<!-- 139649633378376&#45;&gt;139649631792208 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>139649633378376-&gt;139649631792208</title>\n",
       "<path d=\"M334.5,-332.366C334.5,-324.152 334.5,-314.658 334.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-305.607 334.5,-295.607 331,-305.607 338,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649631493592 -->\n",
       "<g class=\"node\" id=\"node12\"><title>139649631493592</title>\n",
       "<polygon fill=\"none\" points=\"177.5,-166.5 177.5,-212.5 491.5,-212.5 491.5,-166.5 177.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-185.8\">dense_dropout: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"326.5,-166.5 326.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"326.5,-189.5 381.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-166.5 381.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-197.3\">(None, 15, 1000)</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-189.5 491.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-174.3\">(None, 15, 1000)</text>\n",
       "</g>\n",
       "<!-- 139649631792208&#45;&gt;139649631493592 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>139649631792208-&gt;139649631493592</title>\n",
       "<path d=\"M334.5,-249.366C334.5,-241.152 334.5,-231.658 334.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-222.607 334.5,-212.607 331,-222.607 338,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649631494040 -->\n",
       "<g class=\"node\" id=\"node13\"><title>139649631494040</title>\n",
       "<polygon fill=\"none\" points=\"204,-83.5 204,-129.5 465,-129.5 465,-83.5 204,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-102.8\">flatten: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"300,-83.5 300,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"300,-106.5 355,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"355,-83.5 355,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"410\" y=\"-114.3\">(None, 15, 1000)</text>\n",
       "<polyline fill=\"none\" points=\"355,-106.5 465,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"410\" y=\"-91.3\">(None, 15000)</text>\n",
       "</g>\n",
       "<!-- 139649631493592&#45;&gt;139649631494040 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>139649631493592-&gt;139649631494040</title>\n",
       "<path d=\"M334.5,-166.366C334.5,-158.152 334.5,-148.658 334.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-139.607 334.5,-129.607 331,-139.607 338,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139649631165296 -->\n",
       "<g class=\"node\" id=\"node14\"><title>139649631165296</title>\n",
       "<polygon fill=\"none\" points=\"200,-0.5 200,-46.5 469,-46.5 469,-0.5 200,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-19.8\">predictions: Dense</text>\n",
       "<polyline fill=\"none\" points=\"318,-0.5 318,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"318,-23.5 373,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"373,-0.5 373,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-31.3\">(None, 15000)</text>\n",
       "<polyline fill=\"none\" points=\"373,-23.5 469,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139649631494040&#45;&gt;139649631165296 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>139649631494040-&gt;139649631165296</title>\n",
       "<path d=\"M334.5,-83.3664C334.5,-75.1516 334.5,-65.6579 334.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"338,-56.6068 334.5,-46.6068 331,-56.6069 338,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(bilstm_model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59352 samples, validate on 14838 samples\n",
      "Epoch 1/10\n",
      "59352/59352 [==============================] - 20s 338us/step - loss: 0.6107 - acc: 0.6445 - val_loss: 0.5357 - val_acc: 0.7328\n",
      "Epoch 2/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.4618 - acc: 0.7808 - val_loss: 0.4214 - val_acc: 0.8024\n",
      "Epoch 3/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.3931 - acc: 0.8217 - val_loss: 0.3870 - val_acc: 0.8267\n",
      "Epoch 4/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.3601 - acc: 0.8420 - val_loss: 0.3789 - val_acc: 0.8333\n",
      "Epoch 5/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.3352 - acc: 0.8551 - val_loss: 0.3592 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.3200 - acc: 0.8625 - val_loss: 0.3528 - val_acc: 0.8469\n",
      "Epoch 7/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.3077 - acc: 0.8692 - val_loss: 0.3483 - val_acc: 0.8499\n",
      "Epoch 8/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.2956 - acc: 0.8742 - val_loss: 0.3550 - val_acc: 0.8461\n",
      "Epoch 9/10\n",
      "59352/59352 [==============================] - 18s 310us/step - loss: 0.2868 - acc: 0.8791 - val_loss: 0.3481 - val_acc: 0.8478\n",
      "Epoch 10/10\n",
      "59352/59352 [==============================] - 18s 311us/step - loss: 0.2823 - acc: 0.8813 - val_loss: 0.3507 - val_acc: 0.8517\n",
      "Test loss: 0.35110518303758165\n",
      "Test accuracy: 0.8503841488392531\n"
     ]
    }
   ],
   "source": [
    "bilstm_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = bilstm_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.926895845746917"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = bilstm_model.predict([x_test, x_rc_test])\n",
    "\n",
    "sklearn.metrics.roc_auc_score([y[1] for y in y_test], probs[:,1], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Params: 1200\n",
      "LSTM Params: 60000.0\n",
      "Attention Params: 30200.0\n",
      "Dense Params: 75000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166400.0"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 25\n",
    "motif_size = 12\n",
    "adjacent_bp_pool_size = 1\n",
    "attention_dim = 200\n",
    "attention_hops = 1\n",
    "num_dense_neurons = 500 \n",
    "count_num_params(seq_size,\n",
    "    num_motifs, \n",
    "    motif_size,\n",
    "    adjacent_bp_pool_size,\n",
    "    attention_dim,\n",
    "    attention_hops,\n",
    "    num_dense_neurons\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 25)\n",
      "cropped_fwd_scores (?, 150, 25)\n",
      "flipped_rev_scores (?, 150, 25)\n",
      "concatenated_motif_scores (?, 150, 50)\n",
      "pooled_scores (?, 150, 50)\n",
      "forward_hidden_states (?, ?, 150)\n",
      "reverse_hidden_states (?, ?, 150)\n",
      "bilstm_hidden_states (?, ?, 300)\n",
      "attention_tanh_layer_out (?, 150, 200)\n",
      "attention_outer_layer_out (?, 150, 1)\n",
      "attention_softmax_layer_out (?, 150, 1)\n",
      "attended_states (?, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "total_seq_length = len(fasta_seq[0])\n",
    "\n",
    "input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "### find motifs ###\n",
    "convolution_layer = Conv1D(filters=num_motifs, \n",
    "    kernel_size=motif_size,\n",
    "    activation='relu',\n",
    "    input_shape=(total_seq_length,4),\n",
    "    name='convolution_layer',\n",
    "    padding = 'same'\n",
    "    )\n",
    "forward_motif_scores = convolution_layer(input_fwd)\n",
    "reverse_motif_scores = convolution_layer(input_rev)\n",
    "print('forward_motif_scores', forward_motif_scores.get_shape())\n",
    "\n",
    "### crop motif scores to avoid parts of sequence where motif score is computed in only one direction ###\n",
    "to_crop = int((total_seq_length - seq_size)/2)\n",
    "crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "    name='crop_layer')\n",
    "cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "print('cropped_fwd_scores', cropped_fwd_scores.get_shape())\n",
    "\n",
    "### flip motif scores ###\n",
    "flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "    output_shape=(seq_size, num_motifs),\n",
    "    name='flip_layer')\n",
    "flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "print('flipped_rev_scores', flipped_rev_scores.get_shape())\n",
    "\n",
    "### concatenate motif scores ###\n",
    "concatenate_layer = keras.layers.Concatenate(axis=2, name='concatenate_layer')\n",
    "concatenated_motif_scores = concatenate_layer([cropped_fwd_scores, flipped_rev_scores])\n",
    "print('concatenated_motif_scores', concatenated_motif_scores.get_shape())\n",
    "\n",
    "### pool across length of sequence ###\n",
    "sequence_pooling_layer = MaxPool1D(pool_size=adjacent_bp_pool_size, \n",
    "    strides=adjacent_bp_pool_size,\n",
    "    name='sequence_pooling_layer')\n",
    "pooled_scores = sequence_pooling_layer(concatenated_motif_scores)\n",
    "print('pooled_scores', pooled_scores.get_shape())\n",
    "\n",
    "## bidirectional LSTM ###\n",
    "forward_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "    return_sequences=True,\n",
    "    input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "    name = 'forward_lstm_layer'\n",
    "    )\n",
    "forward_hidden_states = forward_lstm_layer(pooled_scores)\n",
    "print('forward_hidden_states', forward_hidden_states.get_shape())\n",
    "\n",
    "reverse_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "    return_sequences=True,\n",
    "    input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "    name = 'reverse_lstm_layer',\n",
    "    go_backwards=True,\n",
    "    )\n",
    "reverse_hidden_states = reverse_lstm_layer(pooled_scores)\n",
    "print('reverse_hidden_states', reverse_hidden_states.get_shape())\n",
    "### concatenate lstm hidden states ###\n",
    "lstm_concatenate_layer = Concatenate(axis=2)\n",
    "bilstm_hidden_states = lstm_concatenate_layer([forward_hidden_states, reverse_hidden_states])\n",
    "\n",
    "# bilstm_layer = Bidirectional(LSTM(\n",
    "#     units=int(seq_size/adjacent_bp_pool_size),\n",
    "#     return_sequences=True,\n",
    "#     input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "#     name = 'bilstm_layer'))\n",
    "# bilstm_hidden_states = bilstm_layer(pooled_scores)\n",
    "print('bilstm_hidden_states', bilstm_hidden_states.get_shape())\n",
    "\n",
    "### attention layer ###\n",
    "# transpose hidden states\n",
    "# transpose_layer = Lambda(lambda x: K.permute_dimensions(x,(0,2,1)),\n",
    "#     name='transpose_layer')\n",
    "# transposed_hidden_states = transpose_layer(bilstm_hidden_states)\n",
    "# print('transposed_hidden_states', transposed_hidden_states.get_shape())\n",
    "\n",
    "# tanh layer\n",
    "attention_tanh_layer = Dense(attention_dim,\n",
    "    activation='tanh',\n",
    "    use_bias=False,\n",
    "    name = 'attention_tanh_layer')\n",
    "attention_tanh_layer_out = attention_tanh_layer(bilstm_hidden_states)\n",
    "print('attention_tanh_layer_out', attention_tanh_layer_out.get_shape())\n",
    "\n",
    "# rotate_layer = Lambda(lambda x: K.permute_dimensions(x,(0,2,1)),\n",
    "# #     name='rotate_layer'\n",
    "#     )\n",
    "# rotated_attention_tanh_layer_out = rotate_layer(attention_tanh_layer_out)\n",
    "\n",
    "# outer layer\n",
    "attention_outer_layer = Dense(attention_hops,\n",
    "    activation='relu',\n",
    "    use_bias=False,\n",
    "    name = 'attention_outer_layer')\n",
    "attention_outer_layer_out = attention_outer_layer(attention_tanh_layer_out)\n",
    "print('attention_outer_layer_out', attention_outer_layer_out.get_shape())\n",
    "\n",
    "# apply softmax\n",
    "softmax_layer = Softmax(axis=1, name='attention_softmax_layer')\n",
    "attention_softmax_layer_out = softmax_layer(attention_outer_layer_out)\n",
    "print('attention_softmax_layer_out', attention_softmax_layer_out.get_shape())\n",
    "\n",
    "# attend to hidden states\n",
    "attending_layer = Dot(axes=(1,1),\n",
    "    name='attending_layer')\n",
    "\n",
    "attended_states = attending_layer([attention_softmax_layer_out, bilstm_hidden_states])\n",
    "print('attended_states', attended_states.get_shape())\n",
    "\n",
    "# # fully connected layer\n",
    "dense_layer = Dense(num_dense_neurons, \n",
    "    activation='relu', \n",
    "    name = 'dense_layer'\n",
    "    )\n",
    "\n",
    "dense_output = dense_layer(attended_states)\n",
    "\n",
    "# drop out\n",
    "drop_out = Dropout(0.25,name='dense_dropout')(dense_output)\n",
    "\n",
    "# make prediction\n",
    "flattened = Flatten(name='flatten')(drop_out)\n",
    "predictions = Dense(num_classes,\n",
    "                    name='predictions',\n",
    "                    activation = 'softmax', \n",
    "                   )(flattened)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define and compile model\n",
    "attention_model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "attention_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1300pt\" viewBox=\"0.00 0.00 688.00 1300.00\" width=\"688pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1296)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1296 684,-1296 684,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140176993877576 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140176993877576</title>\n",
       "<polygon fill=\"none\" points=\"40.5,-1245.5 40.5,-1291.5 332.5,-1291.5 332.5,-1245.5 40.5,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"110.5\" y=\"-1264.8\">input_fwd: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"180.5,-1245.5 180.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"208\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"180.5,-1268.5 235.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"208\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"235.5,-1245.5 235.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"284\" y=\"-1276.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"235.5,-1268.5 332.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"284\" y=\"-1253.3\">(None, 200, 4)</text>\n",
       "</g>\n",
       "<!-- 140177099524416 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140177099524416</title>\n",
       "<polygon fill=\"none\" points=\"177,-1162.5 177,-1208.5 504,-1208.5 504,-1162.5 177,-1162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-1181.8\">convolution_layer: Conv1D</text>\n",
       "<polyline fill=\"none\" points=\"345,-1162.5 345,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1193.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"345,-1185.5 400,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.5\" y=\"-1170.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"400,-1162.5 400,-1208.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-1193.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"400,-1185.5 504,-1185.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452\" y=\"-1170.3\">(None, 200, 25)</text>\n",
       "</g>\n",
       "<!-- 140176993877576&#45;&gt;140177099524416 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140176993877576-&gt;140177099524416</title>\n",
       "<path d=\"M228.581,-1245.37C247.425,-1235.46 269.808,-1223.68 289.587,-1213.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"291.249,-1216.36 298.47,-1208.61 287.99,-1210.16 291.249,-1216.36\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176993874776 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140176993874776</title>\n",
       "<polygon fill=\"none\" points=\"350.5,-1245.5 350.5,-1291.5 638.5,-1291.5 638.5,-1245.5 350.5,-1245.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-1264.8\">input_rev: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"486.5,-1245.5 486.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"514\" y=\"-1276.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"486.5,-1268.5 541.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"514\" y=\"-1253.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"541.5,-1245.5 541.5,-1291.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"590\" y=\"-1276.3\">(None, 200, 4)</text>\n",
       "<polyline fill=\"none\" points=\"541.5,-1268.5 638.5,-1268.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"590\" y=\"-1253.3\">(None, 200, 4)</text>\n",
       "</g>\n",
       "<!-- 140176993874776&#45;&gt;140177099524416 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140176993874776-&gt;140177099524416</title>\n",
       "<path d=\"M452.419,-1245.37C433.575,-1235.46 411.192,-1223.68 391.413,-1213.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"393.01,-1210.16 382.53,-1208.61 389.751,-1216.36 393.01,-1210.16\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177433522752 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140177433522752</title>\n",
       "<polygon fill=\"none\" points=\"186.5,-1079.5 186.5,-1125.5 494.5,-1125.5 494.5,-1079.5 186.5,-1079.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-1098.8\">crop_layer: Cropping1D</text>\n",
       "<polyline fill=\"none\" points=\"335.5,-1079.5 335.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-1110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"335.5,-1102.5 390.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-1087.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"390.5,-1079.5 390.5,-1125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442.5\" y=\"-1110.3\">(None, 200, 25)</text>\n",
       "<polyline fill=\"none\" points=\"390.5,-1102.5 494.5,-1102.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442.5\" y=\"-1087.3\">(None, 150, 25)</text>\n",
       "</g>\n",
       "<!-- 140177099524416&#45;&gt;140177433522752 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140177099524416-&gt;140177433522752</title>\n",
       "<path d=\"M340.5,-1162.37C340.5,-1154.15 340.5,-1144.66 340.5,-1135.73\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"344,-1135.61 340.5,-1125.61 337,-1135.61 344,-1135.61\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140175246389600 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140175246389600</title>\n",
       "<polygon fill=\"none\" points=\"116,-996.5 116,-1042.5 397,-1042.5 397,-996.5 116,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-1015.8\">flip_layer: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"238,-996.5 238,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.5\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"238,-1019.5 293,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265.5\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"293,-996.5 293,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345\" y=\"-1027.3\">(None, 150, 25)</text>\n",
       "<polyline fill=\"none\" points=\"293,-1019.5 397,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345\" y=\"-1004.3\">(None, 150, 25)</text>\n",
       "</g>\n",
       "<!-- 140177433522752&#45;&gt;140175246389600 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140177433522752-&gt;140175246389600</title>\n",
       "<path d=\"M317.547,-1079.37C308.009,-1070.17 296.809,-1059.37 286.624,-1049.55\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"289.054,-1047.03 279.426,-1042.61 284.195,-1052.07 289.054,-1047.03\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177432512272 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140177432512272</title>\n",
       "<polygon fill=\"none\" points=\"115.5,-913.5 115.5,-959.5 565.5,-959.5 565.5,-913.5 115.5,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"209.5\" y=\"-932.8\">concatenate_layer: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"303.5,-913.5 303.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"331\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"303.5,-936.5 358.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"331\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"358.5,-913.5 358.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"462\" y=\"-944.3\">[(None, 150, 25), (None, 150, 25)]</text>\n",
       "<polyline fill=\"none\" points=\"358.5,-936.5 565.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"462\" y=\"-921.3\">(None, 150, 50)</text>\n",
       "</g>\n",
       "<!-- 140177433522752&#45;&gt;140177432512272 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140177433522752-&gt;140177432512272</title>\n",
       "<path d=\"M375.24,-1079.37C387.144,-1069.78 399.076,-1057.42 405.5,-1043 413.998,-1023.92 413.998,-1015.08 405.5,-996 400.481,-984.732 392.101,-974.723 382.998,-966.31\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"385.1,-963.501 375.24,-959.626 380.531,-968.804 385.1,-963.501\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140175246389600&#45;&gt;140177432512272 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140175246389600-&gt;140177432512272</title>\n",
       "<path d=\"M279.453,-996.366C288.991,-987.169 300.191,-976.369 310.376,-966.548\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"312.805,-969.068 317.574,-959.607 307.946,-964.029 312.805,-969.068\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177432512216 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140177432512216</title>\n",
       "<polygon fill=\"none\" points=\"141.5,-830.5 141.5,-876.5 539.5,-876.5 539.5,-830.5 141.5,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-849.8\">sequence_pooling_layer: MaxPooling1D</text>\n",
       "<polyline fill=\"none\" points=\"380.5,-830.5 380.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"408\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"380.5,-853.5 435.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"408\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"435.5,-830.5 435.5,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"487.5\" y=\"-861.3\">(None, 150, 50)</text>\n",
       "<polyline fill=\"none\" points=\"435.5,-853.5 539.5,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"487.5\" y=\"-838.3\">(None, 150, 50)</text>\n",
       "</g>\n",
       "<!-- 140177432512272&#45;&gt;140177432512216 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140177432512272-&gt;140177432512216</title>\n",
       "<path d=\"M340.5,-913.366C340.5,-905.152 340.5,-895.658 340.5,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"344,-886.607 340.5,-876.607 337,-886.607 344,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177099553816 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140177099553816</title>\n",
       "<polygon fill=\"none\" points=\"0,-747.5 0,-793.5 333,-793.5 333,-747.5 0,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84\" y=\"-766.8\">forward_lstm_layer: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"168,-747.5 168,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"195.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"168,-770.5 223,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"195.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"223,-747.5 223,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-778.3\">(None, 150, 50)</text>\n",
       "<polyline fill=\"none\" points=\"223,-770.5 333,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-755.3\">(None, 150, 150)</text>\n",
       "</g>\n",
       "<!-- 140177432512216&#45;&gt;140177099553816 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140177432512216-&gt;140177099553816</title>\n",
       "<path d=\"M292.954,-830.366C271.375,-820.321 245.688,-808.363 223.12,-797.858\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.532,-794.654 213.989,-793.607 221.577,-801 224.532,-794.654\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176997364624 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140176997364624</title>\n",
       "<polygon fill=\"none\" points=\"351,-747.5 351,-793.5 680,-793.5 680,-747.5 351,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433\" y=\"-766.8\">reverse_lstm_layer: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"515,-747.5 515,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"542.5\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"515,-770.5 570,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"542.5\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"570,-747.5 570,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-778.3\">(None, 150, 50)</text>\n",
       "<polyline fill=\"none\" points=\"570,-770.5 680,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"625\" y=\"-755.3\">(None, 150, 150)</text>\n",
       "</g>\n",
       "<!-- 140177432512216&#45;&gt;140176997364624 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140177432512216-&gt;140176997364624</title>\n",
       "<path d=\"M388.32,-830.366C410.022,-820.321 435.857,-808.363 458.554,-797.858\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"460.133,-800.984 467.738,-793.607 457.193,-794.631 460.133,-800.984\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177437375008 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140177437375008</title>\n",
       "<polygon fill=\"none\" points=\"115,-664.5 115,-710.5 566,-710.5 566,-664.5 115,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-683.8\">concatenate_65: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"290,-664.5 290,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317.5\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"290,-687.5 345,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317.5\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"345,-664.5 345,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"455.5\" y=\"-695.3\">[(None, 150, 150), (None, 150, 150)]</text>\n",
       "<polyline fill=\"none\" points=\"345,-687.5 566,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"455.5\" y=\"-672.3\">(None, 150, 300)</text>\n",
       "</g>\n",
       "<!-- 140177099553816&#45;&gt;140177437375008 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140177099553816-&gt;140177437375008</title>\n",
       "<path d=\"M214.046,-747.366C235.625,-737.321 261.312,-725.363 283.88,-714.858\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"285.423,-718 293.011,-710.607 282.468,-711.654 285.423,-718\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176997364624&#45;&gt;140177437375008 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140176997364624-&gt;140177437375008</title>\n",
       "<path d=\"M467.68,-747.366C445.978,-737.321 420.143,-725.363 397.446,-714.858\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"398.807,-711.631 388.262,-710.607 395.867,-717.984 398.807,-711.631\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177432687840 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140177432687840</title>\n",
       "<polygon fill=\"none\" points=\"70.5,-581.5 70.5,-627.5 404.5,-627.5 404.5,-581.5 70.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155\" y=\"-600.8\">attention_tanh_layer: Dense</text>\n",
       "<polyline fill=\"none\" points=\"239.5,-581.5 239.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"239.5,-604.5 294.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"267\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"294.5,-581.5 294.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"349.5\" y=\"-612.3\">(None, 150, 300)</text>\n",
       "<polyline fill=\"none\" points=\"294.5,-604.5 404.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"349.5\" y=\"-589.3\">(None, 150, 200)</text>\n",
       "</g>\n",
       "<!-- 140177437375008&#45;&gt;140177432687840 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>140177437375008-&gt;140177432687840</title>\n",
       "<path d=\"M312.355,-664.366C300.433,-654.991 286.392,-643.949 273.712,-633.977\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"275.635,-631.037 265.611,-627.607 271.308,-636.54 275.635,-631.037\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177440096552 -->\n",
       "<g class=\"node\" id=\"node14\"><title>140177440096552</title>\n",
       "<polygon fill=\"none\" points=\"137.5,-332.5 137.5,-378.5 527.5,-378.5 527.5,-332.5 137.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"201.5\" y=\"-351.8\">attending_layer: Dot</text>\n",
       "<polyline fill=\"none\" points=\"265.5,-332.5 265.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"265.5,-355.5 320.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"293\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"320.5,-332.5 320.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424\" y=\"-363.3\">[(None, 150, 1), (None, 150, 300)]</text>\n",
       "<polyline fill=\"none\" points=\"320.5,-355.5 527.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424\" y=\"-340.3\">(None, 1, 300)</text>\n",
       "</g>\n",
       "<!-- 140177437375008&#45;&gt;140177440096552 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>140177437375008-&gt;140177440096552</title>\n",
       "<path d=\"M380.472,-664.348C393.432,-654.96 406.311,-642.744 413.5,-628 454.988,-542.904 459.084,-499.041 415.5,-415 409.226,-402.903 399.174,-392.778 388.198,-384.537\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"389.877,-381.442 379.671,-378.6 385.878,-387.186 389.877,-381.442\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176993744936 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140176993744936</title>\n",
       "<polygon fill=\"none\" points=\"63.5,-498.5 63.5,-544.5 401.5,-544.5 401.5,-498.5 63.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150\" y=\"-517.8\">attention_outer_layer: Dense</text>\n",
       "<polyline fill=\"none\" points=\"236.5,-498.5 236.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236.5,-521.5 291.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291.5,-498.5 291.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346.5\" y=\"-529.3\">(None, 150, 200)</text>\n",
       "<polyline fill=\"none\" points=\"291.5,-521.5 401.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"346.5\" y=\"-506.3\">(None, 150, 1)</text>\n",
       "</g>\n",
       "<!-- 140177432687840&#45;&gt;140176993744936 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>140177432687840-&gt;140176993744936</title>\n",
       "<path d=\"M236.134,-581.366C235.627,-573.152 235.041,-563.658 234.489,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"237.974,-554.372 233.865,-544.607 230.987,-554.804 237.974,-554.372\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176993661952 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140176993661952</title>\n",
       "<polygon fill=\"none\" points=\"52,-415.5 52,-461.5 407,-461.5 407,-415.5 52,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"153.5\" y=\"-434.8\">attention_softmax_layer: Softmax</text>\n",
       "<polyline fill=\"none\" points=\"255,-415.5 255,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"255,-438.5 310,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"282.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"310,-415.5 310,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"358.5\" y=\"-446.3\">(None, 150, 1)</text>\n",
       "<polyline fill=\"none\" points=\"310,-438.5 407,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"358.5\" y=\"-423.3\">(None, 150, 1)</text>\n",
       "</g>\n",
       "<!-- 140176993744936&#45;&gt;140176993661952 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>140176993744936-&gt;140176993661952</title>\n",
       "<path d=\"M231.68,-498.366C231.376,-490.152 231.024,-480.658 230.694,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"234.187,-471.47 230.319,-461.607 227.191,-471.73 234.187,-471.47\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176993661952&#45;&gt;140177440096552 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>140176993661952-&gt;140177440096552</title>\n",
       "<path d=\"M257.645,-415.366C269.567,-405.991 283.608,-394.949 296.288,-384.977\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"298.692,-387.54 304.389,-378.607 294.365,-382.037 298.692,-387.54\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176992529712 -->\n",
       "<g class=\"node\" id=\"node15\"><title>140176992529712</title>\n",
       "<polygon fill=\"none\" points=\"195.5,-249.5 195.5,-295.5 469.5,-295.5 469.5,-249.5 195.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-268.8\">dense_layer: Dense</text>\n",
       "<polyline fill=\"none\" points=\"317.5,-249.5 317.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"317.5,-272.5 372.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"345\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"372.5,-249.5 372.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-280.3\">(None, 1, 300)</text>\n",
       "<polyline fill=\"none\" points=\"372.5,-272.5 469.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-257.3\">(None, 1, 500)</text>\n",
       "</g>\n",
       "<!-- 140177440096552&#45;&gt;140176992529712 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>140177440096552-&gt;140176992529712</title>\n",
       "<path d=\"M332.5,-332.366C332.5,-324.152 332.5,-314.658 332.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"336,-305.607 332.5,-295.607 329,-305.607 336,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176993713624 -->\n",
       "<g class=\"node\" id=\"node16\"><title>140176993713624</title>\n",
       "<polygon fill=\"none\" points=\"182,-166.5 182,-212.5 483,-212.5 483,-166.5 182,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-185.8\">dense_dropout: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"331,-166.5 331,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"358.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"331,-189.5 386,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"358.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"386,-166.5 386,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434.5\" y=\"-197.3\">(None, 1, 500)</text>\n",
       "<polyline fill=\"none\" points=\"386,-189.5 483,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434.5\" y=\"-174.3\">(None, 1, 500)</text>\n",
       "</g>\n",
       "<!-- 140176992529712&#45;&gt;140176993713624 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>140176992529712-&gt;140176993713624</title>\n",
       "<path d=\"M332.5,-249.366C332.5,-241.152 332.5,-231.658 332.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"336,-222.607 332.5,-212.607 329,-222.607 336,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140177432656584 -->\n",
       "<g class=\"node\" id=\"node17\"><title>140177432656584</title>\n",
       "<polygon fill=\"none\" points=\"208.5,-83.5 208.5,-129.5 456.5,-129.5 456.5,-83.5 208.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256.5\" y=\"-102.8\">flatten: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"304.5,-83.5 304.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"304.5,-106.5 359.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"359.5,-83.5 359.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"408\" y=\"-114.3\">(None, 1, 500)</text>\n",
       "<polyline fill=\"none\" points=\"359.5,-106.5 456.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"408\" y=\"-91.3\">(None, 500)</text>\n",
       "</g>\n",
       "<!-- 140176993713624&#45;&gt;140177432656584 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>140176993713624-&gt;140177432656584</title>\n",
       "<path d=\"M332.5,-166.366C332.5,-158.152 332.5,-148.658 332.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"336,-139.607 332.5,-129.607 329,-139.607 336,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140176994010560 -->\n",
       "<g class=\"node\" id=\"node18\"><title>140176994010560</title>\n",
       "<polygon fill=\"none\" points=\"204.5,-0.5 204.5,-46.5 460.5,-46.5 460.5,-0.5 204.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-19.8\">predictions: Dense</text>\n",
       "<polyline fill=\"none\" points=\"322.5,-0.5 322.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"322.5,-23.5 377.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"377.5,-0.5 377.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"419\" y=\"-31.3\">(None, 500)</text>\n",
       "<polyline fill=\"none\" points=\"377.5,-23.5 460.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"419\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 140177432656584&#45;&gt;140176994010560 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>140177432656584-&gt;140176994010560</title>\n",
       "<path d=\"M332.5,-83.3664C332.5,-75.1516 332.5,-65.6579 332.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"336,-56.6068 332.5,-46.6068 329,-56.6069 336,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(attention_model, to_file='attention_model.pdf')\n",
    "SVG(model_to_dot(attention_model, show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_attention_model = keras.utils.multi_gpu_model(attention_model, gpus=2)\n",
    "parallel_attention_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parallel_attention_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=200,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = parallel_attention_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507857947721864"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = attention_model.predict([x_test, x_rc_test])\n",
    "\n",
    "sklearn.metrics.roc_auc_score([y[1] for y in y_test], probs[:,1], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcsAAABOCAYAAAD/0U4aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f066cfe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_output, full_attention, attended_sequence = get_sequence_attention(attention_model,\n",
    "                      str(positive_seqRecords[2].seq),\n",
    "                      5)\n",
    "frame = pd.DataFrame(full_attention).T\n",
    "frame.columns = list(attended_sequence)\n",
    "fig, ax = plt.subplots(figsize=(30,1)) \n",
    "hm=sns.heatmap(frame, square=True, ax=ax, vmin=0, vmax=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcsAAABOCAYAAAD/0U4aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f06083160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_output, full_attention, attended_sequence = get_sequence_attention(attention_model,\n",
    "                      str(positive_seqRecords[9000].seq),\n",
    "                      5)\n",
    "frame = pd.DataFrame(full_attention).T\n",
    "frame.columns = list(attended_sequence)\n",
    "fig, ax = plt.subplots(figsize=(30,1)) \n",
    "hm=sns.heatmap(frame, square=True, ax=ax, vmin=0, vmax=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABcsAAABOCAYAAAD/0U4aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZc3er9GlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7f056d65f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_output, full_attention, attended_sequence = get_sequence_attention(attention_model,\n",
    "                      str(positive_seqRecords[999].seq),\n",
    "                      5)\n",
    "frame = pd.DataFrame(full_attention).T\n",
    "frame.columns = list(attended_sequence)\n",
    "fig, ax = plt.subplots(figsize=(30,1)) \n",
    "hm=sns.heatmap(frame, square=True, ax=ax, vmin=0, vmax=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence_attention(model, sequence, adjacent_bp_pool_size):\n",
    "    get_attention = K.function([model.get_layer('input_fwd').input, \n",
    "                                model.get_layer('input_rev').input,\n",
    "                                K.learning_phase()\n",
    "                               ], \n",
    "                               [model.get_layer('attention_softmax_layer').output])\n",
    "    fwd_seq = sequence[:200]\n",
    "    rev_seq = str(Bio.Seq.Seq(fwd_seq).reverse_complement())\n",
    "    \n",
    "    fwd_seq_array = convert_sequences_to_array([fwd_seq])[0]\n",
    "    rev_seq_array = convert_sequences_to_array([rev_seq])[0]\n",
    "\n",
    "    layer_output = get_attention(([fwd_seq_array], [rev_seq_array], 0))[0]\n",
    "    reshaped_output = layer_output.reshape((layer_output.shape[1], layer_output.shape[2]))\n",
    "\n",
    "\n",
    "    full_attention = []\n",
    "    for x in reshaped_output:\n",
    "        for i in range(adjacent_bp_pool_size):\n",
    "            full_attention.append(x)\n",
    "    full_attention = np.array(full_attention)\n",
    "\n",
    "    crop_distance = int((len(fwd_seq) - full_attention.shape[0])/2)\n",
    "\n",
    "    attended_sequence = fwd_seq[crop_distance:-crop_distance]\n",
    "    return layer_output, full_attention, attended_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_arrays_to_sequences(sequence_arrays):\n",
    "    sequence_list = []\n",
    "    for arr in sequence_arrays:\n",
    "        current_seq = ''\n",
    "        for pos in arr:\n",
    "            if int(pos[0]) == 1:\n",
    "                current_seq += 'A'\n",
    "            elif int(pos[1]) == 1:\n",
    "                current_seq += 'C'\n",
    "            elif int(pos[2]) == 1:\n",
    "                current_seq += 'G'\n",
    "            elif int(pos[3]) == 1:\n",
    "                current_seq += 'T'\n",
    "            else:\n",
    "                current_seq += 'N'\n",
    "        sequence_list.append(current_seq)\n",
    "    return sequence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models For each signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attention_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size,\n",
    "                        adjacent_bp_pool_size,\n",
    "                        attention_dim,\n",
    "                        attention_hops,\n",
    "                        num_dense_neurons):\n",
    "    input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "    input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "    ### find motifs ###\n",
    "    convolution_layer = Conv1D(filters=num_motifs, \n",
    "        kernel_size=motif_size,\n",
    "        activation='relu',\n",
    "        input_shape=(total_seq_length,4),\n",
    "        name='convolution_layer',\n",
    "        padding = 'same'\n",
    "        )\n",
    "    forward_motif_scores = convolution_layer(input_fwd)\n",
    "    reverse_motif_scores = convolution_layer(input_rev)\n",
    "    print('forward_motif_scores', forward_motif_scores.get_shape())\n",
    "\n",
    "    ### crop motif scores to avoid parts of sequence where motif score is computed in only one direction ###\n",
    "    to_crop = int((total_seq_length - seq_size)/2)\n",
    "    crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "        name='crop_layer')\n",
    "    cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "    cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "    print('cropped_fwd_scores', cropped_fwd_scores.get_shape())\n",
    "\n",
    "    ### flip motif scores ###\n",
    "    flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "        output_shape=(seq_size, num_motifs),\n",
    "        name='flip_layer')\n",
    "    flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "    print('flipped_rev_scores', flipped_rev_scores.get_shape())\n",
    "\n",
    "    ### concatenate motif scores ###\n",
    "    concatenate_layer = keras.layers.Concatenate(axis=2, name='concatenate_layer')\n",
    "    concatenated_motif_scores = concatenate_layer([cropped_fwd_scores, flipped_rev_scores])\n",
    "    print('concatenated_motif_scores', concatenated_motif_scores.get_shape())\n",
    "\n",
    "    ### pool across length of sequence ###\n",
    "    sequence_pooling_layer = MaxPool1D(pool_size=adjacent_bp_pool_size, \n",
    "        strides=adjacent_bp_pool_size,\n",
    "        name='sequence_pooling_layer')\n",
    "    pooled_scores = sequence_pooling_layer(concatenated_motif_scores)\n",
    "    print('pooled_scores', pooled_scores.get_shape())\n",
    "\n",
    "    ## bidirectional LSTM ###\n",
    "    forward_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "        return_sequences=True,\n",
    "        input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "        name = 'forward_lstm_layer'\n",
    "        )\n",
    "    forward_hidden_states = forward_lstm_layer(pooled_scores)\n",
    "    print('forward_hidden_states', forward_hidden_states.get_shape())\n",
    "\n",
    "    reverse_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "        return_sequences=True,\n",
    "        input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "        name = 'reverse_lstm_layer',\n",
    "        go_backwards=True,\n",
    "        )\n",
    "    reverse_hidden_states = reverse_lstm_layer(pooled_scores)\n",
    "    print('reverse_hidden_states', reverse_hidden_states.get_shape())\n",
    "    ### concatenate lstm hidden states ###\n",
    "    lstm_concatenate_layer = Concatenate(axis=2)\n",
    "    bilstm_hidden_states = lstm_concatenate_layer([forward_hidden_states, reverse_hidden_states])\n",
    "\n",
    "    print('bilstm_hidden_states', bilstm_hidden_states.get_shape())\n",
    "\n",
    "    # tanh layer\n",
    "    attention_tanh_layer = Dense(attention_dim,\n",
    "        activation='tanh',\n",
    "        use_bias=False,\n",
    "        name = 'attention_tanh_layer')\n",
    "    attention_tanh_layer_out = attention_tanh_layer(bilstm_hidden_states)\n",
    "    print('attention_tanh_layer_out', attention_tanh_layer_out.get_shape())\n",
    "\n",
    "    # outer layer\n",
    "    attention_outer_layer = Dense(attention_hops,\n",
    "        activation='relu',\n",
    "        use_bias=False,\n",
    "        name = 'attention_outer_layer')\n",
    "    attention_outer_layer_out = attention_outer_layer(attention_tanh_layer_out)\n",
    "    print('attention_outer_layer_out', attention_outer_layer_out.get_shape())\n",
    "\n",
    "    # apply softmax\n",
    "    softmax_layer = Softmax(axis=1, name='attention_softmax_layer')\n",
    "    attention_softmax_layer_out = softmax_layer(attention_outer_layer_out)\n",
    "    print('attention_softmax_layer_out', attention_softmax_layer_out.get_shape())\n",
    "\n",
    "    # attend to hidden states\n",
    "    attending_layer = Dot(axes=(1,1),\n",
    "        name='attending_layer')\n",
    "\n",
    "    attended_states = attending_layer([attention_softmax_layer_out, bilstm_hidden_states])\n",
    "    print('attended_states', attended_states.get_shape())\n",
    "\n",
    "    # # fully connected layer\n",
    "    dense_layer = Dense(num_dense_neurons, \n",
    "        activation='relu', \n",
    "        name = 'dense_layer'\n",
    "        )\n",
    "\n",
    "    dense_output = dense_layer(attended_states)\n",
    "\n",
    "    # drop out\n",
    "    drop_out = Dropout(0.25,name='dense_dropout')(dense_output)\n",
    "\n",
    "    # make prediction\n",
    "    flattened = Flatten(name='flatten')(drop_out)\n",
    "    predictions = Dense(num_classes,\n",
    "                        name='predictions',\n",
    "                        activation = 'softmax', \n",
    "                       )(flattened)\n",
    "    # define and compile model\n",
    "    model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Params: 3000\n",
      "LSTM Params: 24000.0\n",
      "Attention Params: 6200.0\n",
      "Dense Params: 15000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48200.0"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_seq_length = len(fasta_seq[0])\n",
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 50\n",
    "motif_size = 15\n",
    "adjacent_bp_pool_size = 5\n",
    "attention_dim = 200\n",
    "attention_hops = 1\n",
    "num_dense_neurons = 500 \n",
    "count_num_params(seq_size,\n",
    "    num_motifs, \n",
    "    motif_size,\n",
    "    adjacent_bp_pool_size,\n",
    "    attention_dim,\n",
    "    attention_hops,\n",
    "    num_dense_neurons\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_seqRecords = list(SeqIO.parse('./fasta_files/c57bl6_kla-1h_peaks.fa', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_kla-1h_peaks_background.fasta', 'fasta'))\n",
    "\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords] + [str(x[:200].seq) for x in negative_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords] + \\\n",
    "    [str(x[:200].reverse_complement().seq) for x in negative_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "sequence_rc_arrays = np.array(sequence_rc_arrays)\n",
    "\n",
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)\n",
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 50)\n",
      "cropped_fwd_scores (?, 150, 50)\n",
      "flipped_rev_scores (?, 150, 50)\n",
      "concatenated_motif_scores (?, 150, 100)\n",
      "pooled_scores (?, 30, 100)\n",
      "forward_hidden_states (?, ?, 30)\n",
      "reverse_hidden_states (?, ?, 30)\n",
      "bilstm_hidden_states (?, ?, 60)\n",
      "attention_tanh_layer_out (?, 30, 200)\n",
      "attention_outer_layer_out (?, 30, 1)\n",
      "attention_softmax_layer_out (?, 30, 1)\n",
      "attended_states (?, 1, 60)\n"
     ]
    }
   ],
   "source": [
    "kla_model = get_attention_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size,\n",
    "                        adjacent_bp_pool_size,\n",
    "                        attention_dim,\n",
    "                        attention_hops,\n",
    "                        num_dense_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_attention_model = keras.utils.multi_gpu_model(kla_model, gpus=2)\n",
    "parallel_attention_model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parallel_attention_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=200,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = parallel_attention_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = kla_model.to_json()\n",
    "with open(\"kla_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "kla_model.save_weights(\"kla_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veh Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_seqRecords = list(SeqIO.parse('./fasta_files/c57bl6_veh_peaks.fa', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_veh_peaks_background.fasta', 'fasta'))\n",
    "\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords] + [str(x[:200].seq) for x in negative_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords] + \\\n",
    "    [str(x[:200].reverse_complement().seq) for x in negative_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "sequence_rc_arrays = np.array(sequence_rc_arrays)\n",
    "\n",
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)\n",
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 25)\n",
      "cropped_fwd_scores (?, 150, 25)\n",
      "flipped_rev_scores (?, 150, 25)\n",
      "concatenated_motif_scores (?, 150, 50)\n",
      "pooled_scores (?, 150, 50)\n",
      "forward_hidden_states (?, ?, 150)\n",
      "reverse_hidden_states (?, ?, 150)\n",
      "bilstm_hidden_states (?, ?, 300)\n",
      "attention_tanh_layer_out (?, 150, 200)\n",
      "attention_outer_layer_out (?, 150, 1)\n",
      "attention_softmax_layer_out (?, 150, 1)\n",
      "attended_states (?, 1, 300)\n"
     ]
    }
   ],
   "source": [
    "veh_model = get_attention_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size,\n",
    "                        adjacent_bp_pool_size,\n",
    "                        attention_dim,\n",
    "                        attention_hops,\n",
    "                        num_dense_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_attention_model = keras.utils.multi_gpu_model(veh_model, gpus=2)\n",
    "parallel_attention_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43225 samples, validate on 10807 samples\n",
      "Epoch 1/50\n",
      "43225/43225 [==============================] - 28s 638us/step - loss: 0.6533 - acc: 0.5935 - val_loss: 0.5984 - val_acc: 0.6762\n",
      "Epoch 2/50\n",
      "43225/43225 [==============================] - 16s 366us/step - loss: 0.5654 - acc: 0.7057 - val_loss: 0.5534 - val_acc: 0.7206\n",
      "Epoch 3/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.5398 - acc: 0.7260 - val_loss: 0.5339 - val_acc: 0.7274\n",
      "Epoch 4/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.5056 - acc: 0.7514 - val_loss: 0.5622 - val_acc: 0.7218\n",
      "Epoch 5/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.4841 - acc: 0.7652 - val_loss: 0.4749 - val_acc: 0.7726\n",
      "Epoch 6/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.4551 - acc: 0.7863 - val_loss: 0.4601 - val_acc: 0.7806\n",
      "Epoch 7/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.4396 - acc: 0.7945 - val_loss: 0.4378 - val_acc: 0.7958\n",
      "Epoch 8/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.4211 - acc: 0.8049 - val_loss: 0.4290 - val_acc: 0.8012\n",
      "Epoch 9/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.4051 - acc: 0.8168 - val_loss: 0.4294 - val_acc: 0.8002\n",
      "Epoch 10/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.3916 - acc: 0.8247 - val_loss: 0.4152 - val_acc: 0.8129\n",
      "Epoch 11/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.3921 - acc: 0.8226 - val_loss: 0.4349 - val_acc: 0.8013\n",
      "Epoch 12/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3826 - acc: 0.8286 - val_loss: 0.4036 - val_acc: 0.8171\n",
      "Epoch 13/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3724 - acc: 0.8335 - val_loss: 0.4126 - val_acc: 0.8073\n",
      "Epoch 14/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.3731 - acc: 0.8342 - val_loss: 0.4215 - val_acc: 0.8072\n",
      "Epoch 15/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3633 - acc: 0.8390 - val_loss: 0.4693 - val_acc: 0.7818\n",
      "Epoch 16/50\n",
      "43225/43225 [==============================] - 16s 366us/step - loss: 0.3621 - acc: 0.8399 - val_loss: 0.4239 - val_acc: 0.8081\n",
      "Epoch 17/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.3552 - acc: 0.8421 - val_loss: 0.3856 - val_acc: 0.8301\n",
      "Epoch 18/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3520 - acc: 0.8440 - val_loss: 0.4039 - val_acc: 0.8171\n",
      "Epoch 19/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3465 - acc: 0.8485 - val_loss: 0.3890 - val_acc: 0.8298\n",
      "Epoch 20/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.3471 - acc: 0.8480 - val_loss: 0.3822 - val_acc: 0.8300\n",
      "Epoch 21/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3465 - acc: 0.8467 - val_loss: 0.4183 - val_acc: 0.8053\n",
      "Epoch 22/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3378 - acc: 0.8528 - val_loss: 0.4145 - val_acc: 0.8190\n",
      "Epoch 23/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3403 - acc: 0.8523 - val_loss: 0.4026 - val_acc: 0.8160\n",
      "Epoch 24/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3400 - acc: 0.8524 - val_loss: 0.4068 - val_acc: 0.8164\n",
      "Epoch 25/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3360 - acc: 0.8530 - val_loss: 0.3837 - val_acc: 0.8324\n",
      "Epoch 26/50\n",
      "43225/43225 [==============================] - 16s 361us/step - loss: 0.3272 - acc: 0.8589 - val_loss: 0.3951 - val_acc: 0.8265\n",
      "Epoch 27/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3312 - acc: 0.8582 - val_loss: 0.4215 - val_acc: 0.8130\n",
      "Epoch 28/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3279 - acc: 0.8582 - val_loss: 0.3869 - val_acc: 0.8271\n",
      "Epoch 29/50\n",
      "43225/43225 [==============================] - 16s 366us/step - loss: 0.3247 - acc: 0.8597 - val_loss: 0.3718 - val_acc: 0.8380\n",
      "Epoch 30/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3220 - acc: 0.8617 - val_loss: 0.4990 - val_acc: 0.7981\n",
      "Epoch 31/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3228 - acc: 0.8622 - val_loss: 0.3822 - val_acc: 0.8373\n",
      "Epoch 32/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3162 - acc: 0.8655 - val_loss: 0.4096 - val_acc: 0.8272\n",
      "Epoch 33/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3146 - acc: 0.8650 - val_loss: 0.3756 - val_acc: 0.8399\n",
      "Epoch 34/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.3113 - acc: 0.8668 - val_loss: 0.3807 - val_acc: 0.8347\n",
      "Epoch 35/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3093 - acc: 0.8673 - val_loss: 0.3777 - val_acc: 0.8393\n",
      "Epoch 36/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.3095 - acc: 0.8676 - val_loss: 0.4057 - val_acc: 0.8327\n",
      "Epoch 37/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.3067 - acc: 0.8690 - val_loss: 0.3957 - val_acc: 0.8266\n",
      "Epoch 38/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3035 - acc: 0.8702 - val_loss: 0.3793 - val_acc: 0.8374\n",
      "Epoch 39/50\n",
      "43225/43225 [==============================] - 16s 366us/step - loss: 0.3013 - acc: 0.8731 - val_loss: 0.3710 - val_acc: 0.8436\n",
      "Epoch 40/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.3006 - acc: 0.8720 - val_loss: 0.3770 - val_acc: 0.8383\n",
      "Epoch 41/50\n",
      "43225/43225 [==============================] - 16s 367us/step - loss: 0.2980 - acc: 0.8734 - val_loss: 0.4191 - val_acc: 0.8194\n",
      "Epoch 42/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.2996 - acc: 0.8733 - val_loss: 0.3699 - val_acc: 0.8408\n",
      "Epoch 43/50\n",
      "43225/43225 [==============================] - 16s 363us/step - loss: 0.2933 - acc: 0.8760 - val_loss: 0.3869 - val_acc: 0.8411\n",
      "Epoch 44/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.2899 - acc: 0.8763 - val_loss: 0.3895 - val_acc: 0.8306\n",
      "Epoch 45/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.2973 - acc: 0.8738 - val_loss: 0.3746 - val_acc: 0.8422\n",
      "Epoch 46/50\n",
      "43225/43225 [==============================] - 16s 364us/step - loss: 0.2870 - acc: 0.8791 - val_loss: 0.4290 - val_acc: 0.8185\n",
      "Epoch 47/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.2859 - acc: 0.8792 - val_loss: 0.3821 - val_acc: 0.8329\n",
      "Epoch 48/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.2843 - acc: 0.8812 - val_loss: 0.3850 - val_acc: 0.8395\n",
      "Epoch 49/50\n",
      "43225/43225 [==============================] - 16s 362us/step - loss: 0.2840 - acc: 0.8797 - val_loss: 0.3920 - val_acc: 0.8285\n",
      "Epoch 50/50\n",
      "43225/43225 [==============================] - 16s 365us/step - loss: 0.2815 - acc: 0.8814 - val_loss: 0.3968 - val_acc: 0.8389\n",
      "Test loss: 0.39790159156400356\n",
      "Test accuracy: 0.83723512542582\n"
     ]
    }
   ],
   "source": [
    "parallel_attention_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=200,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = parallel_attention_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = veh_model.to_json()\n",
    "with open(\"veh_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "veh_model.save_weights(\"veh_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IL4 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_seqRecords = list(SeqIO.parse('./fasta_files/c57bl6_il4-24h_peaks.fa', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_il4-24h_peaks_background.fasta', 'fasta'))\n",
    "\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords] + [str(x[:200].seq) for x in negative_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords] + \\\n",
    "    [str(x[:200].reverse_complement().seq) for x in negative_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "sequence_rc_arrays = np.array(sequence_rc_arrays)\n",
    "\n",
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)\n",
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 50)\n",
      "cropped_fwd_scores (?, 150, 50)\n",
      "flipped_rev_scores (?, 150, 50)\n",
      "concatenated_motif_scores (?, 150, 100)\n",
      "pooled_scores (?, 30, 100)\n",
      "forward_hidden_states (?, ?, 30)\n",
      "reverse_hidden_states (?, ?, 30)\n",
      "bilstm_hidden_states (?, ?, 60)\n",
      "attention_tanh_layer_out (?, 30, 200)\n",
      "attention_outer_layer_out (?, 30, 1)\n",
      "attention_softmax_layer_out (?, 30, 1)\n",
      "attended_states (?, 1, 60)\n"
     ]
    }
   ],
   "source": [
    "il4_model = get_attention_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size,\n",
    "                        adjacent_bp_pool_size,\n",
    "                        attention_dim,\n",
    "                        attention_hops,\n",
    "                        num_dense_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_attention_model = keras.utils.multi_gpu_model(il4_model, gpus=2)\n",
    "parallel_attention_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42678 samples, validate on 10670 samples\n",
      "Epoch 1/50\n",
      "42678/42678 [==============================] - 28s 648us/step - loss: 0.6619 - acc: 0.5795 - val_loss: 0.5859 - val_acc: 0.6894\n",
      "Epoch 2/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.5641 - acc: 0.7090 - val_loss: 0.5341 - val_acc: 0.7312\n",
      "Epoch 3/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.5314 - acc: 0.7340 - val_loss: 0.5078 - val_acc: 0.7522\n",
      "Epoch 4/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.5016 - acc: 0.7546 - val_loss: 0.4801 - val_acc: 0.7705\n",
      "Epoch 5/50\n",
      "42678/42678 [==============================] - 15s 360us/step - loss: 0.4651 - acc: 0.7797 - val_loss: 0.4545 - val_acc: 0.7840\n",
      "Epoch 6/50\n",
      "42678/42678 [==============================] - 16s 365us/step - loss: 0.4439 - acc: 0.7921 - val_loss: 0.4703 - val_acc: 0.7729\n",
      "Epoch 7/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.4271 - acc: 0.8019 - val_loss: 0.4310 - val_acc: 0.8008\n",
      "Epoch 8/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.4104 - acc: 0.8123 - val_loss: 0.4385 - val_acc: 0.7943\n",
      "Epoch 9/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.4089 - acc: 0.8143 - val_loss: 0.4108 - val_acc: 0.8147\n",
      "Epoch 10/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3992 - acc: 0.8210 - val_loss: 0.4329 - val_acc: 0.7998\n",
      "Epoch 11/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3828 - acc: 0.8299 - val_loss: 0.4095 - val_acc: 0.8138\n",
      "Epoch 12/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3748 - acc: 0.8324 - val_loss: 0.4278 - val_acc: 0.7993\n",
      "Epoch 13/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3796 - acc: 0.8314 - val_loss: 0.4023 - val_acc: 0.8208\n",
      "Epoch 14/50\n",
      "42678/42678 [==============================] - 15s 361us/step - loss: 0.3674 - acc: 0.8381 - val_loss: 0.4186 - val_acc: 0.8120\n",
      "Epoch 15/50\n",
      "42678/42678 [==============================] - 16s 365us/step - loss: 0.3631 - acc: 0.8413 - val_loss: 0.3848 - val_acc: 0.8302\n",
      "Epoch 16/50\n",
      "42678/42678 [==============================] - 15s 361us/step - loss: 0.3536 - acc: 0.8455 - val_loss: 0.4093 - val_acc: 0.8191\n",
      "Epoch 17/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3606 - acc: 0.8416 - val_loss: 0.3944 - val_acc: 0.8238\n",
      "Epoch 18/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3511 - acc: 0.8454 - val_loss: 0.3883 - val_acc: 0.8285\n",
      "Epoch 19/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3476 - acc: 0.8481 - val_loss: 0.4279 - val_acc: 0.8152\n",
      "Epoch 20/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3412 - acc: 0.8510 - val_loss: 0.3821 - val_acc: 0.8317\n",
      "Epoch 21/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3401 - acc: 0.8524 - val_loss: 0.3884 - val_acc: 0.8363\n",
      "Epoch 22/50\n",
      "42678/42678 [==============================] - 16s 366us/step - loss: 0.3396 - acc: 0.8529 - val_loss: 0.3819 - val_acc: 0.8338\n",
      "Epoch 23/50\n",
      "42678/42678 [==============================] - 16s 365us/step - loss: 0.3348 - acc: 0.8546 - val_loss: 0.3781 - val_acc: 0.8384\n",
      "Epoch 24/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3352 - acc: 0.8549 - val_loss: 0.3798 - val_acc: 0.8287\n",
      "Epoch 25/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3303 - acc: 0.8549 - val_loss: 0.3839 - val_acc: 0.8281\n",
      "Epoch 26/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3300 - acc: 0.8588 - val_loss: 0.3819 - val_acc: 0.8276\n",
      "Epoch 27/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3218 - acc: 0.8619 - val_loss: 0.3834 - val_acc: 0.8298\n",
      "Epoch 28/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3199 - acc: 0.8626 - val_loss: 0.3744 - val_acc: 0.8352\n",
      "Epoch 29/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3180 - acc: 0.8641 - val_loss: 0.3795 - val_acc: 0.8327\n",
      "Epoch 30/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3217 - acc: 0.8615 - val_loss: 0.3935 - val_acc: 0.8287\n",
      "Epoch 31/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3181 - acc: 0.8630 - val_loss: 0.3839 - val_acc: 0.8285\n",
      "Epoch 32/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3184 - acc: 0.8625 - val_loss: 0.4101 - val_acc: 0.8263\n",
      "Epoch 33/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3140 - acc: 0.8654 - val_loss: 0.3899 - val_acc: 0.8276\n",
      "Epoch 34/50\n",
      "42678/42678 [==============================] - 17s 401us/step - loss: 0.3109 - acc: 0.8676 - val_loss: 0.3795 - val_acc: 0.8355\n",
      "Epoch 35/50\n",
      "42678/42678 [==============================] - 18s 414us/step - loss: 0.3059 - acc: 0.8693 - val_loss: 0.4046 - val_acc: 0.8255\n",
      "Epoch 36/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3082 - acc: 0.8699 - val_loss: 0.3985 - val_acc: 0.8238\n",
      "Epoch 37/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3056 - acc: 0.8698 - val_loss: 0.3773 - val_acc: 0.8371\n",
      "Epoch 38/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.3054 - acc: 0.8699 - val_loss: 0.3822 - val_acc: 0.8339\n",
      "Epoch 39/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3014 - acc: 0.8729 - val_loss: 0.3855 - val_acc: 0.8376\n",
      "Epoch 40/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.3025 - acc: 0.8715 - val_loss: 0.3921 - val_acc: 0.8339\n",
      "Epoch 41/50\n",
      "42678/42678 [==============================] - 16s 363us/step - loss: 0.2955 - acc: 0.8765 - val_loss: 0.3894 - val_acc: 0.8261\n",
      "Epoch 42/50\n",
      "42678/42678 [==============================] - 16s 366us/step - loss: 0.2950 - acc: 0.8756 - val_loss: 0.4017 - val_acc: 0.8277\n",
      "Epoch 43/50\n",
      "42678/42678 [==============================] - 15s 361us/step - loss: 0.2958 - acc: 0.8750 - val_loss: 0.3866 - val_acc: 0.8316\n",
      "Epoch 44/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.3025 - acc: 0.8724 - val_loss: 0.3840 - val_acc: 0.8317\n",
      "Epoch 45/50\n",
      "42678/42678 [==============================] - 16s 363us/step - loss: 0.2926 - acc: 0.8756 - val_loss: 0.3993 - val_acc: 0.8256\n",
      "Epoch 46/50\n",
      "42678/42678 [==============================] - 15s 362us/step - loss: 0.2898 - acc: 0.8770 - val_loss: 0.3841 - val_acc: 0.8333\n",
      "Epoch 47/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.2837 - acc: 0.8818 - val_loss: 0.3910 - val_acc: 0.8316\n",
      "Epoch 48/50\n",
      "42678/42678 [==============================] - 16s 363us/step - loss: 0.2848 - acc: 0.8808 - val_loss: 0.4121 - val_acc: 0.8305\n",
      "Epoch 49/50\n",
      "42678/42678 [==============================] - 16s 364us/step - loss: 0.2809 - acc: 0.8818 - val_loss: 0.4049 - val_acc: 0.8268\n",
      "Epoch 50/50\n",
      "42678/42678 [==============================] - 15s 363us/step - loss: 0.2814 - acc: 0.8821 - val_loss: 0.4073 - val_acc: 0.8276\n",
      "Test loss: 0.4065295668327596\n",
      "Test accuracy: 0.8280224929932913\n"
     ]
    }
   ],
   "source": [
    "parallel_attention_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=200,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = parallel_attention_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = il4_model.to_json()\n",
    "with open(\"il4_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "il4_model.save_weights(\"il4_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('kla_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"kla_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "kla_model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('veh_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"veh_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "veh_model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('il4_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"il4_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "il4_model = loaded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg1_enhancer = 'ggtagccgacgagagaccagctcatcttcaataaggaagtcagagagcagaaggctttgtcagcagggcaagactatactttgttaggaagtgaggcattgttcagacttccttatgctttcttatgaacaggctgtattagccaacagtcctgtc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arg1_enhancer = 'AAAGTGGCACAACTCACGTACAGACAGGACTGTTGGCTAATACAGCCTGTTCATAAGAAAGCATAAGGAAGTCTGAACAATGCCTCACTTCCTAACAAAGTATAGTCTTGCCCTGCTGACAAAGCCTTCTGCTCTCTGACTTCCTTATTGAAGATGAGCTGGTCTCTCGTCGGCTACCACCCTCCGTGACCTTATGCAGA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tnf_enhancer = 'CTAAGCTGTGTCACGGGAGCTGGCAGCACGCTGGCGGATATGCCTTGCCATGGGCCAATTTTGGTTTCAATCTCAGTTTTAGAGGTTGTGTGAAATTCAGTTTCTCTCTTGGGGAGGCCAACAGCTGTCTGGGACTTTCCCCGGGGGGGAGGGCTGATGACTAGGAGTCTTGTGCATCGTCTATAACCACTCTCAGGAAG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdkAAABOCAYAAADcih/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMR+xmIcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d17bdc4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdkAAABOCAYAAADcih/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJK/T/hsFMD0Nv7dAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d17bdc9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdkAAABOCAYAAADcih/PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d17bf55c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "attention_output, full_attention, attended_sequence = get_sequence_attention(veh_model,\n",
    "                      str(positive_seqRecords[index].seq),\n",
    "                      5)\n",
    "frame = pd.DataFrame(full_attention).T\n",
    "frame.columns = list(attended_sequence)\n",
    "fig, ax = plt.subplots(figsize=(30,1)) \n",
    "hm=sns.heatmap(frame, square=True, ax=ax, vmin=0, vmax=0.20)\n",
    "plt.ylabel('Vehicle')\n",
    "plt.show()\n",
    "\n",
    "attention_output, full_attention, attended_sequence = get_sequence_attention(il4_model,\n",
    "                      str(positive_seqRecords[index].seq),\n",
    "                      5)\n",
    "\n",
    "frame = pd.DataFrame(full_attention).T\n",
    "frame.columns = list(attended_sequence)\n",
    "fig, ax = plt.subplots(figsize=(30,1)) \n",
    "hm=sns.heatmap(frame, square=True, ax=ax, vmin=0, vmax=0.20)\n",
    "plt.ylabel('IL4')\n",
    "plt.show()\n",
    "\n",
    "attention_output, full_attention, attended_sequence = get_sequence_attention(kla_model,\n",
    "                      str(positive_seqRecords[index].seq),\n",
    "                      5)\n",
    "frame = pd.DataFrame(full_attention).T\n",
    "frame.columns = list(attended_sequence)\n",
    "fig, ax = plt.subplots(figsize=(30,1)) \n",
    "hm=sns.heatmap(frame, square=True, ax=ax, vmin=0, vmax=0.20)\n",
    "plt.ylabel('KLA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attention_multilabel_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size,\n",
    "                        adjacent_bp_pool_size,\n",
    "                        attention_dim,\n",
    "                        attention_hops,\n",
    "                        num_dense_neurons,\n",
    "                        dropout_rate=0.25):\n",
    "    input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "    input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "    ### find motifs ###\n",
    "    convolution_layer = Conv1D(filters=num_motifs, \n",
    "        kernel_size=motif_size,\n",
    "        activation='relu',\n",
    "        input_shape=(total_seq_length,4),\n",
    "        name='convolution_layer',\n",
    "        padding = 'same'\n",
    "        )\n",
    "    forward_motif_scores = convolution_layer(input_fwd)\n",
    "    reverse_motif_scores = convolution_layer(input_rev)\n",
    "    print('forward_motif_scores', forward_motif_scores.get_shape())\n",
    "\n",
    "    ### crop motif scores to avoid parts of sequence where motif score is computed in only one direction ###\n",
    "    to_crop = int((total_seq_length - seq_size)/2)\n",
    "    crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "        name='crop_layer')\n",
    "    cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "    cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "    print('cropped_fwd_scores', cropped_fwd_scores.get_shape())\n",
    "\n",
    "    ### flip motif scores ###\n",
    "    flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "        output_shape=(seq_size, num_motifs),\n",
    "        name='flip_layer')\n",
    "    flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "    print('flipped_rev_scores', flipped_rev_scores.get_shape())\n",
    "\n",
    "    ### concatenate motif scores ###\n",
    "    concatenate_layer = keras.layers.Concatenate(axis=2, name='concatenate_layer')\n",
    "    concatenated_motif_scores = concatenate_layer([cropped_fwd_scores, flipped_rev_scores])\n",
    "    print('concatenated_motif_scores', concatenated_motif_scores.get_shape())\n",
    "\n",
    "    ### pool across length of sequence ###\n",
    "    sequence_pooling_layer = MaxPool1D(pool_size=adjacent_bp_pool_size, \n",
    "        strides=adjacent_bp_pool_size,\n",
    "        name='sequence_pooling_layer')\n",
    "    pooled_scores = sequence_pooling_layer(concatenated_motif_scores)\n",
    "    print('pooled_scores', pooled_scores.get_shape())\n",
    "\n",
    "    ## bidirectional LSTM ###\n",
    "    forward_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "        return_sequences=True,\n",
    "        input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "        name = 'forward_lstm_layer'\n",
    "        )\n",
    "    forward_hidden_states = forward_lstm_layer(pooled_scores)\n",
    "    print('forward_hidden_states', forward_hidden_states.get_shape())\n",
    "\n",
    "    reverse_lstm_layer = LSTM(units=int(seq_size/adjacent_bp_pool_size),\n",
    "        return_sequences=True,\n",
    "        input_shape = (int(seq_size/adjacent_bp_pool_size), 2*num_motifs),\n",
    "        name = 'reverse_lstm_layer',\n",
    "        go_backwards=True,\n",
    "        )\n",
    "    reverse_hidden_states = reverse_lstm_layer(pooled_scores)\n",
    "    print('reverse_hidden_states', reverse_hidden_states.get_shape())\n",
    "    ### concatenate lstm hidden states ###\n",
    "    lstm_concatenate_layer = Concatenate(axis=2)\n",
    "    bilstm_hidden_states = lstm_concatenate_layer([forward_hidden_states, reverse_hidden_states])\n",
    "\n",
    "    print('bilstm_hidden_states', bilstm_hidden_states.get_shape())\n",
    "\n",
    "    # tanh layer\n",
    "    attention_tanh_layer = Dense(attention_dim,\n",
    "        activation='tanh',\n",
    "        use_bias=False,\n",
    "        name = 'attention_tanh_layer')\n",
    "    attention_tanh_layer_out = attention_tanh_layer(bilstm_hidden_states)\n",
    "    print('attention_tanh_layer_out', attention_tanh_layer_out.get_shape())\n",
    "\n",
    "    # outer layer\n",
    "    attention_outer_layer = Dense(attention_hops,\n",
    "        activation='linear',\n",
    "        use_bias=False,\n",
    "        name = 'attention_outer_layer')\n",
    "    attention_outer_layer_out = attention_outer_layer(attention_tanh_layer_out)\n",
    "    print('attention_outer_layer_out', attention_outer_layer_out.get_shape())\n",
    "\n",
    "    # apply softmax\n",
    "    softmax_layer = Softmax(axis=1, name='attention_softmax_layer')\n",
    "    attention_softmax_layer_out = softmax_layer(attention_outer_layer_out)\n",
    "    print('attention_softmax_layer_out', attention_softmax_layer_out.get_shape())\n",
    "\n",
    "    # attend to hidden states\n",
    "    attending_layer = Dot(axes=(1,1),\n",
    "        name='attending_layer')\n",
    "\n",
    "    attended_states = attending_layer([attention_softmax_layer_out, bilstm_hidden_states])\n",
    "    print('attended_states', attended_states.get_shape())\n",
    "\n",
    "    # # fully connected layer\n",
    "    dense_layer = Dense(num_dense_neurons, \n",
    "        activation='relu', \n",
    "        name = 'dense_layer'\n",
    "        )\n",
    "\n",
    "    dense_output = dense_layer(attended_states)\n",
    "    print('dense_output', dense_output.shape)\n",
    "    \n",
    "    # drop out\n",
    "    drop_out = Dropout(dropout_rate,name='dense_dropout')(dense_output)\n",
    "    print('drop_out', drop_out.shape)\n",
    "    \n",
    "    # make prediction\n",
    "    flattened = Flatten(name='flatten')(drop_out)\n",
    "    print('flattened', flattened.shape)\n",
    "    \n",
    "    predictions = Dense(num_classes,\n",
    "                        name='predictions',\n",
    "                        activation = 'sigmoid', \n",
    "                       )(flattened)\n",
    "    print('predictions', predictions.shape)\n",
    "    \n",
    "    # define and compile model\n",
    "    model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quantile_normalize_df(df_input):\n",
    "    df = df_input.copy()\n",
    "    #compute rank\n",
    "    dic = {}\n",
    "    for col in df:\n",
    "        dic.update({col : sorted(df[col])})\n",
    "    sorted_df = pd.DataFrame(dic)\n",
    "    rank = sorted_df.mean(axis = 1).tolist()\n",
    "    #sort\n",
    "    for col in df:\n",
    "        t = np.searchsorted(np.sort(df[col]), df[col])\n",
    "        df[col] = [rank[i] for i in t]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_seqRecords = list(SeqIO.parse('./background.fasta', 'fasta'))\n",
    "negative_seq = [str(x.seq[:200]) for x in negative_seqRecords]\n",
    "negative_rc_seq = [str(x.seq[:200]) for x in negative_seqRecords]\n",
    "\n",
    "negative_sequence_arrays = convert_sequences_to_array(negative_seq)\n",
    "negative_sequence_arrays = np.array(negative_sequence_arrays)\n",
    "\n",
    "negative_sequence_rc_arrays = convert_sequences_to_array(negative_rc_seq)\n",
    "negative_sequence_rc_arrays = np.array(negative_sequence_rc_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_seqRecords = list(SeqIO.parse('./merged_atac_peaks_filtered_resized.fasta', 'fasta'))\n",
    "positive_seqRecords = [x for x in positive_seqRecords]\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords ]\n",
    "\n",
    "seq_ids = [x.name for x in positive_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "sequence_rc_arrays = np.array(sequence_rc_arrays)\n",
    "\n",
    "index_seqArray_dict = dict(zip(seq_ids, zip(sequence_arrays, sequence_rc_arrays)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_frame = pd.read_csv('./group_atac_summary.tsv' , sep='\\t', low_memory=False)\n",
    "summary_frame = summary_frame.fillna('0')\n",
    "for col in summary_frame.columns[5:]:\n",
    "    floatValues = []\n",
    "    for val in summary_frame[col].values.astype(str):\n",
    "        if ',' in val:\n",
    "            maxVal = np.mean([float(x) for x in val.split(',')])\n",
    "            floatValues.append(maxVal)\n",
    "        else:\n",
    "            floatValues.append(float(val))\n",
    "    summary_frame[col] = floatValues\n",
    "# summary_frame.index = summary_frame['ID'].values\n",
    "summary_frame.index = summary_frame['chr'] + ':' + (summary_frame['start'] - 1).astype(str) + '-' + summary_frame['end'].astype(str)\n",
    "\n",
    "# remove peaks in unknown/random chromosomes\n",
    "summary_frame = summary_frame[~summary_frame['chr'].str.contains('random')]\n",
    "summary_frame = summary_frame[~summary_frame['chr'].str.contains('Un')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c57bl6_ifng-24h    33333\n",
       "c57bl6_il13-24h    23066\n",
       "c57bl6_il1b-24h    13650\n",
       "c57bl6_il23-24h    33482\n",
       "c57bl6_il4-24h     33518\n",
       "c57bl6_il5-24h      2091\n",
       "c57bl6_il6-24h     20308\n",
       "c57bl6_kla-1h      21776\n",
       "c57bl6_tgfb-24h    29540\n",
       "c57bl6_tnfa-24h    31600\n",
       "c57bl6_veh-24h     11500\n",
       "c57bl6_veh         31550\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(summary_frame[[x for x in summary_frame.columns if 'c57' in x]]>0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c57bl6_ifng-24h    0.608588\n",
       "c57bl6_il13-24h    0.421135\n",
       "c57bl6_il1b-24h    0.249219\n",
       "c57bl6_il23-24h    0.611309\n",
       "c57bl6_il4-24h     0.611966\n",
       "c57bl6_il5-24h     0.038177\n",
       "c57bl6_il6-24h     0.370780\n",
       "c57bl6_kla-1h      0.397583\n",
       "c57bl6_tgfb-24h    0.539337\n",
       "c57bl6_tnfa-24h    0.576948\n",
       "c57bl6_veh-24h     0.209965\n",
       "c57bl6_veh         0.576035\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(summary_frame[[x for x in summary_frame.columns if 'c57' in x]]>0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenhan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "peak_count_threshold = 20000\n",
    "peak_counts = (summary_frame[[x for x in summary_frame.columns if 'c57' in x]]>0).sum(axis=0)\n",
    "to_filter_conditions = peak_counts[peak_counts < peak_count_threshold].index.values\n",
    "label_frame = summary_frame[[x for x in summary_frame.columns if 'c57' in x]]\n",
    "for cond in to_filter_conditions:\n",
    "    label_frame.drop(cond, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_frame = summary_frame[['c57bl6_il4-24h', 'c57bl6_kla-1h', 'c57bl6_veh']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quantiled_label_frame = quantile_normalize_df(label_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54771, 3)"
      ]
     },
     "execution_count": 1221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7c879d4c88>"
      ]
     },
     "execution_count": 1222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8HWd99/3PzFm177K8yvvEjmNnj0MSEkhYbqDc7C19\nWppSmkIDD9CHQoCGu5RSbrjL2qQt0BJSCAFuoBAgG04cZ3ES7DhxbMcZb/Im2dq3o6Ozzjx/HB1H\nXiQdHWl0dOTv+/XKK9LMnJmfxjr6neua6/pdhuu6iIiISPEwCx2AiIiITI6St4iISJFR8hYRESky\nSt4iIiJFRslbRESkyCh5i4iIFBm/lye3LGsd8Cvg67Zt33HGvpuAfwLSwP22bX/By1hERETmCs9a\n3pZllQH/AjwyxiHfAt4JXAO83rKstV7FIiIiMpd42W0eB94EtJ25w7Ks5UCPbdvHbNt2gPuBGz2M\nRUREZM7wLHnbtp2ybXt4jN1NQOeo7zuA+V7FIiIiMpd4+sx7EoyJDkil0q7f75uJWDz1jd/9csx9\nH3vd22YwEhERKQLnzI+FSt5tZFrfWQs5R/f6aL29UU8DmknxWPKc2zs7B2c4ktmtoaFC9yQHuk+5\n073Kne5Vbry+Tw0NFefcXpCpYrZtHwYqLctaalmWH3gL8HAhYhERESk2nrW8Lcu6DPgqsBRIWpb1\nLuA+oMW27f8GPgTcO3L4T2zb3udVLCIiInOJZ8nbtu3ngBvG2f84cLVX1xcREZmrVGFNRESkyCh5\ni4iIFBklbxERkSKj5C0iIlJkZkuRFhERkbN8+MO38Dd/80mWL195atvQUIQvfekL9Pb24Dhpqqqq\n+exnP09FxbnnRM9FSt4iIueJx15ozfnYivIwg5HYuMfccPHCqYaUl5/85EesXXshf/zH7wPg+9//\nDx5++AHe+c73FCSeQlDyFhERz9x//6959tmtDA0N0dnZwXve88csWrSYb3/7Tvx+P42N8/jUp/4O\nwzD44hf/ns7ODoaHh3n/+2/hmmuuO3WeoaEIH/vYrXz6058jEhkklUqd2nfzzR849fU999zNY489\ngmGYfPCDH+bSSy/npz+9l0ceydQBu+666/mTP7mZL37x7/H7AwwM9PEP//C/+cpXvkhbWyupVIoP\nfOCDXHbZFTN3k/Kg5C0iIp5qaTnE9753D5FIhJtvfi81NTV885v/RmVlFf/6r99k8+ZNXHHFVVx5\n5Ub+x/94C62tx7n99ttOJW/XhX/8x7/n/e+/heXLV/COd7yHj3/8wzzzzFNceeXV3Hjj61m1ajXH\njh3lscce4dvf/j5tba388Iffp6lpPg888Gu++93/AuCWW/6M17zmJgAqKyv51Kc+y4MP/pa6uno+\n/enP0dfXx0c/+kHuvvvHBbpbuVHyFhERT1188aX4/X6qq6spKyvj6NEjfOYzfwtALBajqqqaiopK\n9u7dw333/QLDMBkY6D/1+rvu+g7z5s3j6quvAWDRosXce+/P2bFjO88++zQf+9iH+NCH/l9KSkpZ\nu3YdpmmyaNFibrvtdrZseZQLL7wIvz+T7i66aAMHDmQKeq5deyEAu3e/yM6dz/Piiy8AEI/HSSaT\nBAKBGbtHk6XkLSIinnIc99TXhmFSV1fPHXd857RjHnjgNwwMDHDnnf/BwMAAH/jAn57aV1FRybZt\nz9Lf30dVVTXxeIxQKMyVV27kyis3cu21r+Z73/sO73zne0671sgVcd1XtiWTSQwjM9HK7w+c+v/7\n3vd+Xve6N07zT+4dTRUTERFP7dnzIul0mr6+PqLRIUzTpKXlEAA/+9mPOXBgP319fcyfvwDTNNmy\n5VGSyVdWX3z3u/+IP/7j9/GNb/wzAB/72K1s2/bsqf2dnR0sWLAQy1rDrl07SaVS9PR08+lPf4LV\nqy12795FKpUilUrx0kt7WL3aOi2+tWvX8eSTWwDo7e3h29++0+tbMmVqeYuIiKeamhZw++230dp6\njFtu+Wvmz1/IP/3T5wkEAtTXN/DWt76DsrIybrvtb3jppd28+c1vpbGxkbvu+u6pc7z5zW/l0Uc3\n8eSTW/jMZ/4XX/val/n+9/8Dn89HeXkFn/jEbdTW1vGGN7yJD3/4FlzX5a/+6lbmz1/AW9/6dj7y\nkVtwHJc/+IP/SVPT/NPie+1rb2LHjm188IPvJ51O8/733zLTt2jSjNHdCbNZZ+dgcQQ6gXteeGTM\n9bzfv7F4umxmgtYTzo3uU+50r3I3Xffq/vt/zaFDB/nwhz82DVHNPjOwnrdxru3qNhcRESky6jYX\nERHPvOlNf1DoEOYktbxFRESKjJK3iIhIkVHyFhERKTJK3iIiIkVGyVtERIrOY489AmSmom3Zspkd\nO7bzd3/3yQJHNbFDhw7w4Q9PfR65RpuLiJwnnmx9JudjKwbCDA6OvyTotQs3TjWkvJw40camTQ9x\nww03nhrNvmPH9oLEUihK3iIi4omhoQif+cwnSSTiXHbZFTz00P383/97H+961x/wX//1E0pLS7nj\njm+wfPkKrr/+NXz+83/H8PAwsViMj3/8b1m7dh1/+Idv461vfTtbtz5JIpHgm9/8V772tS+zd+8e\n7rrruziOQ3V1NcuWrTh13S1bHuXHP/4hPp8fy1rDRz7y8dPi+uIX/566unr27XuZ9vaTfO5z/4hl\nXTDh0qHXXPNqXnhhB319fbS0HOKWWz7E448/wr59+/nc5/6RCy9cx89//lM2bXoQwzC57robeO97\n/4SOjnZuv/02AoEAK1eunpZ7q25zERHxxIMP3s+qVav5t3/7T5YuXcZ4FT27u7t5y1vexr/8y7f5\n4Ac/zD333A1AOp2muXkZd975XRYsWMD27dt473v/lIsvvpQ///O/POs80WiUu+/+T775zX/njju+\nQ0dH+6nVwkZLJpN87Wt38O53/xEPPvhb2tpaeeCBX3Pnnd/lzju/y6OP/o7W1uNAZunQL37x/wBw\n7NhRvvzlr/Gnf3ozP/zh97nzzjv50z+9mU2bHqKtrZXHHnuEf/3X/+TOO7/Lli2PcvLkSX72sx9z\n442v5447vkN9ff103Fq1vEVExBtHjrRwySWXAZz6/1hqa+u4++7/4N57f0AymSQcDp/at2HDJQA0\nNMxjaChCeXn5mOdpaTlEe/tJ/uZvPgxkWv8nT55k/frTjxt9zpde2sP+/faES4cCXHDBWgzDoK6u\nnhUrVuHz+aipqWNoaCd79+7h+PFjfOQjfwVANDrEyZNtHD7ccmoN8UsuuZxnntk6/o3LgZK3iIh4\nwnXBMDKluX2+V9JNdhtAKpUC4Kc//RH19Y3cfvsXePnll7jjjm+cOsbn84065/jLXAQCma7yr33t\njnGPO/ucEy8deubrzjyH3x/g6quv4ZOf/Oxp17rnnrtPnct1nXHjypW6zUVExBPNzc289NJuALZv\nf2UJz9LSMrq7u0in0+zZswuA/v4+Fi5cBMCWLZtPJfVzMU2TdDp9zn1Llizl8OEWent7APjP//w2\nnZ0dE8aay9KhE7GsNezY8RyxWAzXdfnGN/6ZeDzGkiXNvPzyS8D0DaxTy1tERDzxhje8mc985hPc\neutfsn79xae2v/Od7+FTn/o4S5Y0s2zZcgDe+MY384//+L/YvHkT73zne9i06WF++9v7znne5uZl\n2PbLfOtbX6Ws7PQu9HA4zEc/+v/xiU98lGAwwKpVFvX1DRPGmsvSoRNpamriPe95L7fe+peYpsmr\nX30DoVCYd7/7vdx++208/vhmVqxYNalzjkVLgs4wLQmaOy3fmBvdp9zpXuVuuu9VNBrlfe/7Q372\ns19P2zlnAy0JKiIiIjlR8hYREc+VlpbOuVZ3ISl5i4iIFBklbxERkSKj5C0iIlJklLxFRESKjJK3\niIhIkVHyFhERKTJK3iIiIkVGyVtERKTIKHmLiIgUGSVvERGRIqPkLSIiUmSUvEVERIqMkreIiEiR\nUfIWEREpMkreIiIiRUbJW0REpMgoeYuIiBQZJW8REZEio+QtIiJSZJS8RUREioySt4iISJHxe3ly\ny7K+DmwEXOCjtm1vG7XvVuBPgDSw3bbtj3kZi4iIyFzhWcvbsqzrgVW2bV8N/AXwrVH7KoG/Ba6z\nbftaYK1lWRu9ikVERGQu8bLb/EbglwC2be8FakaSNkBi5L9yy7L8QCnQ42EsIiIic4aXybsJ6Bz1\nfefINmzbjgGfBw4BR4Bnbdve52EsIiIic4anz7zPYGS/GGmBfwZYDQwAj1qWtcG27Z1jvbimphS/\n3+d9lDMgFA6cc3tDQ8UMRzL76Z7kRvcpd7pXudO9yk0h7pOXybuNkZb2iAXAiZGv1wCHbNvuArAs\n6wngMmDM5N3bG/UozJkXjyXPub2zc3CGI5ndGhoqdE9yoPuUO92r3Ole5cbr+zTWBwMvu80fBt4F\nYFnWpUCbbdvZn/AwsMayrJKR7y8H9nsYi4iIyJzhWcvbtu2tlmU9Z1nWVsABbrUs62ag37bt/7Ys\n6/8Amy3LSgFbbdt+wqtYRERE5hJPn3nbtn3bGZt2jtr3beDbXl5fRERkLlKFNRERkSKj5C0iIlJk\nlLxFRESKjJK3iIhIkVHyFhERKTJK3iIiIkVGyVtERKTIKHmLiIgUGSVvERGRIqPkLSIiUmSUvEVE\nRIqMkreIiEiRUfIWEREpMkreIiIiRUbJW0REpMgoeYuIiBQZJW8REZEio+QtIiJSZJS8RUREioyS\n9wxJppO4rlvoMEREZA7wFzqA80HaSfP3z3yFimA5DeZyDAKFDklERIqYkvcM6Bzuoi/eT1+8nxNG\nO0uDF1Llqyt0WCIiUqTUbT4D2obaAWgKLiHtOhyI76QzebzAUYmISLFS8p4BbZGTABx5sZGFySsx\n8XEydbTAUYmISLFS8p4BJ4YyydsZLufo3ipKjHISbgzHdQocmYiIFCMl7xlwfPAkbiqAmQoTjxmk\nomUAJNxYgSMTEZFipOTtsUQ6SXesGydazk2XLyYUcol0lwIQd6MFjk5ERIqRkrfH2qMduLi4w+Vc\nsKSGlReAG8u0vOPOcIGjExGRYqTk7bETIyPNneEKls6vYF4TlPjDAAzGlLxFRGTylLw9lh1pXk4N\n1eUhDAOa6jLJe1gtbxERyYOSt8eO9rcB0Fy94NS20rAfNxkgaSh5i4jI5Cl5e6w1chI3EWLFvPpT\n28JhFzdeiuMbVr1zERGZNCVvDw2nYkTSAzjD5SybX3lqeyAIbrwUDFfTxUREZNKUvD10cmSwmjtc\nTnNTxanthgG+dHa6mLrORURkcpS8PZQdrFZKDRWlwdP2BdwSAIZTSt4iIjI5St4eOtTTCsDCivln\n7QsZmeQ9lFTyFhGRyVHy9tCRkZHmVv2is/aV+DLJW4VaRERkspS8PdSV6MSJh1k5/+y1u0vCfty0\njwRK3iIiMjlK3h6JJIdIMow7XEFzU+VZ+0vCBm6slJQZ1XQxERGZFCVvj/TF+gEIOuWUhv1n7c/O\n9cZ0SLqJmQ5PRESKmJK3R/rjAwCU+MrOud8fABKaLiYiIpOn5O2RrmgmeZf7z528Afzp7KA1LQ0q\nIiK5U/L2SOdgLwBVobOfd2cFjEzLO6q53iIiMglK3h7pGc60vGtKxk7eJeZIoZa0kreIiOROydsj\nffFBAOrLqsY8piQYxHVMEnrmLSIik6Dk7ZFIMpO8myqrxzymJAxuvISkocVJREQkd0reHommo7ip\nALXlYw9YC5WAmwzimklc15nB6EREpJgpeXsk4UZxk0GqK0JjHhMOu7jJzP4UyZkKTUREipyStwfS\nTpq0GYdkiIrSwJjH+XxgOpn9SVfJW0REcqPk7YHBZAQAv1uCaRjjHusjs1Ro0lGVNRERyY2Stwey\n1dXCI/O4xxMwMi3vWFLJW0REcnN20e1pZFnW14GNgAt81LbtbaP2LQbuBYLADtu2P+hlLDOpcyhT\n17x0nOpqWUFfkAQQS6rbXEREcuNZy9uyrOuBVbZtXw38BfCtMw75KvBV27avBNKWZS3xKpaZ1jGQ\nqa5WGSyf8NiwP9NtHk8reYuISG687Da/EfglgG3be4Eay7IqASzLMoHrgPtG9t9q2/ZRD2OZUdm6\n5lXhsaurZZUERwas6Zm3iIjkyMtu8ybguVHfd45sGwAagEHg65ZlXQo8Ydv2p8c7WU1NKX6/z6tY\np9WQMwTAkoZGGhoqztofCr8yAr2yDEhB2kie89jzne5JbnSfcqd7lTvdq9wU4j55+sz7DMYZXy8E\nvgkcBn5rWdabbdv+7Vgv7u0tnpW3uiN9AJQZJXR2Dp61Px57pYvccF1cxyBN/JzHns8aGip0T3Kg\n+5Q73avc6V7lxuv7NNYHAy+7zdvItLSzFgAnRr7uAo7Ytn3Qtu008AhwoYexzKihVATXhflVNRMe\nGwwauMkQjqlucxERyY2Xyfth4F0AI13jbbZtDwLYtp0CDlmWtWrk2MsA28NYZlTMiUIqSG1FyYTH\nGgYY6SD4NWBNRERy41nytm17K/CcZVlbyYw0v9WyrJsty3r7yCEfA+4a2d8P/NqrWGZa0hjGTQap\nLA3mdLyRDoCZJp5W61tERCbm6TNv27ZvO2PTzlH7DgDXenn9Qkimk7hmEp9ThWmOX10ty3SCOEDf\n8ADzyuu9DVBERIqeKqxNs4FEZuBCkImrq2X53EwLvX2wz5OYRERkblHynmadkZHqar6Jq6tl+YxM\n8u4aqcwmIiIyngmTt2VZH7QsS5P9cnRypLpaWWDi6mpZ2frm3SPFXURERMaTS8t7PfCiZVl3W5Z1\nndcBFbuuoUzXd1Uo9887ATPT8u6LaU6liIhMbMLkbdv2XwMrgLuB/8eyrK2WZX3SsqyJJzGfh7qH\nM63nupKJS6NmBX2ZlvdAXMlbREQmltMzb9u2HeAgcJzMKmCXAU9YlvU/PYytKGWXA60vq875NdnF\nSSLJIU9iEhGRuWXCqWKWZb0P+HOgHvgu8Drbtnsty6oGtgC/8jbE4hJJDoEB8ypz75gIBzIt7+G0\nkreIiEwsl3nerwdut237ydEbbdvusyzrG96EVbyG0xFcn8H8qtxb3qGgiRv1E2fYw8hERGSuyKXb\n/EvAm7PfWJZ1l2VZ6wBs277Lq8CKVYJhSAapLg/l/BqfD9xUkJQR8zAyERGZK3JJ3ncA94/6/nvA\nnd6EU/xSZgwjHcJn5j6FPlvf3DFjOK7jYXQiIjIX5JJh/LZtP5H9ZvTXcrp4OgFmioA78YIkZzLS\nQTAgmlTXuYiIjC+XZ979lmV9CHiMTLJ/I6A5TeeQneMdNnMvjZplupn65t3RfsqDuVdnExGR808u\nLe8/JzM17KfAvcCqkW1yho6BTPIuMSeffH1uZsR5h+qbi4jIBCZsedu23Ql8YAZiKXrd0UyHRFlg\n8i3vACGSQNeQSqSKiMj4cpnn/V7gk0AtcGqNS9u2l3gYV1HqHc4k73y6vQNmpuXdM6zkLSIi48vl\nmffnybS8j3gcS9Hrj0UAqArnvihJVnCkvnm/6puLiMgEckne+23bftzzSOaAwUQmedeEJ78IW9A/\nUt88oeQtIiLjyyV5b7Us65/IjDZPZTfatv2oV0EVq6FkFICasskn72x982gqOq0xiYjI3JNL8r5p\n5P9Xj9rmAkreZxhOR8EHDeVVk35tKODDdQxijpK3iIiML5fR5q8BsCzLsG3b9T6k4hV3Yrgm1Jfn\n0W0eNGAwSMJUkRYRERnfhPO8LcvaYFnWdmDvyPe3W5Z1leeRFaEkMUgFKC8JTvq1Ph+QDpJS8hYR\nkQnkWtv8/cCJke9/AnzNs4iKWNqIYzhBTMOY+OBz8DthXDNF0klNfLCIiJy3ckneSdu2X8x+Y9v2\nPkYNXJMM13VxfQn8bjjvcwSMzGuHElrXW0RExpZL8k5ZlrWMzCA1LMv6H4wq1iIZkUQUDJcA+Sfv\nsJlZ0ERV1kREZDy5jDb/BPArwLIsqx84DPyZl0EVo85IJuGGzPyTd6m/lL6Rc62sm6bARERkzsll\ntPmLwHrLshqAuG3bahaeQ1ekH4AS3+TrmmeVBUrBhZ4hFWoREZGx5VLb/AeMdJmPfA+Abdvv8y6s\n4pNNuPksSpJVGSqHGPSpRKqIiIwjl27zTaO+DgKvAVq8Cad49Y7UNa+Ywlrc1SWZ5J2tkS4iInIu\nuXSb333Gpu9alvUbj+IpWgMjreXKPBYlyaotrYBeiGi0uYiIjCOXbvMzR6QvBlZ5E07xiiQyZU1r\nSydfXS2rvqwSUH1zEREZXy7d5ikyz7yz08P6gS97FlGRGkpFwZxa8m6srAZgOK0qayIiMrZcus1z\nmQt+3htOZ5J3Y3l13ueoLSvFdQwSbmwaIxMRkbkml27zfxhvv23bn5u+cIpXwo3hulBfkX/L2+/z\nYaSDpAwlbxERGVsurerFwJuAEiAEvBVYCqRH/hNGFiVJBwgHAlM6j+mEcMzENEUlIiJzUS7PvOuA\njbZtpyCzqhjwC83zPl3aiGM6k19N7Ex+QiR8A6TSKfy+XP55RETkfJNLy3tBNnED2LadAOZ7F1Lx\ncRxnyouSZAWNTH3zniHN9RYRkXPLpWm3w7KsZ4AnR75/FfDiOMefd/qHoximS9CYevIOm2EiQOdQ\n/6nR5yIiIqNN2PK2bfsW4LNAG5k1vT9PZn1vGdExUtd8KouSZJX6M+VVuyMqkSoiIueW6zSwMJCw\nbfurwAEP4ylKXSMripX68y+NmlU+Ul61Z1jJW0REzm3C5G1Z1peBvwD+fGTTHwPf8jKoYtMTzSTa\n8iksSpKVrY2u+uYiIjKWXFre19u2/Q5gAMC27S8Al3oaVZHpG576oiRZ1eHMPPHBuJK3iIicWy7J\nO1ur0wWwLMtHbgPdzhuDiUyirQrnX6Alq2akvGokqcVJRETk3HJJ3lsty7oLWGBZ1t8AW4DHPI2q\nyGRXAauZQl3zrPqyzDmGU6pvLiIi55bLaPPPAr8FHgEWAV+zbftTXgdWTIZGVgHLJt6pqK+oAiDm\nKHmLiMi55VLb/Dbbtv838LMZiKcoxdPD4IeG8qopn6umtAzXRYuTiIjImHLpNl9nWdZKzyMpYvGR\nRFs3DS1vv6nFSUREZHxjtrwty1pg23YbmYpqey3L6gYSZNb1dm3bXjJDMc56KSMGaf+01SLPLE4S\nn5ZziYjI3DNetrnPsqxryLTOLUaS9qj/ywjHjGM6oWk7X4AQMd8gyXSKgBYnERGRM4yXGQ4BQ2SS\n9/5R27PJ2+dhXEUjmUpnFiVJl0/bOYNGmLgB3UMRmlTfXEREzjBm8rZt+z0AlmV917btv5y5kIpL\nz9BQZlGS9NTrmmeFzRIGga7BASVvERE5Sy5TxZS4x9ExmKlrXuIrmbZznlqcZGhg2s4pIiJzh6cP\nVC3L+jqwkUw3+0dt2952jmO+BFxt2/YNXsbilZ6RBFvin3pd86yyQBkkoCeqEqkiInK2XFcVmzTL\nsq4HVtm2fTWZhU3OWszEsqy1wKu9imEmZFf/Kg9Mva55VmUo8/y8P66VxURE5GyeJW/gRuCXALZt\n7wVqLMuqPOOYr5JZK7xo9Y0k76rw9A1YqyrJnGswrvrmIiJyNi+7zZuA50Z93zmybQDAsqybydRJ\nP5zLyWpqSvH7Z98A9ziZYioLautoaMitSEsoHDjn9uzrlzTWQ0fm3Lmec64633/+XOk+5U73Kne6\nV7kpxH2ayUnERvYLy7JqyawPfhOwMJcX9/ZGPQpranqG+sGEMCE6O3Pr5o7Hkufcnn19yAkCMBCL\n5HzOuaihoeK8/vlzpfuUO92r3Ole5cbr+zTWBwMvu83byLS0sxYAJ0a+fi3QADwB/Ddw6cjgtqIT\nHVn9q778zCcC+WvMLk6S1uIkIiJyNi+T98PAuwAsy7oUaLNtexDAtu2f2ba91rbtjcDbgR22bX/c\nw1g8Ex9Z/atxGhYlyaoKZwa/JVwlbxEROZtnydu27a3Ac5ZlbSUz0vxWy7Jutizr7V5dsxAS7jC4\nUDmNA9b8Pj+kA6QM1TcXEZGzefrM27bt287YtPMcxxwGbvAyDi+ljTiGE8Q0pvdzkM8JkTIT03pO\nERGZG7zsNp/zXNfF8cXxTeOiJFl+QuBLkEimp/3cIiJS3JS8p2AolgB/kgDTV9c8K2iEMUyX7oiq\nrImIyOmUvKegKzKIYbiEzOmra54VNjPlVjsj/dN+bhERKW5K3lPQGckuSjJ9dc2zXlmcRMlbRERO\np+Q9BT1DmYn5ZYHpT94Vwczo9WztdBERkSwl7ynoGc60vMsD0zdNLCu7OElfTMlbREROp+Q9Bf3x\nzGCy6mmc451VE86UxBtMaHESERE5nZL3FERGEmt16fQXpa8ry5RbHUoqeYuIyOmUvKcgm1jry6av\nrnlWfVmm3Go0reQtIiKnU/KeguGRhUMaprGueVZjRTXwSu10ERGRrJlcEnTOyS4cUlc6/S3v8lAY\n1zFJjqwXLlKMHnuhdcx9N1yc02rAInIOanlPQZIYOCZh//SXRzUMAzMdImUoeYuIyOmUvKcgbcYx\nnRCGYXhyfp8bwvUlcF3Xk/OLiEhxUvLOUzLlgC+B353+VndW0CjB8KXpj0Y9u4aIiBQfJe889Q0N\nY/hTBIzpX5QkKzxSM/3EQJ9n1xARkeKj5J2njsFMzfHsAiJeKPOXAdA1UkNdREQENNo8b11DmYRa\n5vcueZcHyyCuxUlkdhtvRLmIeEMt7zz1REfqmgfLPLtG9UiJVC1OIiIioyl556l/ZMGQ7AIiXqgp\nycwfHxipoS4iIgJK3nkbiI/UNQ9Pf13zrGx980hSyVtERF6h5J2nbF3zWg8WJclqHCm7Gk1pqpiI\niLxCA9byFE1FwQd1HtQ1z2qsyJxb9c2lGA3HU6TSDgG/ScBv4jPVVhCZLkreeYqNJNTG8umva55V\nFigF1yCBkrcUj0g0yc6DXRxqHWB0bcDq8iDrV9bTPK/cs6qEIucLJe88ZRcl8XLAmmEYGOkQaTPu\n2TVEpksq7fCc3cn+Y304LlSVB6mvCpNMOcSTaTp6h3n8hTZqKkJcsqq+0OGKFDUl7zyljDik/fhN\nb29hgDArxVwkAAAgAElEQVRxX4REMk0w4PP0WiL5cl2XJ188wdH2CBWlATasrGfp/ArMUS3sgaEE\nLx7spqVtgEd3tFIa9vO265afdoyI5EYPofLguC6uL47Pw7rmWUEjjOFP0RPRoDWZvZ7f38XR9gjz\nakp467VLWb6g8qykXFkW5Nr183nLNUupKA3wm61H+Pdf7SGRTBcoapHipZZ3HoaGk+BPEHC8G2me\nVeIrJQK0D/TTVOP99UQm62BrP7sP9VBRGuD6SxaeNTDtUGzX6S8IwNpL4Mj+MNtf7qC7P8bH37OB\n8pLADEYtUtyUvPPQMzSEYbqEXO8WJckq85fRmYKuSD+wyPPriUzGwdZ+nt59kmDA5MbLFhEO5vZo\nJxCAN9wU4qlnEhw4NMAX7nmKN94UIhA4vbV+7cKNXoQtUvTUbZ6HzsHMKl8lPu/qmmdVBDMD4rqj\nWpxEZhfXdfnRpn04Llx/8QIqy4KTer3PZ3Ddq4KsWOajs8vh0S1x0mmtXS+SCyXvPHRHM6VRywLe\n1TXPqhqp4NYXU31zmV127Ouk5cQgzU0VzK/L771gGJkEvnihSesJh8efSuA4SuAiE1HyzkPvSCu4\nwsNFSbLqRiq4qb65zCZpx+EXjx/CNIwpT/syTYPXvDrEvEaTliNptu1ITlOUInOXknceekdawdUl\n3s3xzqovy1RZi4yUYxWZDbbuPsmJ7ijXrp8/6e7yc/H7DW56TYiqSoM9e1McOJSahihF5i4l7zz0\nxzPJu6Gs2vNrZUukDqc1VUxmh2Qqza+ebCHgN3nrNUun7byhoMFNN4QIBuCppxN0dWsKmchYNNo8\nD5FkBHwwv7LW82tln3nHXZVIldlh8/Nt9AzEeeNVS6itzH/Gxb5jfefcvvICg5d2Bdj0WILXLI9T\nVe59PQWRYqOWdx6GnUwXdmNFjefXytQ3hyQxz68lMhHHddm0/RjBgMmbNjZP+XwpN0FXqo0DsZ08\nH93C3tg24pUtrLt0mGjU4d9/tYe040xD5CJzi1reeUgQBRcqg94/8zYNE9MJkTLjpB1HKzNJQe09\n0ktXf4xr18+fclGV3lQHLYk9uCPLlwSNMFEnQtQZBP8hqtfPw96zjvuePMzbX718OsIXmTOUvCfJ\ncV3S5jA+J4zPnJla4wHCpANRItGkuhBlxj32Quupr7e80AZARWngtO2TNZDuoSWxBwOTBYGlVPsa\nCJulpNwkA+luIr52Ommn9KJBfrszirWkmrVLvX9MJVIslLwnKTKchECcgOvdUqBnCpolxI1++ob0\n/E8KJ5ZIcax9kOqR1cLyNZQe4GB8F2CwMrSeCt8rj5/8RoBafxM17jz8/hZOcJjgmme447Eh/mDN\ndZSEMn+ybrh44VR/HJGipuQ9SR39gxi+NCWO93O8s0p9pQw60DHYT/O8mfvQIOePXFrRh1oHcFxY\nuajqrPW4z6pfPoa4E2V/fCcOaZYHLzotcY9mGAYLgssp91VzYHgPzuIXefCgy/plDRgG+FuPqXSq\nnNf0AHWSTgz0AFAW8P55d1b5SCW3zsi5R+eKeM11XfYf78c0DJYvqMr7PMeTB0mTZEnAosbfMOHx\nlb5arJKLMRw/yXm7OXiyK+9ri8wlSt6T1BHpBaAqOHMt4KpQ5lrd0f4Zu6bIaJ19w/QPJVgyrzzn\nxUfONJQeoC/dSZlZSb1/Qc6vK/NVsCK4ARyTvopdnOjvyev6InOJkvckZRNobUn+rY/JqivJFIPp\njSl5S2HsP5753Vu1OP/f+7bkIQAWBlac1e0+kepQFYvc9YBBq7mLjkG9F+T8puQ9SX2xTF3zbNnS\nmdBYnnkuOJDUymIy81JphyMnBykvCdBUm99KeoPpXgacHirMmjGfc0+kqbKGqshaDF+azUeeJpJQ\n1UE5fyl5T1IkmSmN2jQD1dWymioy1xpKaXESmXltXUOk0i7NTRWTbjFD5nl566hW91SsnNeIr2cp\nTiDKPz/9PRxXBVzk/KTkPUnRdKa6WtMMVFfLyra8Y64WJ5GZd7Q986GxeV5+gzQHnB6GnH6qfPWU\n+aY2VsQwYE3DMhiopzN9lP968VdTOp9IsVLynqQ4ma666tDMDVgrC5SCY5I0lLxlZqUdl2MdEUrD\nfurynNvdkTwOwILAsmmJKRQyeNWCK3FipWzrfpodJ/ZMy3lFionmeU+CO1JdzXQCBHxTKw05GYZh\n4EuXkvLFcF03r65LkXyc7I6STDmsWFiZ1+9dwokz4HRTZlZSalZMW1zNC8Mc67yMo86T3LXnXrri\nr6U08MrzeM0Bl7lOLe9JGIqlRqqr5TdoZyqClEIgzmAsPuPXlvPXkfbMGI/mefkl3u70CQDq/POn\nLSbIrEhWXxPA12HhmAke3Pcs9tHeMVcqE5lr1PKehK6BIQx/krAz88m71Cxj2IDW3h4qS3KfIysy\n2pOtz5xz+6FYH8vDF522zXFcjrVHCAd9NNSUTPparuvSnTqBgUmtb15e8Y7HNOGCpvns7ushXt3B\nseHDLCmdnq55kdlOyXsSssUhyvwzV10tqyJYSXcaWvu6WbNAyVvGNlaCnqyO3mHiyTSrF1dh5tFl\nHnH6ibvD1Pqa8Bne/KkpKTVYHFnDsfgAncEWqpM1QLUn1xKZTTxN3pZlfR3YCLjAR23b3jZq32uA\nLwFpwAY+YNv2rJ730TFSnrRyBqurZdWWVHE4Au2Dqi4lMyPbZb4k3y7zVGYFsvpp7jI/U1Ojj96W\ndQw1bufA8EtclNaHW5n7PHvmbVnW9cAq27avBv4C+NYZh3wHeJdt29cAFcAbvYplunRFM8m7tmTm\nk3djWc1pMYh4yXVdjrZHCAbMvAqzpN0UvekOgkaYctP7lvDqJZUYHStx/TEea9mO67qeX1OkkLwc\nsHYj8EsA27b3AjWWZY3OepfZtn185OtOoM7DWKZF70h1tbrSme+Wm1+VuT19cVVZE+91D8QYjqdY\n1FCOaU6+y7w33YGDQ71//ozMjvD5wGpYgjNQQ0/6JA8efNzza4oUkpfJu4lMUs7qHNkGgG3bAwCW\nZc0HXg/c72Es02IwMVJdbQYLtGQtqs4k76HU4IxfW84/rZ2ZmgKLGvMb39GdOglAnc/bLvPRysth\nvnMhbjLAb448wPGBthm7tshMm8kBa2d9/LYsqxH4NfDXtm13j/fimppS/P78VjOaLjE3U6BlzdLF\nNFTlP2c1FD73HPGGhrHPWVUbht9DjKFxj5trzqefdSpG36eKgckXUwmFA1SUv/K6E91RDANWN9cS\nCkz8vgvxyu90wokTifZR4a+monRm//2WLwuQPHwJPVW/544dP+Df3v45gv7gacfodyp3ule5KcR9\n8jJ5tzGqpQ0sAE5kvxnpQn8A+Kxt2w9PdLLe3sIvQjCczpSJdKM+OhP5t4DjseQ5t3d2jn9OIx0k\nZURp7xjIa/RvsWloqJjwnsjZ92lwMDbpc8RjSQbJvG44nqKjd5h5NSUk4kkS8XP/vp75+qyOZKbF\nW2U0jPm77qUbNizkvueXMVjTwlc23c1fXfZHp/bpdyp3ule58fo+jfXBwMtu84eBdwFYlnUp0Gbb\n9uif8KvA123bftDDGKaN67qkzGFwfIR9oYLEkC3UMjCUKMj15fzQ1pXpMl+YZ5d5b7oDgBpf47TF\nNBmhkMEHr3gXTrScF/t3sK1tV0HiEPGSZy1v27a3Wpb1nGVZWwEHuNWyrJuBfuAh4H3AKsuyPjDy\nkh/Ztv0dr+KZqmg8Bf44AbekYOVJS81y4kYfJ/oGqC5vKEgMUnhnzuOuGAjn1doey/Hs8+6Gskm/\nNunGiTh9lJtVBM3CfMjdd6yP5eEEy1LXc9h5gLv3/ITOE0FKfeW8+3UXFCQmkenm6TNv27ZvO2PT\nzlFfF+adnaeewRgE4oTcmVvH+0yVwUp6kyOFWhYpecv0cxyXtq4hysJ+qsqCE7/gDL2pzBjVQrW6\nR7u8eQXt+9YRa3iRLd3388aGdxc6JJFpo9rmOTrZ34thQJlv5qurZdWEMx8c2gdUqEW80dE3TDLl\nsKixPK8ept50OwDVsyB5m4bBa5dci9vfQMR/gp392yZ+kUiRUPLOUftgLwAVwcKNvmwoy8wv74z2\nFiwGmduyU8QW1k++yzzhxIk4/ZSb1QXrMj9TWUmAy8teh5sMsj/1LHvbWwodksi0UPLOUbayWbb1\nWwjzK1WoRbzV2hnBZxo01U2+qlpfgQeqjWV5YwNNQ1eD6fClR79DPKWV+aT4KXnnqHc4kzDrywqZ\nvGsBiKhQi3ggMpykL5Kgqa4Uv2/yfxp6RrrMa/yFH49xKLbrtP8WNPkwe5YQM/v4l2d+WujwRKZM\nq4rlqC/eDyXQVFFbsBiqR1r9MWeoYDHI3NXamaljkE+XedwZZsgZoMKsIWDMji7z0UwTLqhfxp6h\nHlrYxb9t3sSamjWnHXPDxQsLFJ3I5KnlnaOBVKbbfEl14boEywNl4JqkfVESyXTB4pC56ZUpYpMf\nlJkdqFbrn/51u6dLSdjH8tB6XMdkd3Iz/bH+Qockkjcl7xw4rkvcGATXoK6kcC1v0zAJuiUQiNM7\nqOd2Mr5YKs7hgaN0D/eQdsdfbTedhpPdUarKg5SXnrt873h6Uh0YGFT7Ct9lPp759RU0Dl8K/iSP\ndv8Wx5nVqxCLjEnd5jnojyQgOETALcNnFra+eomvnDgddPUPMy+PpRrl/HByqIOnT2wjls58yDMN\nk7pwDZfPu5jq0NnjNvr7DNKOm1eXeX+qm2E3QpWvHr8x+cQ/065beDX3tR4nVXaSx9ue4IZF1xc6\nJJFJU8s7B8e7+zCCiRlZl3giFYEKDNOlrV/TxeRsjuvwYtdLbD7+JPF0gjW1q1lVvZyqYCWdw91s\nPvYkA+eoy9/bk/lTkE+X+bGEDUCtb/Z2mY/mM01e2/gmSITpKNmJ3X2w0CGJTJpa3jlo6R5Z3jA0\n80uBnqkmXMXxhAq1yLnt6HiR/X2HKAuU8qr5V1I/6jHP/t6DbO/YyaPHnuCmxddTHsy0sl3Xpafb\nJOA3aawpmdT1XNflaGIfJj6qfPXT+rN45VAsU+t8kXkhx9jBztTvSPb14W89BsC1CzcWMjyRnKjl\nnYPWgcz81aZZUE+8sSzzx7gjquQtp+sa7mF/3yEqgxW8sfm1pyVugFU1K7i4YR3DqRiPHn+CaGoY\ngP4Bl3jMYEFdKaY5uapqvel2hpx+qn31+IzCPlKarHmVVVREV2AE4rw8+BLJlJ5/S/FQ8s5B53Bm\nqfElNYXvFmyuzqyy2psYd/lzOc84rsO29ucBuGLeJQR9565LvqZ2Nevq1jCUjPLMiedwXZdjrZmZ\nCwvz6DI/Gs90mdfM4lHm41lVtxhftB63rIff7X6p0OGI5EzJOwf9ycw0seZZkLwXVWZiiDh65i2v\nsHsP0BfvZ0XVUhpLx+++Xld3AQvK5tEe7eBgfwvHj2eT9+QGq6XcJEcSLxM0Sqg0CzcLYypM02BN\n9RpIhOkL72P7vhOFDkkkJ0reE3Bdl2Ey1dUaJvijOBPqS+rANYibA7iuW+hwZBYYjEfY1bWXkC/E\nhoZ1Ex5vGAZXzLuUgBlgR8cu2vsilFc4lIQmNwTmSHwvCTfGitBFmEbx/ikJ+QMsC14IGOxLPMfO\nI8cLHZLIhIr3HTdDBqNJ3OAQfqeE0BhdkTPJZ/oIOOUQHmIolip0ODILPHv8BdJumksaLsr5d7Q0\nUMJljetJu2n8y3ZTXTu5572u67Iv9jwmPlaG1+cT9qxSW1rFivCFGIEE333xHnoGo4UOSWRcSt4T\nONETwQgOU2oWrqb5mcqNGgx/kuM9GrR2vhtMRGjpPUZNqIqllYsn9dqllUsIxxvxVfbgqz82qdee\nSLYQcfpYErQIm5OfGz4bXdm8igqnCbesmy8/+hNSaQ1gk9lLyXsCh7pOYhhQGyz8NLGsmlDm+eKh\n7rYCRyKFZvcewMXlgtrVk15/23UhemAtpAJ0+fYTSedeLnRfLDM4bnX4kkldczYzDIPXrbqMoFNB\npGIvdzz6UKFDEhmTkvcEjvdnponNKyv88+6sRRWZQWtHek8WOBIppHg6zqH+I5QHS1lSMflFNTo6\nHeJDYepi63Bw2Db0u5zGUfSmOuhMHWeefwlV/tnzvpgOIX+Ij13+F+D42cfj/Oq5nYUOSeSclLwn\n0DHcBcDi6sKPNM9aWZ/5Q31yqKPAkUgh7e9rIe2mWdd4QV4Dxo4cy4wyv2DeYqp9DXSlWjkQnzhZ\n7YvtAOZWq3u05uoF/OHKd2P40jzU8Qt2HNQANpl9lLwn0JfITMlqrmkqcCSvWNmwAIC+lJ55n6/S\nTpp9vQcJmAEuaFgx6de7rsvR42n8flg438eSoEXQCLMr+hSD6bGnIR6Jv8zRhE2Vr555geap/Aiz\n2quXXsJVtddihIb5z933cLxroNAhiZxGyXsCUTfzpp1N3eaVwQoMJ0DCN6ClQc9TLQNHiafjrKxe\nRtA3+cVA+vpdBgddFi3w4fMZBIwgl5a+hjQpnhj85TkTeG+qne1Dm/AbQa4uf9Okn7HPdvuO9bHv\nWB/fe+ZBvvfMgySHA4STDVDRzf9+7AdsOvh0oUMUOUXJexyR4SROIILpBCkLzJ4VvAzDoJRqjFCU\n1q5IocORGea6Lvt6D2JisLpm8q1ugKMjXeZLFr9S0nRRcBUXlmxkyBng0YH/S0/qlTEVMWeIpyK/\nwSHNxrI3UuGbPQM4vWIYBhdUrsGXKsOtPcIDzx1QCVWZNZS8x9HRG8UIDVNqVBY6lLPUheowTBe7\nXSPOzzedw930JwZYVLGQUv/kFhLJOnIsjWHA4oWvJG/DMFhbchWXld5Iwo3x2MDPeSbyAFsGfsGm\n/h8z7ERYV/Iq5geXTdePMuv5DD9W2UUYjp/h2t3c+dDjKo4ks4KS9zhaujswTIfqWTRNLGvhSJnU\nlh6Vczzf7O87BMCq6uV5vT4adejqdmhqNAmFzu76Xh5exzXlbwHgWGIfHaljpEiwMrSBC8KX5x94\nkSrxlXLtwisxDJd9/k385KkXCx2SiJYEHc+xvnaACWtFF8KKugU83Q0nIu2FDkVm0EBikOODrVQF\nK2koqcvrHEePZ7p+R3eZn2lBcDlvqf4ASTdO2CzFZ5zffyoWVTaxPraeF3tfZEv/L1mwu5pXr5u7\nA/Zk9lPLexztkcw0sUVVhV8K9ExLTq0uphHn55On27bh4LKyelneA8aOHM2U1V2yaPwlPINmiDJf\n5XmfuLMubFzJVfUbMUuGuPfgj9hzuLPQIcl5TMl7HD3xTGKcTdPEshpK6sGFpH+QyHCy0OHIDHBc\nhyfbnsVn+FhauSSvc0SHXdpOOjTUm1RU6O2fq+xI9MXpK6lzl2FW9HLH9h/y8y0HeOyF1kKHJ+ch\nvXvHkEo7DJD5ZL2kckGBozlb0BcgbFRghodo7dSI8/PBS902PbFellYuzmt6GEDL4RSuCyuWjd/q\nlnMzDIPra99EmdOIWXuCR9seYTCaKHRYch5S8h7D4ZMDGGV9hN0qSmfRNLHRaoK1GME4LR3qOj8f\nPNGamWe8Ms+BagAHWzKjzJc1qys8H4diuzgS38vKstX40qUw7yAPHNzCwwc0B1xmlt7BY3jhWAuG\nL83C8KJChzKmBeWNnOg5wsHuE0B+831lYk+2PjPmvmsXbpyRGDqiXezptllWuYTacHVe5+jvz4wy\nX7TApKRkbhVYmWl+I8iasvXsiT6Hu+Al7n+6hFctvJzykvx6REQmSy3vMbzc3QLAmsb8WzleW1ab\n6c5vi2i62Fz3eOtWXFxuWHRN3uc42JIZqLZimT6zT4eQWcrqkvUYmCSanudLv/wd0Viq0GHJeULJ\n+xxc16U9nhmEctG82duiXVmTKZbR65xQ4Yg5LJaK8XTbdqqCFVzceFFe53Bdl4MtmVrm400Rk8kp\n91WxIrQOw3TprX+Cr/xqM7GEErh4T8n7HHoG4qRCPZhugAXls2+kedbC8iZ8bhC3rJvu/lihwxGP\nPHtyB7F0jOsWXo3fzK/V3NnlMBhxaV7sIxBQl/l0qvbXc/X8yzFMh47qx/jKLx5nOK4ELt5S8j6H\nPcdOYpYMUeubl9dSizPFNEzqfAsxw8PsPaEyqXOR4zpsOb4Vv+HjmoVX5X2egy2ZWubL1WXuiaVV\ni3mv9Q6MQJKT1Zv58i+2qAtdPDV7M1MBvXjiIAAramZ/BaUVlZmu810d+wociXjB7jlAe7SDy+Zd\nTGWwIq9zJJMuh1pShMOwcL7e8l7Yd6yPVNciNpRcjxGM01G7mdvv2cRD244WOjSZo/ROPocjkcwb\nbsP8VQWOZGJXNa8F4NBAS4EjES9sPv4kANcvelXe5zhwKEU8ARes9mOa6jL30uqSi7m49AaMQILh\nRU/wwAu76B2MFzosmYOUvM8QT6SJ0AEUScu7dhGmE2TI305/RH8k5pKjg8fZ0/0yy6uaaa5cnNc5\nXNdlz94UPhPWrNY0ppmwKryBS0tfi+FPkmjeyhd+9jAnuocKHZbMMUreZzjY1odR3kcJVZQHygod\nzoRMw6QxsAgzNMzWfYcKHY5Mo98eehiANy17Xd7nOHo8zcCgy4rlPs3t9tih2K5T/xlAc3ANhi9F\nbNFT/NOv7udga3+hQ5Q5RMn7DDuPH84UZymdvcVZzrShaTUA21tfLnAkMl1a+o+wu/tlVlQt44Ka\n/B/f7HkpM2jqwjVqdc+0+sB8VoYvwuczSDdv4ysP/4qn95wsdFgyRyh5n2Ffz2EALmycvfO7z3TZ\nwjUAtMWOkkimCxyNTIffjLS6/2D56/NePayrO83JDoeFC0xqqvVWL4QqXz03LbmOEl8J/ubdfH/n\nz/nJ5n04juoyyNToHT1KKu3QkTwOwNqG2VtZ7Uzzy+bhJwTl3ew90lvocGSKDvS18HLvfi6oWcWq\nmvw/RO4eaXWvU6u7oOpKavnUlR+hPtSAv+kIm/t+wdd/8XutBihTouQ9yrP2cdzKE4TcilldnOVM\npmHSXNaMGYrx+wMadV7MXNfl14ceBOAty1+f93l6+xxajqSpqTZYoOlhBbXvWB8v7Ytzbem7aPIv\nx1fVw4Gy3/KpH9zPvmN9hQ5PipTe1aM8sG8rhs/hVfOvnNXFWc7lkvkXALC7e59KpRaxrW2/50Bf\nC+vq1rCsKr/ZDq7rsvXZBK4Ll18SzLvbXaZXwAhybcVbWBu+CjMYw12xlX9+7Kf88omDpB2n0OFJ\nkVG5pRGtnRG6/fswXZM3rMx/8YdC2dB4IT/b/2sSVQc5cnKQpfMrCx2STFJHtIufHfg1Jf4S/sh6\ne97nOXAoTXuHQ/NiH4sXvVLHXK28wjMMgwtLN9IYWMTTgw8SX3iAB3u6eO6eq/jL11/Jknn5FeKR\n809xNS89dN/zOzBLIywtWUVFsLzQ4UxabbiGFaVrMEsj/G7fc4UORyYp7aS5+6Ufk0gn+CPr7dTk\nuexnPO6y7bkEfh9cdYWedc8Wo6eRHYrtYjDdywUll7KodCG+ij665z/MF3/3I376mE1cg04lB0re\nQCyRYtfA8wC8xbquwNHk723WTQC8MPAs0ZgGwxSTh448yuGBo1w+72Iun3dx3ufZ/nySWBwu2RCg\nvExv79nMbwS4dtGVXLdgI+WBcvwLDrI5ei+fuvcXPLWrDUePv2Qc6jYHtuw6AtUnKKWKC2pnf0nU\nsSyvXUSD2Uxn2RF+8NQz/NWNxftBZDaIpxMc6DvEjo4XGUwMknRSJJ0UJgaVoQqqgpXUhWtZVbM8\n79W+AJ5qfZb7WzZRHariD1e/Le/zHGtNY+9PUV1lcOEavbWLgWEYLKpYwNtWvpn7Dj7E461PkVy0\njR+27OP+3RfzR1dtZN2yWo1bkLOc9+9w13XZdOhpjDqHVy/eWPRvkvde9Ea+tfPbvND/DK1dl7Kw\nfvZXiZtNXNfl5d79PHbsKV7u2UfKPb0LM2AGSLtpeuKZ58c7u/ZQ4g+zvv5CLmm8iLW1Fj4zt/Wy\nXdflNy0P8+DhRygLlHLLRe+jNFCaV9wdnWk2b4nj88G1VwdVw7xIZMchpDq7aOQy3lC1nBciT3Gy\n/CB95Vu4c9dOan+/lrdt2MjlF8zDLPK/TzJ9zvvkfd+2PQxV7cV0fbymeWOhw5kyq24F84ILaa9u\n5b82/57b3nVD0X8gmQnJdJJt7c+z+diTtA1lqmAtLJ/P2lqLlJOiLlyD3/RjGAaO6zKUHKI/PoDf\n5+eFjt08e/I5nj35HOWBMq5ouoSrmi5jUfmCMe99T6yX+w4+yLb256kvqePWDe+nsbQhr9h7etL8\n7tE4aQduvD5IY0NuHx5k9jgU23Xq64WhZqqdGrqcY3RVtNNfsZW7Wl7g3p3LuW7JFdy4fjlV5aEC\nRiuzwXmdvLfvb+XBzp9jliR565L/SXlwbrRS32bdxLd33c3Rkid45uXlXL1m9i+wUigDiUEeP/40\nT7Q+TSQ5hGmYXD7vYl67+LpTi4E82frMaa8xDYOKYDkVwXKuXbiRd6x8C0cGjrGt/QW2j3wA2Hzs\nSaqCFayuWcWqmmWU+EswMIilYmxvfwG79wAuLs2Vi/nQ+j/Pe5DkwIDDg48MEk9kWtxLFp/Xb+k5\no8ys5JLma+iPD3BioIcXul8g0bibTdE9/O6hRhb5V3Pd8g1csXoBJSH9m5+Pztt/9cMn+/ne7nsx\nq4a4tOZKXl+E08PGclH9Wi6vu4LtbOOHh35Aafj9bFi2oNBhzSrHB9vYfOxJtrc/T8pNU+ov4XVL\nbuD6Ra+a9Ehv0zBZVtXMsqpm3rHyzezpfpkdHS9i9xxgW/sOtrXvOOs1y6uWcvX8y7li3iUEfJMf\nFe66Lnv3pdj+XJJUGi6/JMDqleft23lOynaprwxfx+Kayzk0vJcD0d3Eatppo50ftz/FvfvrmOdf\nysXzLuDKFcuYX1emnrbzhKfvdsuyvg5sBFzgo7Ztbxu17ybgn4A0cL9t21/wMpbRBqIJvr7l5xgN\nHaPMmboAAAnTSURBVCwILeXmDfnPqZ2NDMPgz9a/k56twxxiN/++6y7e3PEe3nzlyvP6jT2UjLKz\ncze/P7mD/X2ZFdgaS+t5zaLruGr+ZYR8wSlfw2/62dCwjg0N63Bdl7ahkxzuP0rKTWeK5xiwpmYV\n88oa875Gb6/DM9sTnDjpEArCTa8tY0GTRibPVdku9ZAvwIUVlxB1Bukc7mSQbuLVnXTSye+GtvHQ\ntjD+WD2NoSZW1jSzYeEyls2rVct8jvLsX9WyrOuBVbZtX21Z1hrge8DVow75FvAGoBXYYlnWz23b\nfsmreEZr7RwiZgxQa9bz8StvznmAUTExDZOPv+pP+NftP2IvL3J///d59v7VvGnVq7liRTN+39yf\nRpRyUhwbbOVg/2Hs3gPYPQdIjwxAW12zkhsXX8faOsuzanqGYbCwfD4Ly+dP+Vz9Qwl22B08uesE\nLSf+//buNUauuozj+PfMzM7uzm5Ll20LFNpC0T5owHhDSQrIVQQbEcGQSCFaDBi8QIjxDQqCL7wg\nXiKEmAii+MIQUcBAgCCJlgBGifEC8mihtLSFwnbZ28zOzuUcX5yzdHahW6TdOTM7v08yu/85l5ln\nd8+e5/zPnPP8ywCsPCLLuhPyHLK8m/Hx8n6/h7SHQmYRq/sWAWtY3rWKbcUt7ChvZTS3k/Cg7bzM\ndl6e/CuPbYbw6R5ytX76s0sYzA+yvLCUFYuWcdiSQVYsWcxB/T26CG4vwihs6Uqb83lIdjpwD4C7\n/9vMBsxssbuPmdkaYNjdXwQwsweS5ZuSvN+1eoAfLL+C3p5cS/9x9lcmyHDFBz/Dvf9dxqNbH2O4\n92nu3PYMv3p2gIPzS1nes5ylfQMM9vXTny/QncvTlc2Sy2boymbIZbPkMhmCICAIMgQEBMSnUWLR\nnq9RQztoaCfzImb2DCOi1xeYOW/Puq+Fo4y8VpzxftPtMAqphlWqYZWpeoVirUipWmK8OsFweZih\n8m52l3dTi2qvr31Y4VDec/BxHDt4HAd3DxBFMDrxxvvhZ5eXnSjOKl3ZMHtoZPLNJs/42d9sfuOT\nMIqo1kIq1ZCpap3xUoWxYoXRYoWdQ0W27hpnZKICQBDAysMzrH1njlVHZDv6TIrAK9Vt9OSzHJ1f\nQxQdRTmcZHRqjFqmTLleZiI3Qr17iDGGGGMzW8pAGXgVojCAWp5s2EM26iFPD/lMN/lsnp5cN4sL\nBYJ6lnwmT1c2F+8bMlnySbsrm6MrlyOfzZIJsmSTbTEIMgQBBARkgiBuB/HeI25nCIi35UzDstOm\n243bduP8Rm93+w+jkHpUpx7VqdQrlOqTlGolRqZG2FHcwfbidir1Clce9xUW5+NqlWEYUQ9DavWI\ncqVOqVylNFWj9I+XeOb53WzbNU69HnH9xuMp9Mx/gaT5TN6HAo2lvl5Npo0l319tmPcK0NQxOPt6\n9/8UaTvIBBnOW3sW56w5lQeefYInd/2Zib4hhoNhhsP/wDjxY4GJ6lmich/hxBLC8SWEEwM8X+nl\neeCeA3iMeBdPHLDX2ptCIWDl4RkOPSTL0UflKBSUsOWNgiCgN1ugtzDzdsMwqjNRnaQ4NUmpXmIq\nLFOLKtSDCmGmQr1rgjA7ShUoNq5Yamb0rSWqdFMfG+T62/4G4VtLk309OY5ZNUBXrjkdwmZ+GDLX\nHmefe6NlyxYtiD3WVWe+/SIc++uyw9ZzGetTe39J2dsv3PYWHDOfLy4is8znIcJO4h72tBXAS3uZ\nd3gyTURERPZhPpP3w8AFAGb2fmCnu48DuPsLwGIzO9LMcsD6ZHkRERHZh2A+x342s+8AJwMh8EXg\nfcCou//OzE4Gvpssere7f3/eAhEREVlA5jV5i4iIyIG3cO+TEhERWaCUvEVERNqM6uY10VzlYmUP\nM/secBLx9vltd/9tyiG1NDPrBf4FfMvd70g5nJZlZhcBXwNqwLXufn/KIbUcM+sHfgkMAN3A9e7+\nULpRtRYzOxa4F/ihu99sZiuBO4Es8R1VF7v71HzHoZ53kzSWiwUuJS4PK7OY2anAscnv6WPAj1IO\nqR18HRhOO4hWZmaDwHXAicR3t5ybbkQt67OAu/upxHcL/TjdcFqLmfUBPwH+0DD5BuAWdz8J2Axs\nbEYsSt7NM6NcLDBgZovTDakl/Qn4dNIeAfrMbOEVnz9AzOwY4N2AepFzOwN4xN3H3f0ld78s7YBa\n1BAwmLQHkueyxxRwDjPrkpwC3Je0f0+8rc07Je/mmV0SdrpcrDRw97q7T1dpvJR4xLl6mjG1uJuA\nq9MOog0cCRTM7D4z22Rmp6cdUCty918Dq8xsM/GB9FdTDqmluHvN3SdnTe5rOE3+CrD/IxG9BUre\n6VkQ5V7ni5mdS5y8v5R2LK3KzC4BnnD3LWnH0gYC4h7lp4hPDf/czPQ/OIuZbQC2ufs7gNOAm1MO\nqd00bZtS8m6eucrFSgMzOwu4Bjjb3UfTjqeFfRw418yeBD4PfMPMmnLKrg3tAh5Pek7PEQ/Hsyzl\nmFrROuAhAHf/O7BCH1vt00Ry0Sg0sdS3knfz7LVcrOxhZgcBNwLr3V0XYc3B3S909+Pd/QTgZ8RX\nmz+Sdlwt6mHgNDPLJBev9aPPc9/MZuDDAGa2GpjQx1b79AhwftI+H3iwGW+qW8WaxN0fN7OnzOxx\n9pSLlTe6EFgK3GVm09Mucfdt6YUk7c7dd5jZb4Ank0lfdvdwrnU61E+B283sj8T54Qspx9NSzOwD\nxNeZHAlUzewC4CLgDjO7HNgK/KIZsag8qoiISJvRaXMREZE2o+QtIiLSZpS8RURE2oySt4iISJtR\n8hYREWkzSt4iHSypqCUibUbJW6RDJZWzrk07DhH5/6lIi0jnuh1YbWYPExcvmR6sYzuwwd2rZrYR\nuIp4IJ1NwBnufqKZXQlsAErJY4O77276TyDSodTzFulc1xEn5XOIE/BJ7r4OWAKclQxZeyNwpruf\nDqxtWPcG4hK2HyEec31FUyMX6XBK3iIdzt1rQB3YlJTFfC9xidq1wFZ335UsenfDarcBD5rZNcAW\nd/9nM2MW6XRK3iIdzszWARuBjyY96U3JrAxxHf5prw9Q4e5XA58EhoF7zOzsJoUrIih5i3SyEOgC\nDgFecPdiMpLUCUA38BxwtJkNJMufB2BmA2b2TeBFd78VuAX4ULODF+lkGphEpEOZWQ54CqgRH8gX\ngaeBvxBfhX4G8AniXvnWZNkT3f0UM7sJOAV4DagCl7p7U8YxFhElbxGZg5ldDNzv7sNmdjVg7n55\n2nGJdDrdKiYic+kHHjWzUeIe9udSjkdEUM9bRESk7eiCNRERkTaj5C0iItJmlLxFRETajJK3iIhI\nm1HyFhERaTNK3iIiIm3mf3Una7/Jg34kAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c87a1b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(np.log2(np.array([x for x in label_frame.values.flatten()])+1), label = 'peakScore')\n",
    "sns.distplot(np.log2(np.array([x for x in quantiled_label_frame.values.flatten()])+1), label = 'quantile normed')\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('tags')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_threshold = 1\n",
    "labels = (quantiled_label_frame >= score_threshold + 0).values\n",
    "target_indices = quantiled_label_frame[quantiled_label_frame.max(axis=1) >= score_threshold].index.values\n",
    "index_label_dict = dict(zip(target_indices, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c57bl6_il4-24h    33518\n",
       "c57bl6_kla-1h     21776\n",
       "c57bl6_veh        31550\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(quantiled_label_frame >= score_threshold + 0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = quantiled_label_frame.shape[1]\n",
    "labels = [index_label_dict[x] for x in target_indices] + [[0]*num_classes] * len(negative_seq)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq_arrays = np.array([index_seqArray_dict[x][0] for x in target_indices])\n",
    "target_seq_rc_arrays = np.array([index_seqArray_dict[x][1] for x in target_indices])\n",
    "\n",
    "seq_arrays = np.concatenate([target_seq_arrays, negative_sequence_arrays])\n",
    "seq_rc_arrays = np.concatenate([target_seq_rc_arrays, negative_sequence_rc_arrays])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(seq_arrays, seq_rc_arrays, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts [25795 16140 24768]\n",
      "naive accuracy 0.2595853879440475\n"
     ]
    }
   ],
   "source": [
    "print('label counts', labels.sum(axis=0))\n",
    "print('naive accuracy', max(labels.sum(axis=0))/len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_convolution_multilabel_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size):\n",
    "    input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "    input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "    ### find motifs ###\n",
    "    convolution_layer = Conv1D(filters=num_motifs, \n",
    "        kernel_size=motif_size,\n",
    "        activation='relu',\n",
    "        input_shape=(total_seq_length,4),\n",
    "        name='convolution_layer',\n",
    "        padding = 'same'\n",
    "        )\n",
    "    forward_motif_scores = convolution_layer(input_fwd)\n",
    "    reverse_motif_scores = convolution_layer(input_rev)\n",
    "    print('forward_motif_scores', forward_motif_scores.get_shape())\n",
    "\n",
    "    ### crop motif scores to avoid parts of sequence where motif score is computed in only one direction ###\n",
    "    to_crop = int((total_seq_length - seq_size)/2)\n",
    "    crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "        name='crop_layer')\n",
    "    cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "    cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "    print('cropped_fwd_scores', cropped_fwd_scores.get_shape())\n",
    "\n",
    "    ### flip motif scores ###\n",
    "    flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "        output_shape=(seq_size, num_motifs),\n",
    "        name='flip_layer')\n",
    "    flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "    print('flipped_rev_scores', flipped_rev_scores.get_shape())\n",
    "\n",
    "    ### pool across length of sequence ###\n",
    "    sequence_pooling_layer = MaxPool1D(pool_size=seq_size, \n",
    "        strides=adjacent_bp_pool_size,\n",
    "        name='sequence_pooling_layer')\n",
    "    pooled_fwd_scores = sequence_pooling_layer(cropped_fwd_scores)\n",
    "    pooled_rev_scores = sequence_pooling_layer(cropped_rev_scores)\n",
    "    print('pooled_fwd_scores', pooled_fwd_scores.get_shape())\n",
    "    print('pooled_rev_scores', pooled_rev_scores.get_shape())\n",
    "\n",
    "    ### concatenate motif scores ###\n",
    "    concatenate_layer = keras.layers.Concatenate(axis=1, name='concatenate_layer')\n",
    "    concatenated_motif_scores = concatenate_layer([pooled_fwd_scores, pooled_rev_scores])\n",
    "    print('concatenated_motif_scores', concatenated_motif_scores.get_shape())\n",
    "\n",
    "    ## pool across forward and reverse strand ###\n",
    "    strand_pooling_layer = MaxPool1D(pool_size=2, \n",
    "        strides=2,\n",
    "        name='strand_pooling_layer',\n",
    "        )\n",
    "    pooled_strand_scores = strand_pooling_layer(concatenated_motif_scores)\n",
    "    print('pooled_strand_scores', pooled_strand_scores.shape)\n",
    "    \n",
    "    # make prediction\n",
    "    flattened = Flatten(name='flatten')(pooled_strand_scores)\n",
    "    print('flattened', flattened.shape)\n",
    "    \n",
    "    predictions = Dense(num_classes,\n",
    "                        name='predictions',\n",
    "                        activation = 'relu', \n",
    "                       )(flattened)\n",
    "    print('predictions', predictions.shape)\n",
    "    \n",
    "    # define and compile model\n",
    "    model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.adam(),\n",
    "                  metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = len(labels[0])\n",
    "total_seq_length = 200\n",
    "seq_size = 150\n",
    "num_motifs = 30\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 5\n",
    "attention_dim = 100\n",
    "attention_hops = 1\n",
    "num_dense_neurons = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 30)\n",
      "cropped_fwd_scores (?, 150, 30)\n",
      "flipped_rev_scores (?, 150, 30)\n",
      "pooled_fwd_scores (?, 1, 30)\n",
      "pooled_rev_scores (?, 1, 30)\n",
      "concatenated_motif_scores (?, 2, 30)\n",
      "pooled_strand_scores (?, 1, 30)\n",
      "flattened (?, ?)\n",
      "predictions (?, 9)\n"
     ]
    }
   ],
   "source": [
    "signal_convolution_model = get_convolution_multilabel_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "signal_convolution_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = signal_convolution_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signal_convolution_model.predict([x_train[0:100], x_rc_train[0:100]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 25)\n",
      "cropped_fwd_scores (?, 150, 25)\n",
      "flipped_rev_scores (?, 150, 25)\n",
      "concatenated_motif_scores (?, 150, 50)\n",
      "pooled_scores (?, 30, 50)\n",
      "forward_hidden_states (?, ?, 30)\n",
      "reverse_hidden_states (?, ?, 30)\n",
      "bilstm_hidden_states (?, ?, 60)\n",
      "attention_tanh_layer_out (?, 30, 100)\n",
      "attention_outer_layer_out (?, 30, 1)\n",
      "attention_softmax_layer_out (?, 30, 1)\n",
      "attended_states (?, 1, 60)\n",
      "dense_output (?, 1, 100)\n",
      "drop_out (?, 1, 100)\n",
      "flattened (?, ?)\n",
      "predictions (?, 3)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(labels[0])\n",
    "total_seq_length = 200\n",
    "seq_size = 150\n",
    "num_motifs = 25\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 5\n",
    "attention_dim = 100\n",
    "attention_hops = 1\n",
    "num_dense_neurons = 100 \n",
    "\n",
    "signal_model = get_attention_multilabel_model(total_seq_length,\n",
    "                        seq_size,\n",
    "                        num_motifs, \n",
    "                        motif_size,\n",
    "                        adjacent_bp_pool_size,\n",
    "                        attention_dim,\n",
    "                        attention_hops,\n",
    "                        num_dense_neurons,\n",
    "                        dropout_rate=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_fwd (InputLayer)          (None, 200, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_rev (InputLayer)          (None, 200, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "convolution_layer (Conv1D)      (None, 200, 25)      2025        input_fwd[0][0]                  \n",
      "                                                                 input_rev[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "crop_layer (Cropping1D)         (None, 150, 25)      0           convolution_layer[0][0]          \n",
      "                                                                 convolution_layer[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flip_layer (Lambda)             (None, 150, 25)      0           crop_layer[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_layer (Concatenate) (None, 150, 50)      0           crop_layer[0][0]                 \n",
      "                                                                 flip_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequence_pooling_layer (MaxPool (None, 30, 50)       0           concatenate_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "forward_lstm_layer (LSTM)       (None, 30, 30)       9720        sequence_pooling_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reverse_lstm_layer (LSTM)       (None, 30, 30)       9720        sequence_pooling_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_122 (Concatenate)   (None, 30, 60)       0           forward_lstm_layer[0][0]         \n",
      "                                                                 reverse_lstm_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_tanh_layer (Dense)    (None, 30, 100)      6000        concatenate_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_outer_layer (Dense)   (None, 30, 1)        100         attention_tanh_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "attention_softmax_layer (Softma (None, 30, 1)        0           attention_outer_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attending_layer (Dot)           (None, 1, 60)        0           attention_softmax_layer[0][0]    \n",
      "                                                                 concatenate_122[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_layer (Dense)             (None, 1, 100)       6100        attending_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_dropout (Dropout)         (None, 1, 100)       0           dense_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           dense_dropout[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 3)            303         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,968\n",
      "Trainable params: 33,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "signal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parallel_attention_model = keras.utils.multi_gpu_model(signal_model, gpus=2)\n",
    "parallel_attention_model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79496 samples, validate on 19874 samples\n",
      "Epoch 1/50\n",
      "79496/79496 [==============================] - 51s 644us/step - loss: 0.5290 - acc: 0.7745 - val_loss: 0.5165 - val_acc: 0.7764\n",
      "Epoch 2/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4900 - acc: 0.7762 - val_loss: 0.4739 - val_acc: 0.7770\n",
      "Epoch 3/50\n",
      "79496/79496 [==============================] - 29s 371us/step - loss: 0.4680 - acc: 0.7783 - val_loss: 0.4598 - val_acc: 0.7787\n",
      "Epoch 4/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4583 - acc: 0.7804 - val_loss: 0.4620 - val_acc: 0.7790\n",
      "Epoch 5/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4496 - acc: 0.7826 - val_loss: 0.4482 - val_acc: 0.7837\n",
      "Epoch 6/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4425 - acc: 0.7840 - val_loss: 0.4488 - val_acc: 0.7807\n",
      "Epoch 7/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4368 - acc: 0.7864 - val_loss: 0.4440 - val_acc: 0.7842\n",
      "Epoch 8/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4317 - acc: 0.7873 - val_loss: 0.4346 - val_acc: 0.7875\n",
      "Epoch 9/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4285 - acc: 0.7890 - val_loss: 0.4319 - val_acc: 0.7886\n",
      "Epoch 10/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4238 - acc: 0.7898 - val_loss: 0.4298 - val_acc: 0.7890\n",
      "Epoch 11/50\n",
      "79496/79496 [==============================] - 29s 371us/step - loss: 0.4212 - acc: 0.7908 - val_loss: 0.4358 - val_acc: 0.7888\n",
      "Epoch 12/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4202 - acc: 0.7910 - val_loss: 0.4357 - val_acc: 0.7868\n",
      "Epoch 13/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4160 - acc: 0.7923 - val_loss: 0.4238 - val_acc: 0.7891\n",
      "Epoch 14/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4120 - acc: 0.7938 - val_loss: 0.4254 - val_acc: 0.7884\n",
      "Epoch 15/50\n",
      "79496/79496 [==============================] - 30s 371us/step - loss: 0.4103 - acc: 0.7938 - val_loss: 0.4203 - val_acc: 0.7907\n",
      "Epoch 16/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4084 - acc: 0.7943 - val_loss: 0.4200 - val_acc: 0.7927\n",
      "Epoch 17/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.4053 - acc: 0.7957 - val_loss: 0.4154 - val_acc: 0.7923\n",
      "Epoch 18/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.4032 - acc: 0.7962 - val_loss: 0.4131 - val_acc: 0.7920\n",
      "Epoch 19/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.4019 - acc: 0.7971 - val_loss: 0.4183 - val_acc: 0.7919\n",
      "Epoch 20/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.4020 - acc: 0.7968 - val_loss: 0.4288 - val_acc: 0.7911\n",
      "Epoch 21/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.3986 - acc: 0.7966 - val_loss: 0.4280 - val_acc: 0.7924\n",
      "Epoch 22/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3978 - acc: 0.7978 - val_loss: 0.4094 - val_acc: 0.7933\n",
      "Epoch 23/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.3970 - acc: 0.7988 - val_loss: 0.4145 - val_acc: 0.7913\n",
      "Epoch 24/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.3956 - acc: 0.7981 - val_loss: 0.4135 - val_acc: 0.7889\n",
      "Epoch 25/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3942 - acc: 0.7988 - val_loss: 0.4326 - val_acc: 0.7911\n",
      "Epoch 26/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3937 - acc: 0.7994 - val_loss: 0.4098 - val_acc: 0.7938\n",
      "Epoch 27/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.3919 - acc: 0.7999 - val_loss: 0.4085 - val_acc: 0.7944\n",
      "Epoch 28/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3908 - acc: 0.8003 - val_loss: 0.4046 - val_acc: 0.7945\n",
      "Epoch 29/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3895 - acc: 0.8011 - val_loss: 0.4054 - val_acc: 0.7947\n",
      "Epoch 30/50\n",
      "79496/79496 [==============================] - 29s 367us/step - loss: 0.3895 - acc: 0.8011 - val_loss: 0.4039 - val_acc: 0.7942\n",
      "Epoch 31/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3866 - acc: 0.8015 - val_loss: 0.4036 - val_acc: 0.7940\n",
      "Epoch 32/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3863 - acc: 0.8022 - val_loss: 0.4018 - val_acc: 0.7954\n",
      "Epoch 33/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3853 - acc: 0.8025 - val_loss: 0.4042 - val_acc: 0.7948\n",
      "Epoch 34/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3849 - acc: 0.8025 - val_loss: 0.4017 - val_acc: 0.7941\n",
      "Epoch 35/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3827 - acc: 0.8034 - val_loss: 0.4057 - val_acc: 0.7947\n",
      "Epoch 36/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3815 - acc: 0.8037 - val_loss: 0.4025 - val_acc: 0.7946\n",
      "Epoch 37/50\n",
      "79496/79496 [==============================] - 30s 371us/step - loss: 0.3811 - acc: 0.8034 - val_loss: 0.4014 - val_acc: 0.7952\n",
      "Epoch 38/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3815 - acc: 0.8040 - val_loss: 0.4033 - val_acc: 0.7949\n",
      "Epoch 39/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3795 - acc: 0.8042 - val_loss: 0.4001 - val_acc: 0.7957\n",
      "Epoch 40/50\n",
      "79496/79496 [==============================] - 29s 367us/step - loss: 0.3793 - acc: 0.8042 - val_loss: 0.3982 - val_acc: 0.7954\n",
      "Epoch 41/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3780 - acc: 0.8050 - val_loss: 0.4031 - val_acc: 0.7953\n",
      "Epoch 42/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3786 - acc: 0.8047 - val_loss: 0.4045 - val_acc: 0.7950\n",
      "Epoch 43/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3765 - acc: 0.8049 - val_loss: 0.4023 - val_acc: 0.7959\n",
      "Epoch 44/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3767 - acc: 0.8053 - val_loss: 0.3989 - val_acc: 0.7958\n",
      "Epoch 45/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3760 - acc: 0.8052 - val_loss: 0.4047 - val_acc: 0.7959\n",
      "Epoch 46/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3750 - acc: 0.8057 - val_loss: 0.3984 - val_acc: 0.7970\n",
      "Epoch 47/50\n",
      "79496/79496 [==============================] - 29s 368us/step - loss: 0.3744 - acc: 0.8067 - val_loss: 0.3990 - val_acc: 0.7969\n",
      "Epoch 48/50\n",
      "79496/79496 [==============================] - 29s 370us/step - loss: 0.3737 - acc: 0.8065 - val_loss: 0.3999 - val_acc: 0.7945\n",
      "Epoch 49/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3734 - acc: 0.8060 - val_loss: 0.3983 - val_acc: 0.7951\n",
      "Epoch 50/50\n",
      "79496/79496 [==============================] - 29s 369us/step - loss: 0.3736 - acc: 0.8062 - val_loss: 0.3992 - val_acc: 0.7941\n",
      "Test loss: 0.3991017606736573\n",
      "Test accuracy: 0.7934319734921262\n"
     ]
    }
   ],
   "source": [
    "parallel_attention_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=200,\n",
    "          epochs=50,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = signal_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = signal_model.predict([x_test, x_rc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(predictions[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(np.round(predictions), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4882170259408111\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.accuracy_score(np.round(predictions), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # serialize model to JSON\n",
    "# model_json = signal_model.to_json()\n",
    "# with open(\"signal_model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# signal_model.save_weights(\"signal_model.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
