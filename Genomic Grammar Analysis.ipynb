{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic Grammar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Maximum, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import Bio.motifs\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('/home/jtao/analysis/genomic_grammar_analysis/'):\n",
    "    os.mkdir('/home/jtao/analysis/genomic_grammar_analysis')\n",
    "os.chdir('/home/jtao/analysis/genomic_grammar_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sequences_to_array(sequences):\n",
    "    '''\n",
    "    inputs: sequence of nucleotides represented as a string composed of A, C, G, T\n",
    "    outputs: a list of numpy array representations of a sequence with:\n",
    "             A = [1, 0, 0, 0]\n",
    "             C = [0, 1, 0, 0]\n",
    "             G = [0, 0, 1, 0]\n",
    "             T = [0, 0, 0, 1]\n",
    "             \n",
    "    '''\n",
    "\n",
    "    nucleotide_array_dict = {'A': [1, 0, 0, 0],\n",
    "                             'C': [0, 1, 0, 0],\n",
    "                             'G': [0, 0, 1, 0],\n",
    "                             'T': [0, 0, 0, 1],\n",
    "                             'N': [0.25,0.25,0.25,0.25]}\n",
    "\n",
    "    sequence_array_list = []\n",
    "    for seq in sequences:\n",
    "        seq_array = []\n",
    "        for nuc in seq:\n",
    "            seq_array.append(nucleotide_array_dict[nuc])\n",
    "        seq_array = np.array(seq_array)\n",
    "        sequence_array_list.append(seq_array)\n",
    "    sequence_array_list = np.array(sequence_array_list)\n",
    "    return sequence_array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "positive_seqRecords = list(SeqIO.parse('./c57bl6_atf3_veh_idr_peaks.fasta', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background.fasta', 'fasta'))\n",
    "\n",
    "fasta_seq = [str(x.seq)[:200] for x in positive_seqRecords] + [str(x.seq)[:200] for x in negative_seqRecords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_num_params(seq_size,\n",
    "                     num_motifs, \n",
    "                     motif_size,\n",
    "                     adjacent_bp_pool_size,\n",
    "                     attention_dim,\n",
    "                     attention_hops,\n",
    "                     num_dense_neurons\n",
    "                    ):\n",
    "    total_params = 0\n",
    "    \n",
    "    # convolution layer\n",
    "    convolution_params = num_motifs * motif_size * 4\n",
    "    total_params += convolution_params\n",
    "    print('Convolution Params:', convolution_params)\n",
    "    \n",
    "    # lstm layer\n",
    "    # account for pooling\n",
    "    lstm_input_size = num_motifs\n",
    "    # account for reverse complement sequence\n",
    "    lstm_input_size = lstm_input_size * 2\n",
    "    \n",
    "    num_lstm_neurons = seq_size/ adjacent_bp_pool_size\n",
    "    \n",
    "    lstm_params = lstm_input_size * 4 * num_lstm_neurons\n",
    "    # account for bidrectional lstm\n",
    "    lstm_params = lstm_params * 2\n",
    "    \n",
    "    total_params += lstm_params\n",
    "    print('LSTM Params:', lstm_params)\n",
    "    \n",
    "    # attention later\n",
    "    attention_params = attention_dim * (seq_size/adjacent_bp_pool_size*2) \n",
    "    # account for hops of attention\n",
    "    attention_params += attention_dim * attention_hops\n",
    "    \n",
    "    total_params += attention_params\n",
    "    print('Attention Params:', attention_params)\n",
    "    \n",
    "    # dense layer\n",
    "    dense_params = lstm_input_size * num_dense_neurons\n",
    "    total_params += dense_params\n",
    "    print('Dense Params:', dense_params)\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Params: 1600\n",
      "LSTM Params: 24000.0\n",
      "Attention Params: 15100.0\n",
      "Dense Params: 80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40780.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 20\n",
    "motif_size = 20\n",
    "attention_dim = 100 # from A Structured Self-attentive Sentence Embedding\n",
    "attention_hops = 1 # from A Structured Self-attentive Sentence Embedding\n",
    "adjacent_bp_pool_size = 2\n",
    "num_dense_neurons = 2\n",
    "\n",
    "count_num_params(seq_size,\n",
    "    num_motifs, \n",
    "    motif_size,\n",
    "    adjacent_bp_pool_size,\n",
    "    attention_dim,\n",
    "    attention_hops,\n",
    "    num_dense_neurons\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_fwd = Input(shape=(200,4), name='input_fwd')\n",
    "\n",
    "shared_motif_convolution = Conv1D(filters=16, \n",
    "     kernel_size=24,\n",
    "     activation='relu',\n",
    "     input_shape=(200,4))\n",
    "\n",
    "motif_scores_fwd = shared_motif_convolution(input_fwd)\n",
    "\n",
    "max_seq_scores = MaxPooling1D(pool_size=176)(motif_scores_fwd)\n",
    "\n",
    "dense_out = Dense(32, activation='relu')(max_seq_scores)\n",
    "\n",
    "drop_out = Dropout(0.25)(dense_out)\n",
    "\n",
    "flattened = Flatten()(drop_out)\n",
    "\n",
    "predictions = Dense(num_classes, activation = 'softmax')(flattened)\n",
    "\n",
    "model = Model(inputs=input_fwd, outputs=predictions)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
    "                        input_shape=(5, 10)))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"342pt\" viewBox=\"0.00 0.00 276.00 342.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,5 -4,-338 273,-338 273,5 -4,5\" stroke=\"white\"/>\n",
       "<!-- 139847517225648 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139847517225648</title>\n",
       "<polygon fill=\"none\" points=\"34,-297 34,-333 234,-333 234,-297 34,-297\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-311.3\">bidirectional_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139848767086888 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139848767086888</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-223 -0.5,-259 268.5,-259 268.5,-223 -0.5,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-237.3\">bidirectional_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 139847517225648&#45;&gt;139848767086888 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139847517225648-&gt;139848767086888</title>\n",
       "<path d=\"M134,-296.937C134,-288.807 134,-278.876 134,-269.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-269.441 134,-259.441 130.5,-269.441 137.5,-269.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139846882892208 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139846882892208</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-149 -0.5,-185 268.5,-185 268.5,-149 -0.5,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-163.3\">bidirectional_2(lstm_2): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 139848767086888&#45;&gt;139846882892208 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139848767086888-&gt;139846882892208</title>\n",
       "<path d=\"M134,-222.937C134,-214.807 134,-204.876 134,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-195.441 134,-185.441 130.5,-195.441 137.5,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139847517225704 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139847517225704</title>\n",
       "<polygon fill=\"none\" points=\"83,-75 83,-111 185,-111 185,-75 83,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-89.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139846882892208&#45;&gt;139847517225704 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139846882892208-&gt;139847517225704</title>\n",
       "<path d=\"M134,-148.937C134,-140.807 134,-130.876 134,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-121.441 134,-111.441 130.5,-121.441 137.5,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139846881738704 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139846881738704</title>\n",
       "<polygon fill=\"none\" points=\"60,-1 60,-37 208,-37 208,-1 60,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-15.3\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 139847517225704&#45;&gt;139846881738704 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139847517225704-&gt;139846881738704</title>\n",
       "<path d=\"M134,-74.937C134,-66.8072 134,-56.8761 134,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-47.4406 134,-37.4407 130.5,-47.4407 137.5,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
