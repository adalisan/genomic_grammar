{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genomic Grammar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import Bio.motifs\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('/home/jtao/analysis/genomic_grammar_analysis/'):\n",
    "    os.mkdir('/home/jtao/analysis/genomic_grammar_analysis')\n",
    "os.chdir('/home/jtao/analysis/genomic_grammar_analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sequences_to_array(sequences):\n",
    "    '''\n",
    "    inputs: sequence of nucleotides represented as a string composed of A, C, G, T\n",
    "    outputs: a list of numpy array representations of a sequence with:\n",
    "             A = [1, 0, 0, 0]\n",
    "             C = [0, 1, 0, 0]\n",
    "             G = [0, 0, 1, 0]\n",
    "             T = [0, 0, 0, 1]\n",
    "             \n",
    "    '''\n",
    "\n",
    "    nucleotide_array_dict = {'A': [1, 0, 0, 0],\n",
    "                             'C': [0, 1, 0, 0],\n",
    "                             'G': [0, 0, 1, 0],\n",
    "                             'T': [0, 0, 0, 1],\n",
    "                             'N': [0.25,0.25,0.25,0.25]}\n",
    "\n",
    "    sequence_array_list = []\n",
    "    for seq in sequences:\n",
    "        seq_array = []\n",
    "        for nuc in seq:\n",
    "            seq_array.append(nucleotide_array_dict[nuc])\n",
    "        seq_array = np.array(seq_array)\n",
    "        sequence_array_list.append(seq_array)\n",
    "    sequence_array_list = np.array(sequence_array_list)\n",
    "    return sequence_array_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./atac_idr_peaks/c57bl6_il4-24h_peaks.tsv\n",
      "./bed_files/c57bl6_il4-24h_peaks.bed\n",
      "./atac_idr_peaks/c57bl6_kla-1h_peaks.tsv\n",
      "./bed_files/c57bl6_kla-1h_peaks.bed\n",
      "./atac_idr_peaks/c57bl6_veh_peaks.tsv\n",
      "./bed_files/c57bl6_veh_peaks.bed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\tConverted 37096 peaks total\n",
      "\n",
      "\n",
      "\tConverted 31419 peaks total\n",
      "\n",
      "\n",
      "\tConverted 39536 peaks total\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d ./bed_files ]; then mkdir ./bed_files; else rm ./bed_files/*; fi\n",
    "for peak in ./atac_idr_peaks/*tsv;\n",
    "do echo $peak;\n",
    "new_path=${peak/atac_idr_peaks/bed_files};\n",
    "new_path=${new_path/.tsv/.bed};\n",
    "echo $new_path;\n",
    "pos2bed.pl $peak >$new_path\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bed_files/c57bl6_il4-24h_peaks.bed\n",
      "./fasta_files/c57bl6_il4-24h_peaks.fa\n",
      "reading genome mm10\n",
      "./bed_files/c57bl6_kla-1h_peaks.bed\n",
      "./fasta_files/c57bl6_kla-1h_peaks.fa\n",
      "reading genome mm10\n",
      "./bed_files/c57bl6_veh_peaks.bed\n",
      "./fasta_files/c57bl6_veh_peaks.fa\n",
      "reading genome mm10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./fasta_files/*’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d ./fasta_files ]; then mkdir ./fasta_files; else rm ./fasta_files/*; fi\n",
    "for bed_path in ./bed_files/*bed;\n",
    "do echo $bed_path;\n",
    "new_path=${bed_path/bed_files/fasta_files};\n",
    "new_path=${new_path/.bed/.fa};\n",
    "echo $new_path;\n",
    "/home/jtao/code/tba/model_training/extract_sequences.py $bed_path mm10 $new_path\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Background Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bed_files/c57bl6_il4-24h_peaks.bed\n",
      "/home/jtao/code/tba/model_training/generate_background_coordinates.py ./bed_files/c57bl6_il4-24h_peaks.bed ./background_files -genome mm10\n",
      "./bed_files/c57bl6_kla-1h_peaks.bed\n",
      "/home/jtao/code/tba/model_training/generate_background_coordinates.py ./bed_files/c57bl6_kla-1h_peaks.bed ./background_files -genome mm10\n",
      "./bed_files/c57bl6_veh_peaks.bed\n",
      "/home/jtao/code/tba/model_training/generate_background_coordinates.py ./bed_files/c57bl6_veh_peaks.bed ./background_files -genome mm10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./background_files/*’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d ./background_files ]; then mkdir ./background_files; else rm ./background_files/*; fi\n",
    "for bed_path in ./bed_files/*bed;\n",
    "do echo $bed_path;\n",
    "echo /home/jtao/code/tba/model_training/generate_background_coordinates.py $bed_path ./background_files -genome mm10\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ./make_background.sh\n",
    "script_path=\"./make_background.sh\"\n",
    "if [ ! -d ./background_files/ ]; then mkdir ./background_files/ ; fi\n",
    "for i in ./bed_files/*bed;\n",
    "do \n",
    "    factor=${i##*/};\n",
    "    factor=${factor%.bed};\n",
    "    fasta_path=\"./background_files/${factor}_background.fasta\"\n",
    "    bed_path=\"./background_files/${factor}_background.bed\"\n",
    "\n",
    "    echo \"/home/jtao/code/tba/model_training/generate_background_coordinates.py $i ./background_files/ -genome mm10\" >> $script_path;\n",
    "    echo \"mv ./background_files/background.bed $bed_path\" >> $script_path;\n",
    "    echo \"mv ./background_files/background.fasta $fasta_path\" >> $script_path;\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm ./background/*\n",
    "chmod a+x ./*sh\n",
    "bash ./make_background.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_seqRecords = list(SeqIO.parse('./fasta_files/c57bl6_il4-24h_peaks.fa', 'fasta'))\n",
    "negative_seqRecords = list(SeqIO.parse('./background_files/c57bl6_il4-24h_peaks_background.fasta', 'fasta'))\n",
    "\n",
    "fasta_seq = [str(x.seq[:200]) for x in positive_seqRecords] + [str(x[:200].seq) for x in negative_seqRecords]\n",
    "\n",
    "fasta_rc_seq = [str(x[:200].reverse_complement().seq) for x in positive_seqRecords] + \\\n",
    "    [str(x[:200].reverse_complement().seq) for x in negative_seqRecords]\n",
    "\n",
    "sequence_arrays = convert_sequences_to_array(fasta_seq)\n",
    "sequence_arrays = np.array(sequence_arrays)\n",
    "\n",
    "sequence_rc_arrays = convert_sequences_to_array(fasta_rc_seq)\n",
    "sequence_rc_arrays = np.array(sequence_rc_arrays)\n",
    "\n",
    "\n",
    "labels = [1 for x in positive_seqRecords] + [0 for x in negative_seqRecords]\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "x_train, x_test, x_rc_train, x_rc_test, y_train, y_test = model_selection.train_test_split(sequence_arrays, sequence_rc_arrays, labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_num_params(seq_size,\n",
    "                     num_motifs, \n",
    "                     motif_size,\n",
    "                     adjacent_bp_pool_size,\n",
    "                     attention_dim,\n",
    "                     attention_hops,\n",
    "                     num_dense_neurons\n",
    "                    ):\n",
    "    total_params = 0\n",
    "    \n",
    "    # convolution layer\n",
    "    convolution_params = num_motifs * motif_size * 4\n",
    "    total_params += convolution_params\n",
    "    print('Convolution Params:', convolution_params)\n",
    "    \n",
    "    # lstm layer\n",
    "    # account for pooling\n",
    "    lstm_input_size = num_motifs\n",
    "    # account for reverse complement sequence\n",
    "    lstm_input_size = lstm_input_size * 2\n",
    "    \n",
    "    num_lstm_neurons = seq_size/ adjacent_bp_pool_size\n",
    "    \n",
    "    lstm_params = lstm_input_size * 4 * num_lstm_neurons\n",
    "    # account for bidrectional lstm\n",
    "    lstm_params = lstm_params * 2\n",
    "    \n",
    "    total_params += lstm_params\n",
    "    print('LSTM Params:', lstm_params)\n",
    "    \n",
    "    # attention layer\n",
    "    # first layer of perceptron\n",
    "    attention_params = attention_dim * (seq_size/adjacent_bp_pool_size*2) \n",
    "    # account for hops of attention (second layer of perceptron)\n",
    "    attention_params += attention_dim * attention_hops\n",
    "    \n",
    "    total_params += attention_params\n",
    "    print('Attention Params:', attention_params)\n",
    "    \n",
    "    # dense layer\n",
    "    dense_params = lstm_input_size * num_dense_neurons\n",
    "    total_params += dense_params\n",
    "    print('Dense Params:', dense_params)\n",
    "    \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Params: 1600\n",
      "LSTM Params: 48000.0\n",
      "Attention Params: 15050.0\n",
      "Dense Params: 4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68650.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 20\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 1\n",
    "attention_dim = 50 # 350 from A Structured Self-attentive Sentence Embedding\n",
    "attention_hops = 1 # from A Structured Self-attentive Sentence Embedding\n",
    "num_dense_neurons = 100 # 2-layer, 2000 units, from A Structured Self-attentive Sentence Embedding\n",
    "\n",
    "count_num_params(seq_size,\n",
    "    num_motifs, \n",
    "    motif_size,\n",
    "    adjacent_bp_pool_size,\n",
    "    attention_dim,\n",
    "    attention_hops,\n",
    "    num_dense_neurons\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 20\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 1\n",
    "attention_dim = 50 \n",
    "attention_hops = 1 \n",
    "num_dense_neurons = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Motif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 20)\n",
      "cropped_fwd_scores (?, 150, 20)\n",
      "max_fwd_scores (?, 1, 20)\n",
      "max_seq_scores (?, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "total_seq_length = len(fasta_seq[0])\n",
    "\n",
    "input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "# find motifs\n",
    "convolution_layer = Conv1D(filters=num_motifs, \n",
    "    kernel_size=motif_size,\n",
    "    activation='relu',\n",
    "    input_shape=(total_seq_length,4),\n",
    "    name='convolution_layer',\n",
    "    padding = 'same'\n",
    "    )\n",
    "forward_motif_scores = convolution_layer(input_fwd)\n",
    "reverse_motif_scores = convolution_layer(input_rev)\n",
    "print('forward_motif_scores', forward_motif_scores.shape)\n",
    "\n",
    "# crop motif scores to avoid parts of sequence where motif score is computed in only one direction\n",
    "to_crop = int((total_seq_length - seq_size)/2)\n",
    "crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "    name='crop_layer')\n",
    "cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "print('cropped_fwd_scores', cropped_fwd_scores.shape)\n",
    "\n",
    "# # flip motif scores\n",
    "# flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "#     output_shape=(seq_size, num_motifs),\n",
    "#     name='flip_layer')\n",
    "# flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "# print('flipped_rev_scores', flipped_rev_scores.shape)\n",
    "\n",
    "# calculate max scores for each orientation\n",
    "seq_pool_layer = MaxPool1D(pool_size=seq_size)\n",
    "max_fwd_scores = seq_pool_layer(cropped_fwd_scores)\n",
    "max_rev_scores = seq_pool_layer(cropped_rev_scores)\n",
    "print('max_fwd_scores', max_fwd_scores.shape)\n",
    "\n",
    "# calculate max score for strand\n",
    "orientation_max_layer = Maximum()\n",
    "max_seq_scores = orientation_max_layer([max_fwd_scores, max_rev_scores])\n",
    "print('max_seq_scores', max_seq_scores.shape)\n",
    "\n",
    "# fully connected layer\n",
    "dense_out = Dense(32, activation='relu', \n",
    "                 )(max_seq_scores)\n",
    "\n",
    "# drop out\n",
    "drop_out = Dropout(0.25)(dense_out)\n",
    "\n",
    "# make prediction\n",
    "flattened = Flatten()(drop_out)\n",
    "predictions = Dense(num_classes,\n",
    "                    activation = 'softmax', \n",
    "                   )(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define and compile model\n",
    "convolution_model = Model(inputs=[input_fwd, input_rev], outputs=predictions)\n",
    "\n",
    "convolution_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59352 samples, validate on 14838 samples\n",
      "Epoch 1/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3703 - acc: 0.8381 - val_loss: 0.3683 - val_acc: 0.8370\n",
      "Epoch 2/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3694 - acc: 0.8380 - val_loss: 0.3755 - val_acc: 0.8305\n",
      "Epoch 3/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3676 - acc: 0.8381 - val_loss: 0.3692 - val_acc: 0.8356\n",
      "Epoch 4/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3660 - acc: 0.8405 - val_loss: 0.3684 - val_acc: 0.8355\n",
      "Epoch 5/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3639 - acc: 0.8422 - val_loss: 0.3665 - val_acc: 0.8384\n",
      "Epoch 6/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3632 - acc: 0.8424 - val_loss: 0.3643 - val_acc: 0.8384\n",
      "Epoch 7/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3605 - acc: 0.8442 - val_loss: 0.3697 - val_acc: 0.8358\n",
      "Epoch 8/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3605 - acc: 0.8446 - val_loss: 0.3648 - val_acc: 0.8371\n",
      "Epoch 9/10\n",
      "59352/59352 [==============================] - 10s - loss: 0.3589 - acc: 0.8461 - val_loss: 0.3661 - val_acc: 0.8374\n",
      "Epoch 10/10\n",
      "59352/59352 [==============================] - 9s - loss: 0.3588 - acc: 0.8442 - val_loss: 0.3667 - val_acc: 0.8374\n",
      "Test loss: 0.36674343662725367\n",
      "Test accuracy: 0.8374443995067253\n"
     ]
    }
   ],
   "source": [
    "convolution_model.fit([x_train, x_rc_train], y_train,\n",
    "          batch_size=100,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=([x_test, x_rc_test], y_test))\n",
    "score = convolution_model.evaluate([x_test, x_rc_test], y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918505383744855"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = convolution_model.predict([x_test, x_rc_test])\n",
    "\n",
    "sklearn.metrics.roc_auc_score([y[1] for y in y_test], probs[:,1], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_size = 150\n",
    "num_classes = 2\n",
    "num_motifs = 20\n",
    "motif_size = 20\n",
    "adjacent_bp_pool_size = 1\n",
    "attention_dim = 50 \n",
    "attention_hops = 1 \n",
    "num_dense_neurons = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_motif_scores (?, 200, 20)\n",
      "cropped_fwd_scores (?, 150, 20)\n",
      "flipped_rev_scores (?, 150, 20)\n",
      "concatenated_motif_scores (?, 150, 40)\n",
      "pooled_scores (?, 150, 40)\n"
     ]
    }
   ],
   "source": [
    "total_seq_length = len(fasta_seq[0])\n",
    "\n",
    "input_fwd = Input(shape=(total_seq_length,4), name='input_fwd')\n",
    "input_rev = Input(shape=(total_seq_length,4), name='input_rev')\n",
    "\n",
    "# find motifs\n",
    "convolution_layer = Conv1D(filters=num_motifs, \n",
    "    kernel_size=motif_size,\n",
    "    activation='relu',\n",
    "    input_shape=(total_seq_length,4),\n",
    "    name='convolution_layer',\n",
    "    padding = 'same'\n",
    "    )\n",
    "forward_motif_scores = convolution_layer(input_fwd)\n",
    "reverse_motif_scores = convolution_layer(input_rev)\n",
    "print('forward_motif_scores', forward_motif_scores.shape)\n",
    "\n",
    "# crop motif scores to avoid parts of sequence where motif score is computed in only one direction\n",
    "to_crop = int((total_seq_length - seq_size)/2)\n",
    "crop_layer = Cropping1D(cropping=(to_crop, to_crop), \n",
    "    name='crop_layer')\n",
    "cropped_fwd_scores = crop_layer(forward_motif_scores)\n",
    "cropped_rev_scores = crop_layer(reverse_motif_scores)\n",
    "print('cropped_fwd_scores', cropped_fwd_scores.shape)\n",
    "\n",
    "# flip motif scores\n",
    "flip_layer = Lambda(lambda x: K.reverse(x,axes=0),\n",
    "    output_shape=(seq_size, num_motifs),\n",
    "    name='flip_layer')\n",
    "flipped_rev_scores = flip_layer(cropped_rev_scores)\n",
    "print('flipped_rev_scores', flipped_rev_scores.shape)\n",
    "\n",
    "# concatenate motif scores\n",
    "concatenate_layer = keras.layers.Concatenate(axis=2, name='concatenate_layer')\n",
    "concatenated_motif_scores = concatenate_layer([cropped_fwd_scores, flipped_rev_scores])\n",
    "print('concatenated_motif_scores', concatenated_motif_scores.shape)\n",
    "\n",
    "# pool across length of sequence\n",
    "sequence_pooling_layer = MaxPool1D(pool_size=adjacent_bp_pool_size, \n",
    "    strides=adjacent_bp_pool_size,\n",
    "    name='sequence_pooling_layer')\n",
    "pooled_scores = sequence_pooling_layer(concatenated_motif_scores)\n",
    "print('pooled_scores', pooled_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
    "                        input_shape=(5, 10)))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"342pt\" viewBox=\"0.00 0.00 276.00 342.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 338)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,5 -4,-338 273,-338 273,5 -4,5\" stroke=\"white\"/>\n",
       "<!-- 139847517225648 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139847517225648</title>\n",
       "<polygon fill=\"none\" points=\"34,-297 34,-333 234,-333 234,-297 34,-297\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-311.3\">bidirectional_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139848767086888 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139848767086888</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-223 -0.5,-259 268.5,-259 268.5,-223 -0.5,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-237.3\">bidirectional_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 139847517225648&#45;&gt;139848767086888 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139847517225648-&gt;139848767086888</title>\n",
       "<path d=\"M134,-296.937C134,-288.807 134,-278.876 134,-269.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-269.441 134,-259.441 130.5,-269.441 137.5,-269.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139846882892208 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139846882892208</title>\n",
       "<polygon fill=\"none\" points=\"-0.5,-149 -0.5,-185 268.5,-185 268.5,-149 -0.5,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-163.3\">bidirectional_2(lstm_2): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 139848767086888&#45;&gt;139846882892208 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139848767086888-&gt;139846882892208</title>\n",
       "<path d=\"M134,-222.937C134,-214.807 134,-204.876 134,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-195.441 134,-185.441 130.5,-195.441 137.5,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139847517225704 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139847517225704</title>\n",
       "<polygon fill=\"none\" points=\"83,-75 83,-111 185,-111 185,-75 83,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-89.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139846882892208&#45;&gt;139847517225704 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139846882892208-&gt;139847517225704</title>\n",
       "<path d=\"M134,-148.937C134,-140.807 134,-130.876 134,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-121.441 134,-111.441 130.5,-121.441 137.5,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139846881738704 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139846881738704</title>\n",
       "<polygon fill=\"none\" points=\"60,-1 60,-37 208,-37 208,-1 60,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134\" y=\"-15.3\">activation_1: Activation</text>\n",
       "</g>\n",
       "<!-- 139847517225704&#45;&gt;139846881738704 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139847517225704-&gt;139846881738704</title>\n",
       "<path d=\"M134,-74.937C134,-66.8072 134,-56.8761 134,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137.5,-47.4406 134,-37.4407 130.5,-47.4407 137.5,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
